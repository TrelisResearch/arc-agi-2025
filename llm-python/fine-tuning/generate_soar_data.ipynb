{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a2512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7919f997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded SOAR data: 4,926,487 total rows, 1,734,164 filtered rows\n",
      "Target models: ['Mistral-Large-Instruct-2407', 'Qwen2.5-72B-Instruct']\n"
     ]
    }
   ],
   "source": [
    "# Load SOAR data and filter for target models, add score column\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"julien31/soar_arc_train_5M\", columns=['task_id', 'correct_train_input', 'correct_test_input', 'model'])\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "# Filter for target models and add score\n",
    "models_to_keep = ['Mistral-Large-Instruct-2407', 'Qwen2.5-72B-Instruct']\n",
    "df_filtered = df[df['model'].isin(models_to_keep)].copy()\n",
    "df_filtered['score'] = df_filtered.apply(\n",
    "    lambda row: (sum(row['correct_train_input']) + sum(row['correct_test_input'])) / \n",
    "                (len(row['correct_train_input']) + len(row['correct_test_input'])), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded SOAR data: {len(df):,} total rows, {len(df_filtered):,} filtered rows\")\n",
    "print(f\"Target models: {models_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca32956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 400 ARC training tasks\n",
      "TaskLoader initialized with data root: ../../data\n"
     ]
    }
   ],
   "source": [
    "# Get ARC training task IDs\n",
    "\n",
    "from task_loader import TaskLoader\n",
    "\n",
    "loader = TaskLoader(data_root=\"../../data\")\n",
    "training_path = loader.data_root / \"arc-agi-1\" / \"training\"\n",
    "all_training_task_ids = [f.stem for f in training_path.glob(\"*.json\")]\n",
    "\n",
    "print(f\"âœ… Found {len(all_training_task_ids)} ARC training tasks\")\n",
    "print(f\"TaskLoader initialized with data root: {loader.data_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7202e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Selected 40000 SOAR rows from 400 ARC tasks\n",
      "Average samples per task: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Select top/bottom 50 per task and preserve original indices\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_filtered_with_idx = df_filtered.reset_index()  # Preserve original row indices\n",
    "selected_indices = []\n",
    "\n",
    "for task_id in all_training_task_ids:\n",
    "    task_rows = df_filtered_with_idx[df_filtered_with_idx['task_id'] == task_id]\n",
    "    if len(task_rows) == 0:\n",
    "        continue\n",
    "    \n",
    "    task_rows_sorted = task_rows.sort_values('score', ascending=True)\n",
    "    bottom_50 = task_rows_sorted.head(50)\n",
    "    top_50 = task_rows_sorted.tail(50)\n",
    "    selected_rows = pd.concat([bottom_50, top_50])\n",
    "    selected_indices.extend(selected_rows['index'].tolist())\n",
    "\n",
    "print(f\"âœ… Selected {len(selected_indices)} SOAR rows from {len(all_training_task_ids)} ARC tasks\")\n",
    "print(f\"Average samples per task: {len(selected_indices) / len(all_training_task_ids):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c2e568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded full SOAR data: 40,000 rows\n",
      "Columns: ['code', 'correct_train_input', 'predicted_train_output', 'correct_test_input', 'predicted_test_output']... (9 total)\n"
     ]
    }
   ],
   "source": [
    "# Efficiently load full SOAR data for selected rows only\n",
    "\n",
    "ds_full = load_dataset(\"julien31/soar_arc_train_5M\")\n",
    "selected_dataset = ds_full['train'].select(selected_indices)\n",
    "soar_df = selected_dataset.to_pandas()\n",
    "\n",
    "# Add score column to full dataset\n",
    "soar_df['score'] = soar_df.apply(\n",
    "    lambda row: (sum(row['correct_train_input']) + sum(row['correct_test_input'])) / \n",
    "                (len(row['correct_train_input']) + len(row['correct_test_input'])), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded full SOAR data: {len(soar_df):,} rows\")\n",
    "print(f\"Columns: {list(soar_df.columns)[:5]}... ({len(soar_df.columns)} total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d19612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corresponding ARC task data...\n",
      "âœ… Loaded 400 ARC tasks\n"
     ]
    }
   ],
   "source": [
    "# Load corresponding ARC task data\n",
    "print(\"Loading corresponding ARC task data...\")\n",
    "\n",
    "arc_tasks = {}\n",
    "\n",
    "for task_id in soar_df['task_id'].unique():\n",
    "    task_data = loader.load_task(task_id, \"arc-agi-1\")\n",
    "    arc_tasks[task_id] = task_data\n",
    "\n",
    "print(f\"âœ… Loaded {len(arc_tasks)} ARC tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871b20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 40,000\n",
      "Unique ARC tasks: 400\n",
      "Models: {'Qwen2.5-72B-Instruct': 25635, 'Mistral-Large-Instruct-2407': 14365}\n",
      "Mean score: 0.352\n",
      "\n",
      "Columns available:\n",
      "  - code\n",
      "  - correct_train_input\n",
      "  - predicted_train_output\n",
      "  - correct_test_input\n",
      "  - predicted_test_output\n",
      "  - task_id\n",
      "  - model\n",
      "  - generation\n",
      "  - score\n",
      "  - arc_train_examples\n",
      "  - arc_test_examples\n",
      "  - num_train_examples\n",
      "  - num_test_examples\n"
     ]
    }
   ],
   "source": [
    "# Merge SOAR and ARC data\n",
    "\n",
    "def add_arc_data(row):\n",
    "    task_id = row['task_id']\n",
    "    if task_id in arc_tasks:\n",
    "        task_data = arc_tasks[task_id]\n",
    "        row['arc_train_examples'] = task_data.get('train', [])\n",
    "        row['arc_test_examples'] = task_data.get('test', [])\n",
    "        row['num_train_examples'] = len(task_data.get('train', []))\n",
    "        row['num_test_examples'] = len(task_data.get('test', []))\n",
    "    else:\n",
    "        row['arc_train_examples'] = []\n",
    "        row['arc_test_examples'] = []\n",
    "        row['num_train_examples'] = 0\n",
    "        row['num_test_examples'] = 0\n",
    "    return row\n",
    "\n",
    "final_dataset = soar_df.apply(add_arc_data, axis=1)\n",
    "\n",
    "print(f\"Rows: {len(final_dataset):,}\")\n",
    "print(f\"Unique ARC tasks: {final_dataset['task_id'].nunique()}\")\n",
    "print(f\"Models: {final_dataset['model'].value_counts().to_dict()}\")\n",
    "print(f\"Mean score: {final_dataset['score'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nColumns available:\")\n",
    "for col in final_dataset.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e56bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying dataset integrity...\n",
      "Testing that running the code on ARC test examples produces the same results as recorded in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:06<00:00, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… VERIFICATION SUCCESSFUL!\n",
      "All 103 test cases from 100 sampled rows match recorded results\n",
      "Dataset integrity confirmed - the code executions produce the expected boolean outcomes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset integrity by running code on ARC test examples\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scoring import ProgramExecutor, GridScorer\n",
    "\n",
    "print(\"Verifying dataset integrity...\")\n",
    "print(\"Testing that running the code on ARC test examples produces the same results as recorded in the dataset\")\n",
    "\n",
    "# Sample 100 random rows\n",
    "sample_size = 100\n",
    "sample_indices = random.sample(range(len(final_dataset)), min(sample_size, len(final_dataset)))\n",
    "sample_data = final_dataset.iloc[sample_indices]\n",
    "\n",
    "executor = ProgramExecutor(timeout=1.0)  # 1 second timeout\n",
    "scorer = GridScorer()\n",
    "\n",
    "verification_failures = []\n",
    "total_test_cases = 0\n",
    "\n",
    "for idx, row in tqdm(sample_data.iterrows(), total=len(sample_data), desc=\"Verifying rows\"):\n",
    "    task_id = row['task_id']\n",
    "    code = row['code']\n",
    "    arc_test_examples = row['arc_test_examples']\n",
    "    recorded_correct_test = row['correct_test_input']\n",
    "    \n",
    "    if not arc_test_examples:\n",
    "        print(f\"WARNING: No ARC test examples for task {task_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Run the code on each test example\n",
    "    for test_idx, test_example in enumerate(arc_test_examples):\n",
    "        if test_idx >= len(recorded_correct_test):\n",
    "            print(f\"WARNING: More ARC test examples than recorded results for task {task_id}\")\n",
    "            break\n",
    "            \n",
    "        test_input = test_example['input']\n",
    "        expected_output = test_example['output']\n",
    "        recorded_correct = recorded_correct_test[test_idx]\n",
    "        \n",
    "        # Execute the program\n",
    "        predicted_output, error, timed_out = executor.execute_program(code, test_input)\n",
    "        \n",
    "        if timed_out:\n",
    "            actual_correct = False\n",
    "            failure_reason = \"timeout\"\n",
    "        elif error or predicted_output is None:\n",
    "            actual_correct = False\n",
    "            failure_reason = f\"error: {error}\"\n",
    "        else:\n",
    "            # Score the prediction\n",
    "            score_result = scorer.score_grid(predicted_output, expected_output)\n",
    "            actual_correct = score_result['correct']\n",
    "            failure_reason = \"execution_successful\"\n",
    "        \n",
    "        total_test_cases += 1\n",
    "        \n",
    "        # Check if actual result matches recorded result\n",
    "        if actual_correct != recorded_correct:\n",
    "            failure_info = {\n",
    "                'row_idx': idx,\n",
    "                'task_id': task_id,\n",
    "                'test_idx': test_idx,\n",
    "                'recorded_correct': recorded_correct,\n",
    "                'actual_correct': actual_correct,\n",
    "                'failure_reason': failure_reason,\n",
    "                'error': error if error else None\n",
    "            }\n",
    "            verification_failures.append(failure_info)\n",
    "            \n",
    "            print(f\"\\nâŒ VERIFICATION FAILURE:\")\n",
    "            print(f\"   Task: {task_id}, Test case: {test_idx}\")\n",
    "            print(f\"   Recorded correct: {recorded_correct}\")\n",
    "            print(f\"   Actual correct: {actual_correct}\")\n",
    "            print(f\"   Reason: {failure_reason}\")\n",
    "            \n",
    "            # Exit immediately on first failure as requested\n",
    "            print(f\"\\nðŸ›‘ Exiting immediately due to verification failure!\")\n",
    "            print(f\"Total test cases checked: {total_test_cases}\")\n",
    "            break\n",
    "    \n",
    "    # Exit outer loop if we had a failure\n",
    "    if verification_failures:\n",
    "        break\n",
    "\n",
    "if not verification_failures:\n",
    "    print(f\"\\nâœ… VERIFICATION SUCCESSFUL!\")\n",
    "    print(f\"All {total_test_cases} test cases from {len(sample_data)} sampled rows match recorded results\")\n",
    "    print(f\"Dataset integrity confirmed - the code executions produce the expected boolean outcomes\")\n",
    "else:\n",
    "    print(f\"\\nâŒ VERIFICATION FAILED!\")\n",
    "    print(f\"Found {len(verification_failures)} mismatches in {total_test_cases} test cases\")\n",
    "    print(f\"Dataset may have integrity issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe7193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for generating SFT training examples\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from generate_training_data import create_prompt_for_task\n",
    "\n",
    "def create_training_example_from_soar_row(row):\n",
    "    \"\"\"Create a training example from a SOAR dataset row\"\"\"\n",
    "    task_id = row['task_id']\n",
    "    code = row['code']\n",
    "    arc_train_examples = row['arc_train_examples']\n",
    "    arc_test_examples = row['arc_test_examples']\n",
    "    \n",
    "    # Create task data structure\n",
    "    task_data = {\n",
    "        'train': arc_train_examples,\n",
    "        'test': arc_test_examples\n",
    "    }\n",
    "    \n",
    "    # Create system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"\n",
    "    }\n",
    "    \n",
    "    # Create user message with the prompt\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": create_prompt_for_task(task_data)\n",
    "    }\n",
    "    \n",
    "    # Create assistant message with just the program\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Final answer:\\n```python\\n{code}\\n```\"\n",
    "    }\n",
    "    \n",
    "    # Create the training example\n",
    "    training_example = {\n",
    "        \"messages\": [system_message, user_message, assistant_message],\n",
    "        \"task_id\": task_id,  # Add metadata for tracking\n",
    "        \"score\": row['score'],\n",
    "        \"model\": row['model']\n",
    "    }\n",
    "    \n",
    "    return training_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad519d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SFT training data from verified SOAR dataset...\n",
      "Valid rows with ARC examples: 40,000 out of 40,000\n",
      "âœ… Training data directory ready: ../training_data\n"
     ]
    }
   ],
   "source": [
    "# Setup directories and filter valid data\n",
    "print(\"Creating SFT training data from verified SOAR dataset...\")\n",
    "\n",
    "# Create training_data directory if it doesn't exist\n",
    "training_data_dir = Path(\"../training_data\")\n",
    "training_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Filter out rows without ARC examples\n",
    "valid_rows = final_dataset[\n",
    "    (final_dataset['num_train_examples'] > 0) & \n",
    "    (final_dataset['num_test_examples'] > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"Valid rows with ARC examples: {len(valid_rows):,} out of {len(final_dataset):,}\")\n",
    "print(f\"âœ… Training data directory ready: {training_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae937ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 40,000 training examples\n"
     ]
    }
   ],
   "source": [
    "# Create training examples from SOAR data\n",
    "\n",
    "training_examples = []\n",
    "for idx, row in valid_rows.iterrows():\n",
    "    try:\n",
    "        training_example = create_training_example_from_soar_row(row)\n",
    "        training_examples.append(training_example)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating training example for task {row['task_id']}: {e}\")\n",
    "\n",
    "print(f\"âœ… Created {len(training_examples):,} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5854588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split: 36000 training, 4000 validation\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation sets (90/10 split, balanced by task)\n",
    "\n",
    "# Set random seed for reproducible splits\n",
    "random.seed(42)\n",
    "\n",
    "# Sort by score and split\n",
    "training_examples_df = pd.DataFrame(training_examples)\n",
    "training_examples_df = training_examples_df.sort_values('score')\n",
    "\n",
    "# Group by task to ensure we don't split examples from the same task\n",
    "task_groups = training_examples_df.groupby('task_id')\n",
    "task_ids = list(task_groups.groups.keys())\n",
    "random.shuffle(task_ids)\n",
    "\n",
    "# 10% for validation (or max 500 examples)\n",
    "val_size = min(int(len(task_ids) * 0.1), 500 // 2)  # Approximate, will adjust based on examples per task\n",
    "val_task_ids = task_ids[:val_size]\n",
    "train_task_ids = task_ids[val_size:]\n",
    "\n",
    "# Split examples\n",
    "train_examples = training_examples_df[training_examples_df['task_id'].isin(train_task_ids)].to_dict('records')\n",
    "val_examples = training_examples_df[training_examples_df['task_id'].isin(val_task_ids)].to_dict('records')\n",
    "\n",
    "print(f\"âœ… Split: {len(train_examples)} training, {len(val_examples)} validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6315207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SFT datasets created successfully!\n",
      "Training data: ../training_data/soar_sft_train.jsonl (36000 examples)\n",
      "Validation data: ../training_data/soar_sft_val.jsonl (4000 examples)\n"
     ]
    }
   ],
   "source": [
    "# Clean metadata and prepare files for saving\n",
    "\n",
    "# Remove metadata before saving\n",
    "for examples in [train_examples, val_examples]:\n",
    "    for example in examples:\n",
    "        example.pop('task_id', None)\n",
    "        example.pop('score', None)\n",
    "        example.pop('model', None)\n",
    "\n",
    "train_file = training_data_dir / f\"soar_sft_train.jsonl\"\n",
    "val_file = training_data_dir / f\"soar_sft_val.jsonl\"\n",
    "\n",
    "# Save training data\n",
    "with open(train_file, 'w') as f:\n",
    "    for example in train_examples:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "# Save validation data\n",
    "with open(val_file, 'w') as f:\n",
    "    for example in val_examples:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(f\"âœ… SFT datasets created successfully!\")\n",
    "print(f\"Training data: {train_file} ({len(train_examples)} examples)\")\n",
    "print(f\"Validation data: {val_file} ({len(val_examples)} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6209d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Dataset statistics:\n",
      "Training set:\n",
      "  - Tasks: 360\n",
      "  - Score range: 0.000 - 1.000\n",
      "  - Mean score: 0.349\n",
      "  - Models: {'Qwen2.5-72B-Instruct': 23074, 'Mistral-Large-Instruct-2407': 12926}\n",
      "Validation set:\n",
      "  - Tasks: 40\n",
      "  - Score range: 0.000 - 1.000\n",
      "  - Mean score: 0.381\n",
      "  - Models: {'Qwen2.5-72B-Instruct': 2561, 'Mistral-Large-Instruct-2407': 1439}\n",
      "\n",
      "ðŸŽ¯ Files ready for SFT training!\n",
      "Example training message structure:\n",
      "  - System: 92 chars\n",
      "  - User: 2828 chars\n",
      "  - Assistant: 680 chars\n"
     ]
    }
   ],
   "source": [
    "# Display dataset statistics and summary\n",
    "\n",
    "# Recreate metadata for statistics (without modifying the saved files)\n",
    "train_df = pd.DataFrame([{\n",
    "    'task_id': row['task_id'], \n",
    "    'score': row['score'], \n",
    "    'model': row['model']\n",
    "} for row in training_examples_df[training_examples_df['task_id'].isin(train_task_ids)].to_dict('records')])\n",
    "\n",
    "val_df = pd.DataFrame([{\n",
    "    'task_id': row['task_id'], \n",
    "    'score': row['score'], \n",
    "    'model': row['model']\n",
    "} for row in training_examples_df[training_examples_df['task_id'].isin(val_task_ids)].to_dict('records')])\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset statistics:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  - Tasks: {train_df['task_id'].nunique()}\")\n",
    "print(f\"  - Score range: {train_df['score'].min():.3f} - {train_df['score'].max():.3f}\")\n",
    "print(f\"  - Mean score: {train_df['score'].mean():.3f}\")\n",
    "print(f\"  - Models: {train_df['model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"Validation set:\")\n",
    "print(f\"  - Tasks: {val_df['task_id'].nunique()}\")\n",
    "print(f\"  - Score range: {val_df['score'].min():.3f} - {val_df['score'].max():.3f}\")\n",
    "print(f\"  - Mean score: {val_df['score'].mean():.3f}\")\n",
    "print(f\"  - Models: {val_df['model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Files ready for SFT training!\")\n",
    "print(f\"Example training message structure:\")\n",
    "if train_examples:\n",
    "    # Reload one example to check structure\n",
    "    with open(train_file, 'r') as f:\n",
    "        example = json.loads(f.readline())\n",
    "    print(f\"  - System: {len(example['messages'][0]['content'])} chars\")\n",
    "    print(f\"  - User: {len(example['messages'][1]['content'])} chars\") \n",
    "    print(f\"  - Assistant: {len(example['messages'][2]['content'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ecb13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Example training data structure:\n",
      "============================================================\n",
      "System message:\n",
      "----------------------------------------\n",
      "You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\n",
      "\n",
      "User message (first 500 chars):\n",
      "----------------------------------------\n",
      "You are solving an ARC (Abstraction and Reasoning Corpus) task. \n",
      "I will show you training examples with input and output grids, plus a test input grid. Your task is to:\n",
      "\n",
      "1. **Analyze the training examples** to discover patterns that map input grids to output grids\n",
      "2. **Write a Python program** that implements your best understanding of the transformation  \n",
      "3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\n",
      "4. **Attempt a solution** - even if ...\n",
      "\n",
      "Assistant message (first 300 chars):\n",
      "----------------------------------------\n",
      "Final answer:\n",
      "```python\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = grid_lst.copy()\n",
      "    n = len(grid)\n",
      "    m = len(grid[0])\n",
      "    border_color = grid[0][0]\n",
      "    for i in range(1, n - 1):\n",
      "        for j in range(1, m - 1):\n",
      "            if grid[i][j] == 0:\n",
      "                grid[i ...\n"
     ]
    }
   ],
   "source": [
    "# Show examples of the generated training data\n",
    "print(\"ðŸ“‹ Example training data structure:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load and show a sample from the training file\n",
    "with open(train_file, 'r') as f:\n",
    "    sample_line = f.readline()\n",
    "    sample_data = json.loads(sample_line)\n",
    "\n",
    "print(\"System message:\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_data['messages'][0]['content'])\n",
    "\n",
    "print(\"\\nUser message (first 500 chars):\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_data['messages'][1]['content'][:500] + \"...\")\n",
    "\n",
    "print(\"\\nAssistant message (first 300 chars):\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_data['messages'][2]['content'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aece96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already logged in as: l3wish\n",
      "Loading JSONL files into HuggingFace Datasets...\n",
      "Dataset loaded:\n",
      "- Train: 36,000 examples\n",
      "- Validation: 4,000 examples\n",
      "- Features: ['messages']\n",
      "\n",
      "Uploading dataset to Hugging Face Hub: Trelis/soar-arc-sft-2025-07-21_01\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2c21eb8a114b2aa6938a17abac81cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc88817f0f4647449195cf90dcbcedd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ed4c1a3f0a489ea3dee0645eed0b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae2100153c141aca6670e118121af84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating dataset card...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/code/trelis-arc/llm-python/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:9692: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully uploaded dataset to: https://huggingface.co/datasets/Trelis/soar-arc-sft-2025-07-21_01\n",
      "ðŸŽ¯ Ready to use in training scripts with: load_dataset('Trelis/soar-arc-sft-2025-07-21_01')\n"
     ]
    }
   ],
   "source": [
    "# Upload the SOAR SFT dataset to Hugging Face Hub\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, login\n",
    "import json\n",
    "\n",
    "# Login to Hugging Face (if not already logged in)\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user = api.whoami()\n",
    "    print(f\"Already logged in as: {user['name']}\")\n",
    "except Exception:\n",
    "    print(\"Please log in to Hugging Face Hub:\")\n",
    "    # login()\n",
    "\n",
    "# Load the JSONL files back into datasets\n",
    "def load_jsonl_to_dataset(file_path):\n",
    "    \"\"\"Load a JSONL file into a HuggingFace Dataset\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "print(\"Loading JSONL files into HuggingFace Datasets...\")\n",
    "train_dataset = load_jsonl_to_dataset(train_file)\n",
    "val_dataset = load_jsonl_to_dataset(val_file)\n",
    "\n",
    "# Create a DatasetDict\n",
    "soar_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "# Create dataset info\n",
    "dataset_description = \"\"\"\n",
    "# SOAR ARC-AGI SFT Dataset\n",
    "\n",
    "This dataset contains verified examples from the SOAR dataset, specifically curated for supervised fine-tuning on ARC (Abstraction and Reasoning Corpus) tasks.\n",
    "\n",
    "## Dataset Details\n",
    "\n",
    "- **Total Examples**: 40,000 (36,000 train, 4,000 validation)\n",
    "- **Source Models**: Mistral-Large-Instruct-2407, Qwen2.5-72B-Instruct\n",
    "- **Task Type**: Abstract reasoning puzzles with Python code solutions\n",
    "- **Quality**: Verified dataset integrity - all code executions match recorded outcomes\n",
    "- **Format**: Standard chat format with system/user/assistant messages\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "Each example contains:\n",
    "- `messages`: List of chat messages (system, user, assistant)\n",
    "  - System: Role definition for ARC reasoning\n",
    "  - User: Full ARC task prompt with training examples and test input\n",
    "  - Assistant: Python code solution in required format\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Trelis/soar-arc-sft-...\")\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "## Citation\n",
    "\n",
    "Based on the SOAR dataset: https://huggingface.co/datasets/julien31/soar_arc_train_5M\n",
    "\n",
    "Generated and verified using the ARC-AGI 2025 pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the repository name\n",
    "repo_name = f\"Trelis/soar-arc-sft-{datetime.now().strftime('%Y-%m-%d_%H%M')}\"\n",
    "\n",
    "print(f\"\\nUploading dataset to Hugging Face Hub: {repo_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "try:\n",
    "    # Push to hub with description and tags\n",
    "    soar_dataset.push_to_hub(\n",
    "        repo_name,\n",
    "        private=True,  # Set to True if you want it private initially\n",
    "        commit_message=\"Add verified SOAR ARC-AGI SFT dataset\",\n",
    "    )\n",
    "    \n",
    "    # Update the dataset card\n",
    "    print(\"Updating dataset card...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=dataset_description.encode(),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"dataset\",\n",
    "        commit_message=\"Add dataset description\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Successfully uploaded dataset to: https://huggingface.co/datasets/{repo_name}\")\n",
    "    print(f\"ðŸŽ¯ Ready to use in training scripts with: load_dataset('{repo_name}')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error uploading dataset: {e}\")\n",
    "    print(\"You may need to:\")\n",
    "    print(\"1. Ensure you're logged in with `huggingface-cli login`\")\n",
    "    print(\"2. Have write permissions to the Trelis organization\")\n",
    "    print(\"3. Or change the repo_name to your personal account\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
