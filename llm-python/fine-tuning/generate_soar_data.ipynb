{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a2512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7919f997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/code/trelis-arc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SOAR data: 4,926,487 total rows, 1,734,164 filtered rows\n",
      "Target models: ['Mistral-Large-Instruct-2407', 'Qwen2.5-72B-Instruct']\n"
     ]
    }
   ],
   "source": [
    "# Load SOAR data and filter for target models, add score column\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"julien31/soar_arc_train_5M\", columns=['task_id', 'correct_train_input', 'correct_test_input', 'model'])\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "# Filter for target models and add score\n",
    "models_to_keep = ['Mistral-Large-Instruct-2407', 'Qwen2.5-72B-Instruct']\n",
    "df_filtered = df[df['model'].isin(models_to_keep)].copy()\n",
    "df_filtered['score'] = df_filtered.apply(\n",
    "    lambda row: (sum(row['correct_train_input']) + sum(row['correct_test_input'])) / \n",
    "                (len(row['correct_train_input']) + len(row['correct_test_input'])), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Loaded SOAR data: {len(df):,} total rows, {len(df_filtered):,} filtered rows\")\n",
    "print(f\"Target models: {models_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca32956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 ARC training tasks\n"
     ]
    }
   ],
   "source": [
    "# Get ARC training task IDs\n",
    "\n",
    "from task_loader import TaskLoader\n",
    "\n",
    "loader = TaskLoader(data_root=\"../../data\")\n",
    "training_path = loader.data_root / \"arc-agi-1\" / \"training\"\n",
    "all_training_task_ids = [f.stem for f in training_path.glob(\"*.json\")]\n",
    "\n",
    "print(f\"Found {len(all_training_task_ids)} ARC training tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7202e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selected 20000 SOAR rows from 400 ARC tasks\n",
      "Average samples per task: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Select top/bottom 25 per task and preserve original indices\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_filtered_with_idx = df_filtered.reset_index()  # Preserve original row indices\n",
    "selected_indices = []\n",
    "\n",
    "for task_id in all_training_task_ids:\n",
    "    task_rows = df_filtered_with_idx[df_filtered_with_idx['task_id'] == task_id]\n",
    "    if len(task_rows) == 0:\n",
    "        continue\n",
    "    \n",
    "    task_rows_sorted = task_rows.sort_values('score', ascending=True)\n",
    "    bottom_25 = task_rows_sorted.head(25)\n",
    "    top_25 = task_rows_sorted.tail(25)\n",
    "    selected_rows = pd.concat([bottom_25, top_25])\n",
    "    selected_indices.extend(selected_rows['index'].tolist())\n",
    "\n",
    "print(f\" Selected {len(selected_indices)} SOAR rows from {len(all_training_task_ids)} ARC tasks\")\n",
    "print(f\"Average samples per task: {len(selected_indices) / len(all_training_task_ids):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c2e568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full SOAR data: 20,000 rows\n",
      "Columns: ['code', 'correct_train_input', 'predicted_train_output', 'correct_test_input', 'predicted_test_output', 'task_id', 'model', 'generation', 'score']... (9 total)\n"
     ]
    }
   ],
   "source": [
    "# Load full SOAR data for selected rows only\n",
    "\n",
    "ds_full = load_dataset(\"julien31/soar_arc_train_5M\")\n",
    "selected_dataset = ds_full['train'].select(selected_indices)\n",
    "soar_df = selected_dataset.to_pandas()\n",
    "\n",
    "# Add score column to full dataset\n",
    "soar_df['score'] = soar_df.apply(\n",
    "    lambda row: (sum(row['correct_train_input']) + sum(row['correct_test_input'])) / \n",
    "                (len(row['correct_train_input']) + len(row['correct_test_input'])), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Loaded full SOAR data: {len(soar_df):,} rows\")\n",
    "print(f\"Columns: {list(soar_df.columns)}... ({len(soar_df.columns)} total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d19612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corresponding ARC task data...\n",
      "Loaded 400 ARC tasks\n"
     ]
    }
   ],
   "source": [
    "# Load corresponding ARC task data\n",
    "print(\"Loading corresponding ARC task data...\")\n",
    "\n",
    "arc_tasks = {}\n",
    "\n",
    "for task_id in soar_df['task_id'].unique():\n",
    "    task_data = loader.load_task(task_id, \"arc-agi-1\")\n",
    "    arc_tasks[task_id] = task_data\n",
    "\n",
    "print(f\"Loaded {len(arc_tasks)} ARC tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871b20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 20,000\n",
      "Unique ARC tasks: 400\n",
      "Models: {'Qwen2.5-72B-Instruct': 12596, 'Mistral-Large-Instruct-2407': 7404}\n",
      "Mean score: 0.399\n"
     ]
    }
   ],
   "source": [
    "# Merge SOAR and ARC data\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def convert_soar_to_list(grid):\n",
    "    \"\"\"Convert SOAR prediction format to Python list of lists\"\"\"\n",
    "    if grid is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # SOAR format: numpy array of numpy arrays (dtype=object)\n",
    "        if isinstance(grid, np.ndarray):\n",
    "            if grid.dtype == object:\n",
    "                # This is an array of row arrays - convert each row\n",
    "                return [row.tolist() for row in grid]\n",
    "            else:\n",
    "                # Regular 2D numpy array\n",
    "                return grid.tolist()\n",
    "        elif isinstance(grid, list):\n",
    "            # Already a list, ensure proper nesting\n",
    "            if all(isinstance(row, (list, np.ndarray)) for row in grid):\n",
    "                return [row.tolist() if isinstance(row, np.ndarray) else row for row in grid]\n",
    "            else:\n",
    "                return grid\n",
    "        else:\n",
    "            return grid\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error converting SOAR grid: {e}\")\n",
    "        print(f\"Grid type: {type(grid)}, content: {grid}\")\n",
    "        return grid\n",
    "\n",
    "def add_arc_data(row):\n",
    "    task_id = row['task_id']\n",
    "    if task_id in arc_tasks:\n",
    "        task_data = arc_tasks[task_id]\n",
    "        \n",
    "        # Deep copy the original ARC data to avoid modifying the source\n",
    "        arc_train_examples = copy.deepcopy(task_data.get('train', []))\n",
    "        arc_test_examples = copy.deepcopy(task_data.get('test', []))\n",
    "        \n",
    "        # Override outputs with SOAR predicted outputs for hindsight relabelling (convert to lists)\n",
    "        predicted_train_outputs = row['predicted_train_output']\n",
    "        predicted_test_outputs = row['predicted_test_output']\n",
    "        \n",
    "        # Update train examples with predicted outputs \n",
    "        for i, predicted_output in enumerate(predicted_train_outputs):\n",
    "            if i < len(arc_train_examples):\n",
    "                arc_train_examples[i]['output'] = convert_soar_to_list(predicted_output)\n",
    "        \n",
    "        # Update test examples with predicted outputs\n",
    "        for i, predicted_output in enumerate(predicted_test_outputs):\n",
    "            if i < len(arc_test_examples):\n",
    "                arc_test_examples[i]['output'] = convert_soar_to_list(predicted_output)\n",
    "        \n",
    "        row['arc_train_examples'] = arc_train_examples\n",
    "        row['arc_test_examples'] = arc_test_examples\n",
    "        row['num_train_examples'] = len(arc_train_examples)\n",
    "        row['num_test_examples'] = len(arc_test_examples)\n",
    "    else:\n",
    "        row['arc_train_examples'] = []\n",
    "        row['arc_test_examples'] = []\n",
    "        row['num_train_examples'] = 0\n",
    "        row['num_test_examples'] = 0\n",
    "    return row\n",
    "\n",
    "# Re-apply the merging with the corrected function\n",
    "final_dataset = soar_df.apply(add_arc_data, axis=1)\n",
    "\n",
    "print(f\"Rows: {len(final_dataset):,}\")\n",
    "print(f\"Unique ARC tasks: {final_dataset['task_id'].nunique()}\")\n",
    "print(f\"Models: {final_dataset['model'].value_counts().to_dict()}\")\n",
    "print(f\"Mean score: {final_dataset['score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e56bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying dataset integrity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:29<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: All 431 examples verified correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset integrity - check that code produces overridden outputs correctly\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scoring import ProgramExecutor, GridScorer\n",
    "import numpy as np\n",
    "\n",
    "def grids_equal(grid1, grid2):\n",
    "    \"\"\"Compare two grids for exact equality, handling various data types\"\"\"\n",
    "    if grid1 is None or grid2 is None:\n",
    "        return grid1 == grid2\n",
    "    \n",
    "    try:\n",
    "        # Convert to numpy arrays for comparison\n",
    "        arr1 = np.array(grid1)\n",
    "        arr2 = np.array(grid2)\n",
    "        \n",
    "        # Check shapes first\n",
    "        if arr1.shape != arr2.shape:\n",
    "            return False\n",
    "            \n",
    "        # Check element-wise equality\n",
    "        return np.array_equal(arr1, arr2)\n",
    "    except Exception:\n",
    "        # Fallback to list comparison\n",
    "        return grid1 == grid2\n",
    "\n",
    "print(\"Verifying dataset integrity...\")\n",
    "\n",
    "# Sample 100 random rows\n",
    "sample_size = 100\n",
    "sample_indices = random.sample(range(len(final_dataset)), min(sample_size, len(final_dataset)))\n",
    "sample_data = final_dataset.iloc[sample_indices]\n",
    "\n",
    "executor = ProgramExecutor(timeout=1.0)\n",
    "scorer = GridScorer()\n",
    "\n",
    "total_checks = 0\n",
    "successful_checks = 0\n",
    "\n",
    "for idx, row in tqdm(sample_data.iterrows(), total=len(sample_data), desc=\"Verifying\"):\n",
    "    task_id = row['task_id']\n",
    "    code = row['code']\n",
    "    arc_train_examples = row['arc_train_examples']\n",
    "    arc_test_examples = row['arc_test_examples']\n",
    "    \n",
    "    if not arc_test_examples:\n",
    "        print(f\"ERROR: No ARC test examples for task {task_id}\")\n",
    "        break\n",
    "    \n",
    "    # Check all examples (train + test)\n",
    "    all_examples = [\n",
    "        ('train', i, example) for i, example in enumerate(arc_train_examples)\n",
    "    ] + [\n",
    "        ('test', i, example) for i, example in enumerate(arc_test_examples)\n",
    "    ]\n",
    "    \n",
    "    for example_type, example_idx, example in all_examples:\n",
    "        input_grid = example['input']\n",
    "        expected_output = example['output']\n",
    "        \n",
    "        # Execute the program\n",
    "        predicted_output, error, timed_out = executor.execute_program(code, input_grid)\n",
    "        \n",
    "        total_checks += 1\n",
    "        \n",
    "        # Check for execution errors\n",
    "        if timed_out or error or predicted_output is None:\n",
    "            print(f\"EXECUTION ERROR: Task {task_id}, {example_type} example {example_idx}\")\n",
    "            print(f\"Error: {error}\")\n",
    "            print(f\"Timed out: {timed_out}\")\n",
    "            break\n",
    "        \n",
    "        # Check if outputs match\n",
    "        if not grids_equal(predicted_output, expected_output):\n",
    "            print(f\"OUTPUT MISMATCH: Task {task_id}, {example_type} example {example_idx}\")\n",
    "            print(f\"Expected shape: {len(expected_output)}x{len(expected_output[0]) if expected_output else 0}\")\n",
    "            print(f\"Got shape: {len(predicted_output)}x{len(predicted_output[0]) if predicted_output else 0}\")\n",
    "            break\n",
    "        \n",
    "        successful_checks += 1\n",
    "    else:\n",
    "        continue  # Continue outer loop if inner loop completed without break\n",
    "    break  # Exit outer loop if inner loop broke\n",
    "\n",
    "# Final result\n",
    "if successful_checks == total_checks and total_checks > 0:\n",
    "    print(f\"SUCCESS: All {total_checks} examples verified correctly\")\n",
    "else:\n",
    "    print(f\"FAILED: {successful_checks}/{total_checks} examples matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe7193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for generating SFT training examples\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from generate_training_data import create_prompt_for_task\n",
    "\n",
    "def create_training_example_from_soar_row(row):\n",
    "    \"\"\"Create a training example from a SOAR dataset row\"\"\"\n",
    "    task_id = row['task_id']\n",
    "    code = row['code']\n",
    "    arc_train_examples = row['arc_train_examples']\n",
    "    arc_test_examples = row['arc_test_examples']\n",
    "    \n",
    "    # Create task data structure\n",
    "    task_data = {\n",
    "        'train': arc_train_examples,\n",
    "        'test': arc_test_examples\n",
    "    }\n",
    "    \n",
    "    # Create system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"\n",
    "    }\n",
    "    \n",
    "    # Create user message with the prompt\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": create_prompt_for_task(task_data)\n",
    "    }\n",
    "    \n",
    "    # Create assistant message with just the program\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Final answer:\\n```python\\n{code}\\n```\"\n",
    "    }\n",
    "    \n",
    "    # Create the training example\n",
    "    training_example = {\n",
    "        \"messages\": [system_message, user_message, assistant_message],\n",
    "        \"task_id\": task_id,  # Add metadata for tracking\n",
    "        \"score\": row['score'],\n",
    "        \"model\": row['model']\n",
    "    }\n",
    "    \n",
    "    return training_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae937ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 20,000 training examples\n"
     ]
    }
   ],
   "source": [
    "# Create training examples from SOAR data\n",
    "\n",
    "training_examples = []\n",
    "for idx, row in final_dataset.iterrows():\n",
    "    try:\n",
    "        training_example = create_training_example_from_soar_row(row)\n",
    "        training_examples.append(training_example)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating training example for task {row['task_id']}: {e}\")\n",
    "\n",
    "print(f\"âœ… Created {len(training_examples):,} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5854588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation size: 20 tasks\n",
      "âœ… Split: 19000 training, 1000 validation\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation sets (90/10 split, balanced by task)\n",
    "\n",
    "# Set random seed for reproducible splits\n",
    "random.seed(42)\n",
    "\n",
    "# Sort by score and split\n",
    "training_examples_df = pd.DataFrame(training_examples)\n",
    "training_examples_df = training_examples_df.sort_values('score')\n",
    "\n",
    "# Group by task to ensure we don't split examples from the same task\n",
    "task_groups = training_examples_df.groupby('task_id')\n",
    "task_ids = list(task_groups.groups.keys())\n",
    "random.shuffle(task_ids)\n",
    "\n",
    "# 10% for validation (or max 20 tasks)\n",
    "val_size = min(int(len(task_ids) * 0.1), 20)  # Approximate, will adjust based on examples per task\n",
    "print(f\"Validation size: {val_size} tasks\")\n",
    "val_task_ids = task_ids[:val_size]\n",
    "train_task_ids = task_ids[val_size:]\n",
    "\n",
    "# Split examples\n",
    "train_examples = training_examples_df[training_examples_df['task_id'].isin(train_task_ids)].to_dict('records')\n",
    "val_examples = training_examples_df[training_examples_df['task_id'].isin(val_task_ids)].to_dict('records')\n",
    "\n",
    "print(f\"âœ… Split: {len(train_examples)} training, {len(val_examples)} validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6315207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SFT datasets created successfully!\n",
      "Training data: ../training_data/soar_sft_train.jsonl (19000 examples)\n",
      "Validation data: ../training_data/soar_sft_val.jsonl (1000 examples)\n"
     ]
    }
   ],
   "source": [
    "# Clean metadata and prepare files for saving\n",
    "\n",
    "training_data_dir = Path(\"../training_data\")\n",
    "\n",
    "# Remove metadata before saving\n",
    "for examples in [train_examples, val_examples]:\n",
    "    for example in examples:\n",
    "        example.pop('task_id', None)\n",
    "        example.pop('score', None)\n",
    "        example.pop('model', None)\n",
    "\n",
    "train_file = training_data_dir / f\"soar_sft_train.jsonl\"\n",
    "val_file = training_data_dir / f\"soar_sft_val.jsonl\"\n",
    "\n",
    "# Save training data\n",
    "with open(train_file, 'w') as f:\n",
    "    for example in train_examples:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "# Save validation data\n",
    "with open(val_file, 'w') as f:\n",
    "    for example in val_examples:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(f\"âœ… SFT datasets created successfully!\")\n",
    "print(f\"Training data: {train_file} ({len(train_examples)} examples)\")\n",
    "print(f\"Validation data: {val_file} ({len(val_examples)} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6209d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Dataset statistics:\n",
      "Training set:\n",
      "  - Tasks: 380\n",
      "  - Score range: 0.000 - 1.000\n",
      "  - Mean score: 0.400\n",
      "  - Models: {'Qwen2.5-72B-Instruct': 11931, 'Mistral-Large-Instruct-2407': 7069}\n",
      "Validation set:\n",
      "  - Tasks: 20\n",
      "  - Score range: 0.000 - 1.000\n",
      "  - Mean score: 0.375\n",
      "  - Models: {'Qwen2.5-72B-Instruct': 665, 'Mistral-Large-Instruct-2407': 335}\n",
      "\n",
      "ðŸŽ¯ Files ready for SFT training!\n",
      "Example training message structure:\n",
      "  - System: 92 chars\n",
      "  - User: 2478 chars\n",
      "  - Assistant: 821 chars\n"
     ]
    }
   ],
   "source": [
    "# Display dataset statistics and summary\n",
    "\n",
    "# Recreate metadata for statistics (without modifying the saved files)\n",
    "train_df = pd.DataFrame([{\n",
    "    'task_id': row['task_id'], \n",
    "    'score': row['score'], \n",
    "    'model': row['model']\n",
    "} for row in training_examples_df[training_examples_df['task_id'].isin(train_task_ids)].to_dict('records')])\n",
    "\n",
    "val_df = pd.DataFrame([{\n",
    "    'task_id': row['task_id'], \n",
    "    'score': row['score'], \n",
    "    'model': row['model']\n",
    "} for row in training_examples_df[training_examples_df['task_id'].isin(val_task_ids)].to_dict('records')])\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset statistics:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  - Tasks: {train_df['task_id'].nunique()}\")\n",
    "print(f\"  - Score range: {train_df['score'].min():.3f} - {train_df['score'].max():.3f}\")\n",
    "print(f\"  - Mean score: {train_df['score'].mean():.3f}\")\n",
    "print(f\"  - Models: {train_df['model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"Validation set:\")\n",
    "print(f\"  - Tasks: {val_df['task_id'].nunique()}\")\n",
    "print(f\"  - Score range: {val_df['score'].min():.3f} - {val_df['score'].max():.3f}\")\n",
    "print(f\"  - Mean score: {val_df['score'].mean():.3f}\")\n",
    "print(f\"  - Models: {val_df['model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Files ready for SFT training!\")\n",
    "print(f\"Example training message structure:\")\n",
    "if train_examples:\n",
    "    # Reload one example to check structure\n",
    "    with open(train_file, 'r') as f:\n",
    "        example = json.loads(f.readline())\n",
    "    print(f\"  - System: {len(example['messages'][0]['content'])} chars\")\n",
    "    print(f\"  - User: {len(example['messages'][1]['content'])} chars\") \n",
    "    print(f\"  - Assistant: {len(example['messages'][2]['content'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ecb13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Example training data structure:\n",
      "============================================================\n",
      "System message:\n",
      "----------------------------------------\n",
      "You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\n",
      "\n",
      "User message (first 500 chars):\n",
      "----------------------------------------\n",
      "You are solving an ARC (Abstraction and Reasoning Corpus) task. \n",
      "I will show you training examples with input and output grids, plus a test input grid. Your task is to:\n",
      "\n",
      "1. **Analyze the training examples** to discover patterns that map input grids to output grids\n",
      "2. **Write a Python program** that implements your best understanding of the transformation  \n",
      "3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\n",
      "4. **Attempt a solution** - even if ...\n",
      "\n",
      "Assistant message (first 300 chars):\n",
      "----------------------------------------\n",
      "Final answer:\n",
      "```python\n",
      "def transform(grid):\n",
      "\n",
      "    def evaluate_subgrid(subgrid):\n",
      "        score = sum((sum(row) for row in subgrid))\n",
      "        center = subgrid[1][1]\n",
      "        if center != 0:\n",
      "            score += 10\n",
      "        cross_points = [(0, 1), (1, 0), (1, 2), (2, 1)]\n",
      "        cross_score = sum((subgri...\n"
     ]
    }
   ],
   "source": [
    "# Show examples of the generated training data\n",
    "print(\"ðŸ“‹ Example training data structure:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load and show a sample from the training file\n",
    "with open(train_file, 'r') as f:\n",
    "    sample_line = f.readline()\n",
    "    sample_data = json.loads(sample_line)\n",
    "\n",
    "print(\"System message:\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_data['messages'][0]['content'])\n",
    "\n",
    "print(\"\\nUser message (first 500 chars):\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_data['messages'][1]['content'][:500] + \"...\")\n",
    "\n",
    "print(\"\\nAssistant message (first 300 chars):\")\n",
    "print(\"-\" * 40)\n",
    "print(sample_data['messages'][2]['content'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13aece96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already logged in as: l3wish\n",
      "Loading JSONL files into HuggingFace Datasets...\n",
      "\n",
      "Uploading dataset to Hugging Face Hub: Trelis/soar-arc-sft-2025-07-23_0859\n",
      "This may take a few minutes...\n",
      "\n",
      "Uploading dataset to Hugging Face Hub: Trelis/soar-arc-sft-2025-07-23_0859\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 299.94ba/s]\n",
      "\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.04s/ shards]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.04s/ shards]s]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 202.52ba/s]\n",
      "\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.32s/ shards]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating dataset card...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/code/trelis-arc/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:9692: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully uploaded dataset to: https://huggingface.co/datasets/Trelis/soar-arc-sft-2025-07-23_0859\n",
      "ðŸŽ¯ Ready to use in training scripts with: load_dataset('Trelis/soar-arc-sft-2025-07-23_0859')\n"
     ]
    }
   ],
   "source": [
    "# Upload the SOAR SFT dataset to Hugging Face Hub\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, login\n",
    "import json\n",
    "\n",
    "# Login to Hugging Face (if not already logged in)\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user = api.whoami()\n",
    "    print(f\"Already logged in as: {user['name']}\")\n",
    "except Exception:\n",
    "    print(\"Please log in to Hugging Face Hub:\")\n",
    "    # login()\n",
    "\n",
    "# Load the JSONL files back into datasets\n",
    "def load_jsonl_to_dataset(file_path):\n",
    "    \"\"\"Load a JSONL file into a HuggingFace Dataset\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "print(\"Loading JSONL files into HuggingFace Datasets...\")\n",
    "train_dataset = load_jsonl_to_dataset(train_file)\n",
    "val_dataset = load_jsonl_to_dataset(val_file)\n",
    "\n",
    "# Create a DatasetDict\n",
    "soar_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "# Create dataset info\n",
    "dataset_description = \"\"\"\n",
    "# SOAR ARC-AGI SFT Dataset\n",
    "\n",
    "This dataset contains verified examples from the SOAR dataset, specifically curated for supervised fine-tuning on ARC (Abstraction and Reasoning Corpus) tasks.\n",
    "\n",
    "## Dataset Details\n",
    "\n",
    "- **Total Examples**: 40,000 (36,000 train, 4,000 validation)\n",
    "- **Source Models**: Mistral-Large-Instruct-2407, Qwen2.5-72B-Instruct\n",
    "- **Task Type**: Abstract reasoning puzzles with Python code solutions\n",
    "- **Quality**: Verified dataset integrity - all code executions match recorded outcomes\n",
    "- **Format**: Standard chat format with system/user/assistant messages\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "Each example contains:\n",
    "- `messages`: List of chat messages (system, user, assistant)\n",
    "  - System: Role definition for ARC reasoning\n",
    "  - User: Full ARC task prompt with training examples and test input\n",
    "  - Assistant: Python code solution in required format\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Trelis/soar-arc-sft-...\")\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "## Citation\n",
    "\n",
    "Based on the SOAR dataset: https://huggingface.co/datasets/julien31/soar_arc_train_5M\n",
    "\n",
    "Generated and verified using the ARC-AGI 2025 pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the repository name\n",
    "repo_name = f\"Trelis/soar-arc-sft-{datetime.now().strftime('%Y-%m-%d_%H%M')}\"\n",
    "\n",
    "print(f\"\\nUploading dataset to Hugging Face Hub: {repo_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "try:\n",
    "    # Push to hub with description and tags\n",
    "    soar_dataset.push_to_hub(\n",
    "        repo_name,\n",
    "        private=False,  # Set to True if you want it private initially\n",
    "        commit_message=\"Add verified SOAR ARC-AGI SFT dataset\",\n",
    "    )\n",
    "    \n",
    "    # Update the dataset card\n",
    "    print(\"Updating dataset card...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=dataset_description.encode(),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"dataset\",\n",
    "        commit_message=\"Add dataset description\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Successfully uploaded dataset to: https://huggingface.co/datasets/{repo_name}\")\n",
    "    print(f\"ðŸŽ¯ Ready to use in training scripts with: load_dataset('{repo_name}')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error uploading dataset: {e}\")\n",
    "    print(\"You may need to:\")\n",
    "    print(\"1. Ensure you're logged in with `huggingface-cli login`\")\n",
    "    print(\"2. Have write permissions to the Trelis organization\")\n",
    "    print(\"3. Or change the repo_name to your personal account\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trelis-arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
