# Notes on the SOAR Paper

## Questions During Review
- How many samples are needed before there is duplication of programs generated?
- How was the training set generated? Using the ARC-AGI-1 training dataset only?
- How many rows of data were used for fine-tuning?
- What single ablation gives the greatest improvement in performance?
- Does a "training iteration" involve running the model again on the same data to get a new set of samples? Are samples from iteration 1 thrown away?

## Summary of Performance Improvements

Based on the ARC-AGI-1 test set:
- Sampling brings open source models from from 1-2% to 14-26%
- After four training iterations, SOAR solves an extra 10-19% tasks across model sizes. SOAR includes improving the sampling AND improving the feedback.
- Given access to target tasks, but not to their ground truth solutions, SOAR learns to solve an extra 3-5% tasks across model sizes with test-time training.

So, SOAR at train and at test time brings performance from 1-2% up to 14+10+3=27% to 26+19+5=50%, 27-50%. The paper reaches 52%, gaining some further benefit from cross training across samples from different models.

And here is the overall improvement allowing for i) training data generated by stronger models and ii) test time training:
```markdown
Our 7B model improved from its initial 14.25% to 36.25% accuracy on ARC-test, a 2.5 fold increase.
```

### Replication Notes on ARC AGI 1 Evaluation Set.
- Qwen4 with no thinking, single shot: 0.5%
- Qwen4 with no thinking, 8-shot: 0.5%
- Qwen4 with thinking, single shot: 

---

## Other Notes

### Re the idea of hindsight relabelling

Actually this comes from [Butt et al.](https://arxiv.org/pdf/2402.04858). 

### Regarding ensembling for generation of the test output.

**I'm confused by this, it would seem that programs that correctly predict all training examples such be much more strongly favoured.**

[ChatGPT here](https://chatgpt.com/share/687a1896-0828-8003-920a-46977dca1a94)

Here is how the final test output is determined in SOAR:
```markdown
Ensembling with weighted majority voting. We start
with 6k candidate programs (3k from sampling, 3k from
refinement). Each program is evaluated on the ARC taskâ€™s
input-output examples to compute its example accuracy, and
is also run on the test input to produce an output grid. We
then group programs by their test output grid and assign
each unique grid a score: the sum of example accuracies of
all programs that produced it. This gives us a weighted vote
over test outputs, favoring grids produced by more accurate
programs (see Appendix D.1 for more details). This en-
sembling approach helps mitigate individual program errors
while capturing common patterns across successful solu-
tions. We eventually return two candidate solutions, as per
the benchmark rules (Chollet, 2024).
```

Note also this later comment:
```markdown
However, majority
voting is not an ideal aggregation strategy; we observed
an average score gap of 9.5% between majority voting and
oracle performance across our models, where the oracle
is defined as a task being solved if at least one solution
produces the correct output. This gap indicates room for
improvement in developing better ensembling methods.
```

### Re the filtering of samples to use for fine-tuning

**It seems like there is test leakage in the filtering approach, but this is only done for initial training, which is fine. Also, it seems odd to rank based on train + test examples without giving much higher weight to programs that get all correct.**

For each fine-tuning iteration, SOAR samples 3000 programs per task.

These are then ranked according to train AND test score (so there is test leakage):
```markdown
This is done by ranking solutions according to their
accuracy on input-output examples and test pairs, then sam-
pling 25 top performing solutions (greedy approach), then
sampling 25 bottom performing solutions to introduce some
diversity in the set of relabelled problem-solution pairs.
```

>[!TIP]
>Consider ablating this ensembling versus just choosing the first program that gets all train grids correct (a drawback here is if the program is overfitting the training grids by hardcoding information into the program).

### Re the filtering of samples to use for fine-tuning

**Here too, there seems to be test leakage, which is fine for pre-training but not for test-time - SOAR does not fine-tune on refinement examples at test time - although the papers suggests they could. Relatedly, the most correcct pre-training thing to do here is likely to ensure correctness on all train + on the test example for a given task (need to check the code if this is the case).**

The paper is a bit unclear but it appears that samples are filtered based on test-output matching ground truth (but possibly considering train examples too? need to check the code).

```markdown
For tasks in ARC-train where we have access to
ground truth outputs, we can identify correct refinements:
cases where an incorrect program f was successfully re-
fined into a correct program f+.
```

### Regarding sampling and relabelling

ALL samples are relabelled. For samples correct on all training examples, this relabelling does nothing, but it means there is some signal provided by a small number of entirely correct programs on training examples. [Note that Ronan's testing prior to July 18th did not include these.]

## Regarding fine-tuning of sampling vs of refinement
Provided you sample, it seems that fine-tuning the refinement gives a larger performance improvement (on ARC-train) than fine-tuning with re-labelling.

| Method | Sample-3k acc | Sample&Refine |
|--------|---------------|---------------|
| no finetuning | 29.67 | 34.83 |
| finetune: uniform | - | 42.67 |
| finetune: diverse | - | 42.88 |

**Table 3.** Improvement of LLM program refinement efficiency after finetuning: ARC-train performance of Sample&Refine search methods (6k budget) using a non-finetuned Qwen-2.5-Coder-14B model in the sampling step before refining sampled solutions with different Qwen-2.5-Coder-14B models finetuned for program refinement (% solved).

| Method | Sample-3k acc |
|--------|---------------|
| no finetuning | 29.29 |
| finetune: correct-only | 34.67 |
| finetune: uniform | 32.38 |
| finetune: greedy | 34.3 |
| finetune: greedy-diverse | 36.46 |

**Table 2.** Improvement of LLM program sampling efficiency after finetuning. ARC-train performance after sampling 3k samples with Qwen-2.5-Coder-14B models finetuned for program sampling (% solved).

>[!TIP]
>Possibly focusing first on refinement fine-tuning is a bigger boost than doing hindsight relabelling to improve sampling. Perhaps even doing hindsight relabelling on the feedback stage makes more sense than trying to improve sampling - although doing feedback requires setting up a MCTS/one-armed bandit type approach.

## You get a big boost - at least on train data - from a strong teacher model

| Model size | SOAR(3 train) | SOAR(all train) |
|------------|---------------|-----------------|
| QC-7B | 19.9 | 33.0 |
| QC-14B | 24.5 | 39.1 |
| QC-32B | 28.0 | 41.1 |
| Q-72B | 34.6 | 39.8 |
| Mistral-Large-123B | 28.5 | 40.1 |

**Table 5.** ARC-test accuracy after training base models on a subset of 1) the data obtained at the 2nd SOAR iteration using that same model size, SOAR(3 train); 2) all data collected by all models and all previous train iterations SOAR(all train).

Keep in mind that the sampling approach is 25 greedy + 25 diverse (bad), so sampling across data from multiple iterations and models (which is what is happening in (all train) is going to mostly sample from the strong model for the greedy samples. My guess is that the 25 diverse samples would better also be taken from the stronger model (rather than from the whole dataset).

>[!TIP]
>Consider sampling training data only from a strong teacher model.

## SOAR makes small models very sample efficient

While an un-tuned model needs ~1000 samples to saturate in performance, SOAR seems to saturate with hundres of examples and SOAR with ttt seems to saturate with just 10s of examples!!!

>[!TIP]
>This is a key efficiency insight, it suggests a benefit of gathering data from very strong models and using those to tune the small model, then you move to TTT - and possibly all of this can be done quite sample efficiently then.

![SOAR effect of TTT](soar-effect-of-ttt.png)

## Fastest path to a leading Kaggle leaderboard position?

### Phase A:
A key graph is ![SOAR-teacher-effects.png](SOAR-teacher-effects.png) because it is the closest thing to us:
1. Taking a a strong model (e.g. o3, or models) and generating samples.
2. Using those samples to train the small model - even if only on sampling, no refinement.
3. Doing TTT to further improve performance.

All of this likely requires max sampling of ~32 per task to get most of the benefit.

This should bring us to better than 30%, maybe higher (as Qwen3 is stronger than 2.5, and o3 will give better data than the paper gets from open source models), on ARC-AGI-1.

Open Questions:
1. Do we generate assistant responses that only consist of a program OR whether it's best to have some explaining text before that (I don't mean reasoning). Probably best to return only a program because any explanation may be wrong when we do hindsight relabelling. (of course, one would think that adding an explanation before may help.)
2. Do we strip comments from the code (or just prompt not to include any code comments)? May be beneficial to strip them because, again, for hindsight relabelling, comments may be wrong. (of course, one would think that code comments help to get more answers correct, we could ablate either way.)
3. How do we de-duplicate samples?
    - Dedup exact matches (which may be few).
    - Dedup programs that give the same train outputs (we can print how many dedups are of programs where all train outputs are the same).
4. How to generate the data?
- Use the ARC-AGI-1 training data.
- Models to run (cheap first cut):
    - kimi k2 - single shot
    - o4-mini - single shot
    - claude 3.5 sonnet - single shot
    - deepseek r1 0506 - single shot
    - qwen3 4b, no thinking - 4-shot.
    => Gives a dataset of 8 rows per task.
5. Do we use ground truth data for filtering? Probably not, the dataset is anyway small so any programs that get a program correct will be included.
6. Open question - how might reasoning capabilities be maintained? Perhaps by including lots of reasoning traces in the examples - for where a test problem is correct.

This should solve a lot of the train examples when we then run Qwen3 4b with 8-shots. We can add that data to the pool, dedup and train again. Probably this brings a smaller incremental improvement, both on train and on evaluation.

We can then do test time training on the evaluation set to see how high we can score.

At this point, potentially we can go to phase B OR we could aim to integrate reasoning into the program generation.

### Phase B:
We could then try adding refinement, then with hindsight re-labelling to see if performance can improve further again.

Hard to know where this gets us, maybe another 10%?

The harder question then is how this scores on ARC-AGI-2.

### Phase C:
Move to python program intermediation, where we train on intermediate programs. (more to be figured out on how this might work).)


