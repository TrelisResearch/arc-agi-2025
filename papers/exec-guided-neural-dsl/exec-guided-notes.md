# Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing
Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning

By Simon Ouellette

See ChatGPT [here](https://chatgpt.com/share/688244f3-5d88-8003-925c-82c7de91eb6f) for a summary of the paper.

Ronan's notes:
- Training data is generated by rolling forward on primitives.
- Integrates a feedback step in DSL (potentially more powerful than just re-compounding primitives like dreamcoder).
- Primitives are incomplete and it's non obvious how to make them complete.
- It's a from scratch approach so you don't get to bootstrap with what an LLM has.
- The approach is adaptive as it has feedback, but not creative in the sense of being able to adapt to new types of problems. 