#!/usr/bin/env python3
import subprocess
import sys
import os
import argparse
import signal
import time
import requests
import select
from dotenv import load_dotenv
import sys
sys.path.insert(0, os.path.dirname(__file__))
from runpod_utils import delete_pod

load_dotenv()


def wait_for_openai_endpoint(base_url, model_name, timeout=600, check_interval=10):
    """Wait for the OpenAI-compatible endpoint to be ready"""
    print(f"\nüß† Waiting for OpenAI endpoint at {base_url} to be ready...")
    
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            import openai
            client = openai.OpenAI(
                api_key="dummy",  # RunPod doesn't need real key
                base_url=base_url
            )
            
            client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10,
                timeout=10
            )
            print("‚úÖ OpenAI endpoint is ready!")
            return True
        except Exception as e:
            elapsed = int(time.time() - start_time)
            print(f"‚è≥ Endpoint not ready yet... ({elapsed}s elapsed)")
            time.sleep(check_interval)
    
    print(f"‚ùå Endpoint failed to become ready after {timeout}s")
    return False

def prompt_keep_pod(timeout=10):
    """Prompt user whether to keep pod running, with timeout"""
    print(f"\nüí∞ Keep pod running for manual use? [y/N] (timeout in {timeout}s): ", end="", flush=True)
    
    # Use select for timeout on Unix systems
    try:
        if hasattr(select, 'select'):
            ready, _, _ = select.select([sys.stdin], [], [], timeout)
            if ready:
                response = sys.stdin.readline().strip().lower()
                return response.startswith('y')
            else:
                print("\n‚è∞ Timeout - defaulting to No (will delete pod)")
                return False
        else:
            # Fallback for Windows - no timeout
            response = input().strip().lower()
            return response.startswith('y')
    except Exception:
        return False

def run_arc_tasks_with_graceful_handling(dataset, model_path, base_url, subset="all_evaluation", max_attempts=64, no_transductive_penalty=False, max_workers=32, splitter=False, max_tokens=2000, reasoning_effort="low", refinement_ds=None, early_stop_threshold=None, rex_stats=False):
    """Run ARC tasks - task runner handles its own graceful shutdown"""
    print(f"\nüéØ Running ARC tasks for {dataset} with subset {subset}...")
    print(f"üìä Task Runner Configuration:")
    print(f"   Max workers: {max_workers}")
    print(f"   Max attempts: {max_attempts}")
    print(f"   Max tokens: {max_tokens}")
    print(f"   Reasoning effort: {reasoning_effort}")
    
    # Check if this is a Qwen model
    is_qwen_model = 'qwen' in model_path.lower() and 'thinking' not in model_path.lower()
    if is_qwen_model:
        print(f"   Qwen thinking: DISABLED (--qwen-no-think)")
    
    print(f"   Executor: UNSAFE (runs directly, no sandboxing)")
    if no_transductive_penalty:
        print(f"   Transductive penalty: DISABLED")
    if splitter:
        print(f"   Training data splitter: ENABLED (randomly selecting & shuffling training examples)")
    if refinement_ds:
        print(f"   Refinement mode: ENABLED (using programs from {refinement_ds})")
    if early_stop_threshold:
        print(f"   Early stop threshold: {early_stop_threshold}")
    if rex_stats:
        print(f"   REx stats: ENABLED")
    
    cmd = [
        "uv", "run", "python", "-m", "llm_python.run_arc_tasks_soar",
        "--dataset", dataset,
        "--subset", subset,
        "--max_workers", str(max_workers),
        "--max_attempts", str(max_attempts),
        "--model", model_path,
        "--base-url", base_url,
        "--unsafe-executor",
        "--max-tokens", str(max_tokens),
        "--reasoning-effort", reasoning_effort,
    ]
    
    # Only add --qwen-no-think for Qwen models
    if is_qwen_model:
        cmd.append("--qwen-no-think")
    
    if no_transductive_penalty:
        cmd.append("--no-transductive-penalty")
    
    if splitter:
        cmd.append("--splitter")
    
    if refinement_ds:
        cmd.extend(["--refinement-ds", refinement_ds])

    if early_stop_threshold:
        cmd.extend(["--early-stop-threshold", str(early_stop_threshold)])

    if rex_stats:
        cmd.append("--rex-stats")
    
    print(f"üìù Full command: {' '.join(cmd)}")
    
    # Start subprocess
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
                              universal_newlines=True, bufsize=1)
    
    try:
        # Stream output in real-time
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            if line:
                print(line.rstrip())
        
        # Wait for process to complete
        return_code = process.wait()
        
        if return_code == 0:
            print(f"‚úÖ ARC tasks completed successfully for {dataset}!")
            return True
        else:
            print(f"‚ùå ARC tasks failed with return code {return_code}")
            return False
    
    except KeyboardInterrupt:
        print(f"\nüõë Ctrl+C received - forwarding to ARC tasks for graceful shutdown...")
        
        # Forward SIGINT to subprocess
        process.send_signal(signal.SIGINT)
        
        # Wait for graceful shutdown (task runner handles this well)
        print(f"‚è≥ Waiting for ARC tasks to complete gracefully...")
        try:
            return_code = process.wait(timeout=301)  # Just over API timeout for safety
            print(f"‚úÖ ARC tasks shut down gracefully")
            return False  # Interrupted = not successful
        except subprocess.TimeoutExpired:
            print(f"‚ö†Ô∏è  ARC tasks didn't shut down gracefully, terminating...")
            process.terminate()
            process.wait()
            return False

def terminate_pod_process(pod_process):
    """Terminate the pod creation subprocess gracefully"""
    if pod_process and pod_process.poll() is None:
        print(f"\nüõë Terminating pod creation...")
        pod_process.send_signal(signal.SIGINT)
        try:
            pod_process.wait(timeout=30)
            print(f"‚úÖ Pod creation shut down gracefully")
        except subprocess.TimeoutExpired:
            print(f"‚ö†Ô∏è  Pod creation didn't respond, force terminating...")
            pod_process.terminate()
            pod_process.wait()

def handle_cleanup_decision(pod_id, base_url=None):
    """Handle the decision to keep or delete the pod"""
    if not pod_id:
        return
    
    print(f"\nüí∞ Pod {pod_id} is still running and incurring costs.")
    keep_pod = prompt_keep_pod()
    
    if keep_pod:
        print(f"\nüöÄ Keeping pod running for manual use:")
        if base_url:
            print(f"   Endpoint: {base_url}")
        print(f"   Console: https://console.runpod.io/pods/{pod_id}")
        print(f"   ‚ö†Ô∏è  Remember to delete the pod when done to stop charges!")
    else:
        delete_pod(pod_id)

def terminate_and_cleanup(pod_process, pod_id, base_url=None):
    """Terminate pod process and handle cleanup decision"""
    terminate_pod_process(pod_process)
    handle_cleanup_decision(pod_id, base_url)

def main():
    parser = argparse.ArgumentParser(
        description='Create a RunPod pod and automatically run ARC tasks once ready',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s arc-agi-1 Trelis/Qwen3-4B_dsarc-agi-1-train-programs-best-length-filtered-250_20250811-155856-c904
  %(prog)s arc-agi-2 Trelis/Qwen3-4B_dsarc-agi-2-custom-model --subset shortest_1
  %(prog)s arc-agi-1 Trelis/Qwen3-4B_model --subset all_evaluation --max_attempts 32
  %(prog)s arc-agi-2 Trelis/arc-1-fake-ttt-blended-c802-FP8-Dynamic --subset all_evaluation --kv-cache-dtype fp8_e5m2
  %(prog)s arc-agi-1 Trelis/Qwen3-4B_model --gpu-count 4 --template sglang

This script will:
1. Create a RunPod pod with the specified model and GPU count
2. Wait for the OpenAI endpoint to be ready
3. Run ARC tasks on specified subset (default: all_evaluation) with configurable max attempts (default: 64)
4. Keep the pod running until you press Ctrl+C
        """
    )
    
    parser.add_argument('dataset', 
                       default='arc-prize-2025',
                       help='Dataset to run (e.g., arc-prize-2025, arc-agi-1, arc-agi-2)')
    parser.add_argument('model_path', 
                       help='Full model path (e.g., Trelis/Qwen3-4B_...)')
    parser.add_argument('--template', 
                       default='sglang',
                       help='RunPod template to use (default: sglang, also supports vllm)')
    parser.add_argument('--gpu-count',
                       type=int,
                       default=1,
                       help='Number of GPUs to allocate (default: 1)')
    parser.add_argument('--subset',
                       default='evaluation',
                       help='Dataset subset to run (default: evaluation)')
    parser.add_argument('--skip-tasks',
                       action='store_true',
                       help='Skip running ARC tasks (only create pod)')
    parser.add_argument('--no-health-check',
                       action='store_true',
                       help='Skip health check after pod creation')
    parser.add_argument('--kv-cache-dtype',
                       type=str,
                       help='KV cache data type for sglang/vllm servers (e.g., fp8_e5m2)')
    parser.add_argument('--max-attempts', '--max_attempts',
                       type=int,
                       default=64,
                       dest='max_attempts',
                       help='Maximum number of attempts for ARC tasks (default: 64)')
    parser.add_argument('--no-transductive-penalty',
                       action='store_true',
                       help='Disable transductive penalty in voting (passed to run_arc_tasks_soar.py)')
    parser.add_argument('--max-workers',
                       type=int,
                       default=32,
                       help='Maximum number of parallel workers (default: 32)')
    parser.add_argument('--splitter',
                       action='store_true',
                       help='Randomly select and shuffle a subset of training input-output pairs')
    parser.add_argument('--max-tokens',
                       type=int,
                       default=2000,
                       help='Maximum tokens for model generation (default: 2000)')
    parser.add_argument('--reasoning-effort',
                       choices=['low', 'medium', 'high'],
                       default='medium',
                       help='Reasoning effort level for OSS models (low, medium, high)')
    parser.add_argument('--refinement-ds',
                       type=str,
                       help='Refinement dataset: HuggingFace dataset or parquet file containing draft programs to refine')
    parser.add_argument('--early-stop-threshold',
                       type=int,
                       help='Early stop threshold to pass through to task runner')
    parser.add_argument('--rex-stats',
                       action='store_true',
                       help='Enable REx stats logging for refinement tracking')
    
    args = parser.parse_args()
    
    # Check for required environment variable
    if 'RUNPOD_API_KEY' not in os.environ:
        print("‚ùå Error: RUNPOD_API_KEY environment variable not set!")
        sys.exit(1)
    
    # Auto-adjust max_tokens for GPT-OSS models if not explicitly set by user
    model_lower = args.model_path.lower()
    if ('gpt-oss' in model_lower and ('20b' in model_lower or '120b' in model_lower)) or 'thinking' in model_lower:
        if args.max_tokens == 2000:  # Only if user didn't override the default
            args.max_tokens = 12000
            print(f"üß† Detected GPT-OSS model, auto-setting max_tokens to {args.max_tokens}")
    
    # Step 1: Create the pod
    print(f"üöÄ Step 1: Creating RunPod pod with model {args.model_path}")
    if args.gpu_count > 1:
        print(f"   Using {args.gpu_count} GPUs for data parallelism")
    
    create_cmd = [
        "uv", "run", "python", "runpod/create_pod.py", 
        args.template,
        args.model_path,  # Now passed as positional argument
    ]
    
    # Add GPU count if not default
    if args.gpu_count != 1:
        create_cmd.extend(["--gpu-count", str(args.gpu_count)])
    
    if args.no_health_check:
        create_cmd.append("--no-health-check")
    
    # Add kv-cache-dtype parameter if provided
    if args.kv_cache_dtype:
        create_cmd.extend(["--kv-cache-dtype", args.kv_cache_dtype])
    
    print(f"üìù Running: {' '.join(create_cmd)}")
    
    # Start the pod creation process with unbuffered output
    # Set environment to ensure unbuffered output
    env = os.environ.copy()
    env['PYTHONUNBUFFERED'] = '1'
    
    pod_process = subprocess.Popen(
        create_cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
        bufsize=0,  # Use unbuffered mode
        env=env
    )
    
    # Variables to track pod status
    pod_id = None
    direct_ip = None
    external_port = None
    proxy_url = None
    endpoint_ready = False
    
    # Read output from the pod creation process
    print("\n--- Pod Creation Output ---")
    print("", flush=True)
    
    try:
        while True:
            line = pod_process.stdout.readline()
            if not line and pod_process.poll() is not None:
                break
            if line:
                # Print immediately with flush
                sys.stdout.write(line)
                sys.stdout.flush()
                
                # Parse pod ID
                if "Pod created successfully! ID:" in line:
                    pod_id = line.split("ID:")[1].strip()
                
                # Parse direct connection info
                if "Direct TCP:" in line and direct_ip is None:
                    parts = line.split("Direct TCP:")[1].strip()
                    if ":" in parts:
                        direct_ip, external_port = parts.split(":")
                        direct_ip = direct_ip.strip()
                        external_port = external_port.strip()
                
                # Parse proxy connection
                if "TCP proxy:" in line and proxy_url is None:
                    proxy_url = line.split("TCP proxy:")[1].strip()
                
                # Check if endpoint is ready
                if "OPENAI ENDPOINT READY!" in line:
                    endpoint_ready = True
                    # Don't break, keep reading to get all output
                
                # Check for health check completion
                if "Health check completed successfully!" in line:
                    endpoint_ready = True
                
                # Check if health check was skipped
                if args.no_health_check and "The pod is now running" in line:
                    endpoint_ready = True
                
                # Check if waiting message appears (pod is running, keep alive)
                if "The pod is now running. Press Ctrl+C" in line:
                    endpoint_ready = True
                    break  # This is where create_pod.py enters its wait loop
                    
    except KeyboardInterrupt:
        print(f"\nüõë Ctrl+C during pod creation - cleaning up...")
        terminate_pod_process(pod_process)
        # create_pod.py handles its own cleanup when terminated
        print(f"‚úÖ Pod cleanup handled by create_pod.py")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Error reading pod creation output: {e}")
        pass  # Error details already printed
    
    # Check if process ended with error
    if not endpoint_ready and pod_process.poll() is not None:
        print(f"\n‚ùå Pod creation process ended unexpectedly with code: {pod_process.returncode}")
        sys.exit(1)
    
    # Determine the base URL to use
    base_url = None
    if direct_ip and external_port:
        base_url = f"http://{direct_ip}:{external_port}/v1"
        print(f"\nüåç Using direct connection: {base_url}")
    elif pod_id:
        # Use proxy if no direct connection available
        base_url = f"http://{pod_id}-8080.proxy.runpod.net/v1"
        print(f"\nüîå Using proxy connection: {base_url}")
    else:
        print("\n‚ùå Failed to determine pod endpoint URL")
        terminate_pod_process(pod_process)
        sys.exit(1)
    
    # Additional wait to ensure the endpoint is ready
    if not args.skip_tasks:
        # Double-check the endpoint is ready
        if not wait_for_openai_endpoint(base_url, args.model_path, timeout=300):
            print("‚ùå OpenAI endpoint failed to become ready")
            terminate_pod_process(pod_process)
            sys.exit(1)
        
        # Step 2: Run ARC tasks
        print(f"\nüéØ Step 2: Running ARC tasks for {args.dataset}")
        
        task_success = run_arc_tasks_with_graceful_handling(
            args.dataset, args.model_path, base_url, args.subset, args.max_attempts, args.no_transductive_penalty, args.max_workers, args.splitter, args.max_tokens, args.reasoning_effort, args.refinement_ds, args.early_stop_threshold, args.rex_stats
        )
        
        if task_success:
            print(f"\nüéâ All tasks completed successfully!")
        else:
            print(f"\n‚ö†Ô∏è  Tasks completed with issues or were cancelled.")
        
        # Always just prompt for cleanup (keep pod process running)
        handle_cleanup_decision(pod_id, base_url)
    else:
        print("\n‚è≠Ô∏è  Skipping ARC tasks as requested")
        
        # Step 3: Keep the create_pod.py process running until user decides
        print(f"\nüí° Pod is running. Press Ctrl+C when ready to decide about pod.")
        
        try:
            # Wait for the pod process to complete (it will run until Ctrl+C)
            pod_process.wait()
        except KeyboardInterrupt:
            print(f"\nüõë Ctrl+C received")
        
        # Always terminate and prompt in skip-tasks mode
        terminate_and_cleanup(pod_process, pod_id, base_url)

if __name__ == "__main__":
    main()