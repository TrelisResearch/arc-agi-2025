name: "vllm-pod-tcp"
cloudType: "SECURE"
computeType: "GPU"
imageName: "vllm/vllm-openai:latest"
gpuCount: 1
gpuTypeIds: ["NVIDIA H200"]
containerDiskInGb: 50
volumeInGb: 50
volumeMountPath: "/root/.cache/huggingface"
ports: ["8080/tcp"]
# dataCenterIds will default to shared config.DEFAULT_DATA_CENTERS if not specified
# Uncomment to override: dataCenterIds: ["US-IL-1", "US-TX-3"]

# Environment variables for HuggingFace access and cache
env:
  HUGGING_FACE_HUB_TOKEN: "{{ RUNPOD_SECRET_HUGGING_FACE_HUB_TOKEN }}"
  HF_TOKEN: "{{ RUNPOD_SECRET_HUGGING_FACE_HUB_TOKEN }}"   # some libs read HF_TOKEN
  HF_HOME: "/root/.cache/huggingface"                      # optional: ensure cache on the mounted volume

dockerStartCmd:
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--kv-cache-dtype"
  - "fp8_e4m3"
  - "--data-parallel-size"
  - "1"

# stripped by your script before POST
healthCheck:
  type: "openai"
  timeout: 600
