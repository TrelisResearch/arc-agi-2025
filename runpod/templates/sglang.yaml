name: "sglang-pod-tcp"
cloudType: "SECURE"
computeType: "GPU"
imageName: "lmsysorg/sglang:latest"
gpuCount: 1
gpuTypeIds: ["NVIDIA H200"]
containerDiskInGb: 50
volumeInGb: 100
volumeMountPath: "/root/.cache/huggingface"
ports: ["8080/tcp"]
# dataCenterIds will default to shared config.DEFAULT_DATA_CENTERS if not specified
# Uncomment to override: dataCenterIds: ["US-IL-1", "US-TX-3"]

# ‚Üê Add this
env:
  HUGGING_FACE_HUB_TOKEN: "{{ RUNPOD_SECRET_HUGGING_FACE_HUB_TOKEN }}"
  HF_TOKEN: "{{ RUNPOD_SECRET_HUGGING_FACE_HUB_TOKEN }}"   # some libs read HF_TOKEN
  HF_HOME: "/root/.cache/huggingface"                      # optional: ensure cache on the mounted volume

dockerStartCmd:
  - "python3"
  - "-m"
  - "sglang.launch_server"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--enable-metrics"
  # --dp will be added dynamically based on GPU count
  # --model-path will be added dynamically
  # --kv-cache-dtype will be added dynamically (default: fp8_e4m3)

# stripped by your script before POST
healthCheck:
  type: "openai"
  timeout: 600
