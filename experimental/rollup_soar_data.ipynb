{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8af7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349ee7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lewis/julien_soar/parquet\n",
      "/home/lewis/julien_soar/arc_1_train_refinement.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-72B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-7B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_refinement.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-14B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_refinement.pkl\n",
      "/home/lewis/julien_soar/arc_1_train_refinement.pkl\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-32B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Mistral-Large-Instruct-2407_solution.parquet\n"
     ]
    }
   ],
   "source": [
    "for file in Path.home().joinpath(\"julien_soar\").iterdir():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcdfbe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for arc_1_train_refinement.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee37714bc0>\n",
      "required group field_id=-1 schema {\n",
      "  required binary field_id=-1 row_id (String);\n",
      "  required binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 reasoning (String);\n",
      "  required binary field_id=-1 code (String);\n",
      "  required group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  required group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  required group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  required group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  required binary field_id=-1 model (String);\n",
      "  required boolean field_id=-1 is_transductive;\n",
      "  optional binary field_id=-1 refined_from_id (String);\n",
      "  optional binary field_id=-1 compound_inspiration_id (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-72B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee37657080>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-Coder-7B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee3755bcc0>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_refinement.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee3756abc0>\n",
      "required group field_id=-1 schema {\n",
      "  required binary field_id=-1 row_id (String);\n",
      "  required binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 reasoning (String);\n",
      "  required binary field_id=-1 code (String);\n",
      "  required group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  required group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  required group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  required group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  required binary field_id=-1 model (String);\n",
      "  required boolean field_id=-1 is_transductive;\n",
      "  optional binary field_id=-1 refined_from_id (String);\n",
      "  optional binary field_id=-1 compound_inspiration_id (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-Coder-14B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee1cdb9d40>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-Coder-32B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee1cdb9d80>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Mistral-Large-Instruct-2407_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x73ee1cdbba80>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parquet schema inspection\n",
    "for file in Path.home().joinpath(\"julien_soar\").iterdir():\n",
    "    if file.suffix == '.parquet':\n",
    "        print(f\"Schema for {file.name}:\")\n",
    "        if pq is not None:\n",
    "            pf = pq.ParquetFile(file)\n",
    "            print(pf.schema)\n",
    "        else:\n",
    "            print(\"pyarrow not available, falling back to pandas (loads data).\")\n",
    "            df = pd.read_parquet(file)\n",
    "            print(df.dtypes)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b53bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llm_python.datasets.collector import generate_unique_hex_id\n",
    "from llm_python.datasets.io import write_soar_parquet\n",
    "\n",
    "def process_refinement_pickle(pkl_path, out_dir):\n",
    "    \"\"\"\n",
    "    Loads a pickle file with dict-of-list structure, merges task_id into each dict, flattens, and writes to parquet.\n",
    "    \"\"\"\n",
    "    obj = pd.read_pickle(pkl_path)\n",
    "    rows = []\n",
    "    for task_id, sample_list in obj.items():\n",
    "        for sample in sample_list:\n",
    "            sample = dict(sample)  # copy to avoid mutating original\n",
    "            sample['task_id'] = task_id\n",
    "            sample['row_id'] = generate_unique_hex_id()\n",
    "            sample['is_transductive'] = False\n",
    "            rows.append(sample)\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_path = Path(out_dir) / (Path(pkl_path).stem + '.parquet')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_soar_parquet(df, out_path)\n",
    "    print(f\"Wrote {out_path} with {len(df)} rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6586684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/lewis/julien_soar/arc_1_val_refinement.parquet with 17408 rows.\n",
      "Wrote /home/lewis/julien_soar/arc_1_train_refinement.parquet with 19002 rows.\n"
     ]
    }
   ],
   "source": [
    "pkl_files = [f for f in Path.home().joinpath(\"julien_soar\").iterdir() if f.suffix == '.pkl']\n",
    "for pkl_file in pkl_files:\n",
    "    process_refinement_pickle(pkl_file, Path.home().joinpath(\"julien_soar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.validation import (\n",
    "    validate_soar_dataframe,\n",
    "    validate_soar_dataframe_correctness,\n",
    "    validate_soar_row,\n",
    ")\n",
    "from llm_python.transduction.code_classifier import CodeTransductionClassifier\n",
    "from llm_python.utils.numpy import convert_numpy_types\n",
    "\n",
    "\n",
    "def clean_soar_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the SOAR DataFrame by ensuring required columns are present and correctly typed.\n",
    "    \"\"\"\n",
    "    # We have already checked required columns in previous steps.\n",
    "\n",
    "    print(\"Validating row correctness...\")\n",
    "    df = df.copy()\n",
    "    df[\"predicted_train_output\"] = df[\"predicted_train_output\"].apply(convert_numpy_types)\n",
    "    df[\"predicted_test_output\"] = df[\"predicted_test_output\"].apply(convert_numpy_types)\n",
    "    df[\"correct_train_input\"] = df[\"correct_train_input\"].apply(convert_numpy_types)\n",
    "    df[\"correct_test_input\"] = df[\"correct_test_input\"].apply(convert_numpy_types)\n",
    "    df[\"reasoning\"] = df[\"reasoning\"].astype(str)\n",
    "\n",
    "    def validate_and_log_errors(df: pd.DataFrame) -> pd.Series:\n",
    "        errors_set = set()\n",
    "        def validate_row(row):\n",
    "            result = validate_soar_row(row)\n",
    "            if hasattr(result, \"errors\") and result.errors:\n",
    "                errors_set.update(result.errors)\n",
    "            return result.is_valid\n",
    "        is_valid_series = df.apply(validate_row, axis=1)\n",
    "        if errors_set:\n",
    "            print(f\"Unique validation errors encountered: {errors_set}\")\n",
    "        return is_valid_series\n",
    "\n",
    "    df[\"is_valid\"] = validate_and_log_errors(df)\n",
    "    print(f\"After cleaning, {df['is_valid'].sum()} out of {len(df)} rows are valid.\")\n",
    "    df = df[df[\"is_valid\"]]\n",
    "    df = df.drop(columns=[\"is_valid\"])\n",
    "\n",
    "    df = df[~df[\"code\"].str.lower().str.contains(\"random|randbelow|rvs\")]\n",
    "    print(f\"Kept {len(df)}/{len(df)} rows after filtering for randomness.\")\n",
    "\n",
    "    transductive_classifier = CodeTransductionClassifier()\n",
    "    df[\"is_transductive\"] = df[\"code\"].apply(lambda code: transductive_classifier.is_transductive(code)[0])\n",
    "    print(f\"{len(df[df['is_transductive']])} out of {len(df)} rows are transductive.\")\n",
    "\n",
    "    correctness_result = validate_soar_row_correctness(row)\n",
    "    print(correctness_result.summary())\n",
    "    if not correctness_result.is_valid:\n",
    "        raise ValueError(\n",
    "            \"Validation failed: Some programs do not meet the correctness requirements.\"\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8554689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id                                                       string[pyarrow]\n",
      "task_id                                                      string[pyarrow]\n",
      "reasoning                                              large_string[pyarrow]\n",
      "code                                                   large_string[pyarrow]\n",
      "correct_train_input                                list<item: bool>[pyarrow]\n",
      "correct_test_input                                 list<item: bool>[pyarrow]\n",
      "predicted_train_output     list<item: list<item: list<item: int64>>>[pyar...\n",
      "predicted_test_output      list<item: list<item: list<item: int64>>>[pyar...\n",
      "model                                                        string[pyarrow]\n",
      "is_transductive                                                bool[pyarrow]\n",
      "refined_from_id                                              string[pyarrow]\n",
      "compound_inspiration_id                                      string[pyarrow]\n",
      "dtype: object\n",
      "row_id                                      7873ca05d0710d5711fd22e7d218965d\n",
      "task_id                                                             1b60fb0c\n",
      "reasoning                                                               <NA>\n",
      "code                       import numpy as np\\n\\ndef find_shape(grid, col...\n",
      "correct_train_input                                       [True, True, True]\n",
      "correct_test_input                                                    [True]\n",
      "predicted_train_output     [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1,...\n",
      "predicted_test_output      [[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 1,...\n",
      "model                                                                   None\n",
      "is_transductive                                                        False\n",
      "refined_from_id                                                         <NA>\n",
      "compound_inspiration_id                                                 <NA>\n",
      "Name: 0, dtype: object\n",
      "Validating row correctness...\n",
      "After cleaning, 19002 out of 19002 rows are valid.\n",
      "Kept 19000/19000 rows after filtering for randomness.\n",
      "Correctness validation:\n",
      "    Total programs: 19000\n",
      "    Sample size: 1000\n",
      "    Correctness valid: FAIL\n",
      "    Errors: 2405\n",
      "    Sample errors:\n",
      "      Row 3648, Train Output 0: predicted != actual\n",
      "      Row 3648, Train Output 2: predicted != actual\n",
      "      Row 3648, Train Output 3: predicted != actual\n"
     ]
    }
   ],
   "source": [
    "from llm_python.datasets.io import read_soar_parquet\n",
    "\n",
    "\n",
    "parquet_files = [f for f in Path.home().joinpath(\"julien_soar\").iterdir() if f.suffix == '.parquet']\n",
    "\n",
    "for file in parquet_files[:1]:\n",
    "    df = read_soar_parquet(file)\n",
    "    print(df.dtypes)\n",
    "    print(df.iloc[0])\n",
    "    df = clean_soar_dataframe(df)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f215744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    rows, cols = grid.shape\n",
      "    new_grid = np.copy(grid)\n",
      "    rows_to_transform = []\n",
      "    for row in grid:\n",
      "        unique_values = np.unique(row)\n",
      "        if len(unique_values) == 1 and unique_values[0] == 0:\n",
      "            rows_to_transform.append(True)\n",
      "        elif len(unique_values) == 2 and 0 in unique_values and (4 in unique_values):\n",
      "            if np.sum(row == 0) > np.sum(row == 4):\n",
      "                rows_to_transform.append(True)\n",
      "            else:\n",
      "                rows_to_transform.append(False)\n",
      "        else:\n",
      "            rows_to_transform.append(False)\n",
      "    cols_to_transform = []\n",
      "    for col in grid.T:\n",
      "        unique_values = np.unique(col)\n",
      "        if len(unique_values) == 1 and unique_values[0] == 0:\n",
      "            cols_to_transform.append(True)\n",
      "        elif len(unique_values) == 2 and 0 in unique_values and (4 in unique_values):\n",
      "            if np.sum(col == 0) > np.sum(col == 4):\n",
      "                cols_to_transform.append(True)\n",
      "            else:\n",
      "                cols_to_transform.append(False)\n",
      "        else:\n",
      "            cols_to_transform.append(False)\n",
      "    for row_idx in range(rows):\n",
      "        if rows_to_transform[row_idx]:\n",
      "            new_grid[row_idx, new_grid[row_idx] == 0] = 2\n",
      "    for col_idx in range(cols):\n",
      "        if cols_to_transform[col_idx]:\n",
      "            new_grid[new_grid[:, col_idx] == 0, col_idx] = 2\n",
      "    return new_grid.tolist()\n",
      "\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    non_black_coords = np.argwhere(grid != 0)\n",
      "    if len(non_black_coords) == 0:\n",
      "        return grid_lst\n",
      "    non_zero_colors = grid[non_black_coords[:, 0], non_black_coords[:, 1]]\n",
      "    color_counts = Counter(non_zero_colors)\n",
      "    border_color, _ = color_counts.most_common(1)[0]\n",
      "    result = np.full((2, 2), border_color, dtype=int).tolist()\n",
      "    return result\n",
      "\n",
      "def transform(grid):\n",
      "\n",
      "    def find_regions(grid):\n",
      "        visited = set()\n",
      "        regions = []\n",
      "\n",
      "        def dfs(x, y, region):\n",
      "            if (x, y) in visited or not 0 <= x < len(grid) or (not 0 <= y < len(grid[0])) or (grid[x][y] != 2):\n",
      "                return\n",
      "            visited.add((x, y))\n",
      "            region.append((x, y))\n",
      "            dfs(x + 1, y, region)\n",
      "            dfs(x - 1, y, region)\n",
      "            dfs(x, y + 1, region)\n",
      "            dfs(x, y - 1, region)\n",
      "        for i in range(len(grid)):\n",
      "            for j in range(len(grid[0])):\n",
      "                if grid[i][j] == 2 and (i, j) not in visited:\n",
      "                    region = []\n",
      "                    dfs(i, j, region)\n",
      "                    regions.append(region)\n",
      "        return regions\n",
      "\n",
      "    def transform_region(region, grid, color):\n",
      "        min_x = min((x for x, y in region))\n",
      "        max_x = max((x for x, y in region))\n",
      "        min_y = min((y for x, y in region))\n",
      "        max_y = max((y for x, y in region))\n",
      "        for i in range(min_x - 1, max_x + 2):\n",
      "            for j in range(min_y - 1, max_y + 2):\n",
      "                if 0 <= i < len(grid) and 0 <= j < len(grid[0]) and (grid[i][j] == 0):\n",
      "                    grid[i][j] = color\n",
      "    regions = find_regions(grid)\n",
      "    for region in regions:\n",
      "        if len(region) > 1:\n",
      "            transform_region(region, grid, 3)\n",
      "    return grid\n",
      "\n",
      "def transform(grid: list[list[int]]) -> list[list[int]]:\n",
      "    n = len(grid)\n",
      "    output_grid = grid[:n // 2] + grid[:n // 2][::-1]\n",
      "    return output_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "\n",
      "    def find_3x3_block(row, col):\n",
      "        try:\n",
      "            block = [grid_lst[row][col:col + 3], grid_lst[row + 1][col:col + 3], grid_lst[row + 2][col:col + 3]]\n",
      "            if all((cell == 0 for row in block for cell in row)):\n",
      "                return (row, col)\n",
      "        except IndexError:\n",
      "            return None\n",
      "        return None\n",
      "    visited_blocks = set()\n",
      "    for row in range(len(grid_lst) - 2):\n",
      "        for col in range(len(grid_lst[0]) - 2):\n",
      "            block_start = find_3x3_block(row, col)\n",
      "            if block_start and block_start not in visited_blocks:\n",
      "                row, col = block_start\n",
      "                grid_lst[row][col:col + 3] = [1, 1, 1]\n",
      "                grid_lst[row + 1][col:col + 3] = [1, 1, 1]\n",
      "                grid_lst[row + 2][col:col + 3] = [1, 1, 1]\n",
      "                visited_blocks.add((row, col))\n",
      "    return grid_lst\n",
      "\n",
      "def transform(grid):\n",
      "    new_grid = [row[:] for row in grid]\n",
      "\n",
      "    def should_insert_two(x, y, grid):\n",
      "        if y + 2 < len(grid[0]) and grid[x][y + 2] == 1 and (grid[x][y + 1] == 0):\n",
      "            return (x, y + 1)\n",
      "        if y - 2 >= 0 and grid[x][y - 2] == 1 and (grid[x][y - 1] == 0):\n",
      "            return (x, y - 1)\n",
      "        if x + 2 < len(grid) and grid[x + 2][y] == 1 and (grid[x + 1][y] == 0):\n",
      "            return (x + 1, y)\n",
      "        if x - 2 >= 0 and grid[x - 2][y] == 1 and (grid[x - 1][y] == 0):\n",
      "            return (x - 1, y)\n",
      "        return None\n",
      "    changes = set()\n",
      "    for i in range(len(grid)):\n",
      "        for j in range(len(grid[0])):\n",
      "            if grid[i][j] == 1:\n",
      "                pos = should_insert_two(i, j, grid)\n",
      "                if pos is not None:\n",
      "                    changes.add(pos)\n",
      "    for x, y in changes:\n",
      "        new_grid[x][y] = 2\n",
      "    return new_grid\n",
      "\n",
      "def transform(grid: list[list[int]]) -> list[list[int]]:\n",
      "    n = len(grid)\n",
      "    transformed_grid = []\n",
      "    transformed_grid.extend(grid)\n",
      "    for i in range(n):\n",
      "        transformed_grid.append(grid[n - 1 - i])\n",
      "    return transformed_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    rows, cols = (len(grid_lst), len(grid_lst[0]))\n",
      "    horizontally_mirrored = []\n",
      "    for row in grid_lst:\n",
      "        mirrored_row = row + row[::-1]\n",
      "        horizontally_mirrored.append(mirrored_row)\n",
      "    vertically_mirrored = horizontally_mirrored + horizontally_mirrored[::-1]\n",
      "    return vertically_mirrored\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    rows, cols = (len(grid_lst), len(grid_lst[0]))\n",
      "    output_grid = [[0] * cols for _ in range(rows)]\n",
      "    rules = {(0, 0, 5): (3, 3, 3), (0, 5, 0): (4, 4, 4), (5, 0, 0): (2, 2, 2)}\n",
      "    for i in range(rows):\n",
      "        row = tuple(grid_lst[i])\n",
      "        if row in rules:\n",
      "            output_grid[i] = list(rules[row])\n",
      "        else:\n",
      "            output_grid[i] = [0] * cols\n",
      "    return output_grid\n",
      "\n",
      "def transform(grid):\n",
      "    result = [[0 for _ in range(len(grid[0]))] for _ in range(len(grid))]\n",
      "    for col in range(len(grid[0])):\n",
      "        if col % 2 == 0:\n",
      "            result[0][col] = grid[0][col // 2]\n",
      "            result[1][col] = grid[1][col // 2]\n",
      "        else:\n",
      "            result[0][col] = grid[1][col // 2]\n",
      "            result[1][col] = grid[0][col // 2]\n",
      "    return result\n",
      "\n",
      "import numpy as np\n",
      "from scipy.ndimage import label\n",
      "\n",
      "def find_bounding_box(mask):\n",
      "    rows = np.any(mask, axis=1)\n",
      "    cols = np.any(mask, axis=0)\n",
      "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
      "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
      "    return (rmin, rmax, cmin, cmax)\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    output_grids = []\n",
      "    for number in np.unique(grid):\n",
      "        if number == 0:\n",
      "            continue\n",
      "        number_mask = grid == number\n",
      "        labeled_array, num_features = label(number_mask)\n",
      "        for i in range(1, num_features + 1):\n",
      "            block_mask = labeled_array == i\n",
      "            rmin, rmax, cmin, cmax = find_bounding_box(block_mask)\n",
      "            output_grid = grid[rmin:rmax + 1, cmin:cmax + 1].copy()\n",
      "            output_grids.append(output_grid)\n",
      "    target_shapes = [(5, 6), (9, 9), (5, 17)]\n",
      "    for target_shape in target_shapes:\n",
      "        for output_grid in output_grids:\n",
      "            if output_grid.shape == target_shape:\n",
      "                return output_grid.tolist()\n",
      "    closest_grid = None\n",
      "    min_diff = float('inf')\n",
      "    for target_shape in target_shapes:\n",
      "        for output_grid in output_grids:\n",
      "            diff = abs(output_grid.shape[0] - target_shape[0]) + abs(output_grid.shape[1] - target_shape[1])\n",
      "            if diff < min_diff:\n",
      "                min_diff = diff\n",
      "                closest_grid = output_grid\n",
      "    return closest_grid.tolist()\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "\n",
      "    def replace_middle_rows(grid, start_row, end_row, start_col, end_col):\n",
      "        for row in range(start_row + 1, end_row - 1):\n",
      "            for col in range(start_col + 1, end_col - 1):\n",
      "                grid[row][col] = 0\n",
      "    rows = len(grid_lst)\n",
      "    cols = len(grid_lst[0])\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid_lst[row][col] != 0:\n",
      "                color = grid_lst[row][col]\n",
      "                start_row = row\n",
      "                end_row = row\n",
      "                start_col = col\n",
      "                end_col = col\n",
      "                while end_row < rows and grid_lst[end_row][col] == color:\n",
      "                    end_row += 1\n",
      "                while end_col < cols and grid_lst[row][end_col] == color:\n",
      "                    end_col += 1\n",
      "                if end_row - start_row > 2 and end_col - start_col > 2:\n",
      "                    replace_middle_rows(grid_lst, start_row, end_row, start_col, end_col)\n",
      "    return grid_lst\n",
      "\n",
      "def transform(grid):\n",
      "    from collections import defaultdict\n",
      "    import numpy as np\n",
      "    flat_grid = [value for row in grid for value in row if value != 0]\n",
      "    color_count = defaultdict(int)\n",
      "    for color in flat_grid:\n",
      "        color_count[color] += 1\n",
      "    max_color = max(color_count, key=color_count.get)\n",
      "    new_grid = [[max_color for _ in range(2)] for _ in range(2)]\n",
      "    return new_grid\n",
      "\n",
      "import numpy as np\n",
      "def transform(grid):\n",
      "    top_half = grid.copy()\n",
      "    bottom_half = grid[::-1]\n",
      "    transformed_grid = np.vstack((top_half, bottom_half))\n",
      "    return transformed_grid\n",
      "\n",
      "def transform(grid):\n",
      "\n",
      "    def find_largest_connected_component(grid):\n",
      "        rows, cols = (len(grid), len(grid[0]))\n",
      "        visited = [[False] * cols for _ in range(rows)]\n",
      "        max_size = 0\n",
      "        max_component = []\n",
      "\n",
      "        def dfs(r, c):\n",
      "            if r < 0 or r >= rows or c < 0 or (c >= cols) or visited[r][c] or (grid[r][c] == 0):\n",
      "                return 0\n",
      "            visited[r][c] = True\n",
      "            size = 1\n",
      "            size += dfs(r - 1, c)\n",
      "            size += dfs(r + 1, c)\n",
      "            size += dfs(r, c - 1)\n",
      "            size += dfs(r, c + 1)\n",
      "            return size\n",
      "        for r in range(rows):\n",
      "            for c in range(cols):\n",
      "                if not visited[r][c] and grid[r][c] != 0:\n",
      "                    size = dfs(r, c)\n",
      "                    if size > max_size:\n",
      "                        max_size = size\n",
      "        return max_size\n",
      "    max_size = find_largest_connected_component(grid)\n",
      "    rows, cols = (len(grid), len(grid[0]))\n",
      "    visited = [[False] * cols for _ in range(rows)]\n",
      "    result = None\n",
      "\n",
      "    def dfs(r, c, component):\n",
      "        if r < 0 or r >= rows or c < 0 or (c >= cols) or visited[r][c] or (grid[r][c] == 0):\n",
      "            return\n",
      "        visited[r][c] = True\n",
      "        component.append((r, c))\n",
      "        dfs(r - 1, c, component)\n",
      "        dfs(r + 1, c, component)\n",
      "        dfs(r, c - 1, component)\n",
      "        dfs(r, c + 1, component)\n",
      "    for r in range(rows):\n",
      "        for c in range(cols):\n",
      "            if not visited[r][c] and grid[r][c] != 0:\n",
      "                component = []\n",
      "                dfs(r, c, component)\n",
      "                if len(component) == max_size:\n",
      "                    result = component\n",
      "                    break\n",
      "    if result is None:\n",
      "        return []\n",
      "    min_r = min((r for r, _ in result))\n",
      "    max_r = max((r for r, _ in result))\n",
      "    min_c = min((c for _, c in result))\n",
      "    max_c = max((c for _, c in result))\n",
      "    subgrid = [[grid[r][c] for c in range(min_c, max_c + 1)] for r in range(min_r, max_r + 1)]\n",
      "    return subgrid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    transformed_grid = []\n",
      "    rows = len(grid_lst)\n",
      "    cols = len(grid_lst[0])\n",
      "    for row in range(rows):\n",
      "        original_row = grid_lst[row]\n",
      "        reversed_row = original_row[::-1]\n",
      "        new_row = original_row + reversed_row\n",
      "        mid_index = len(new_row) // 2\n",
      "        new_row[mid_index] = original_row[-1]\n",
      "        transformed_grid.append(new_row)\n",
      "    return transformed_grid\n",
      "\n",
      "def transform(grid):\n",
      "    blue_col = None\n",
      "    for col in range(len(grid[0])):\n",
      "        if grid[-1][col] == 1:\n",
      "            blue_col = col\n",
      "            break\n",
      "    if blue_col is None:\n",
      "        return grid\n",
      "    direction = 1\n",
      "    current_col = blue_col\n",
      "    for row in range(len(grid) - 1, -1, -1):\n",
      "        grid[row][current_col] = 1\n",
      "        next_col = (current_col + direction) % len(grid[0])\n",
      "        if next_col == 0 and direction == -1 or (next_col == len(grid[0]) - 1 and direction == 1):\n",
      "            direction = -direction\n",
      "        current_col = next_col\n",
      "    for row in range(len(grid)):\n",
      "        for col in range(len(grid[0])):\n",
      "            if grid[row][col] == 0:\n",
      "                grid[row][col] = 8\n",
      "    return grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    rows, cols = (len(grid_lst), len(grid_lst[0]))\n",
      "    new_grid = [[0] * cols for _ in range(rows)]\n",
      "    row_flags = [False] * rows\n",
      "    col_flags = [False] * cols\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid_lst[row][col] == 3:\n",
      "                row_flags[row] = True\n",
      "            elif grid_lst[row][col] == 1:\n",
      "                row_flags[row] = True\n",
      "            elif grid_lst[row][col] == 2:\n",
      "                col_flags[col] = True\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if row_flags[row]:\n",
      "                if grid_lst[row][col] == 3:\n",
      "                    new_grid[row] = [3] * cols\n",
      "                elif grid_lst[row][col] == 1:\n",
      "                    new_grid[row] = [1] * cols\n",
      "            if col_flags[col]:\n",
      "                if not row_flags[row] or (grid_lst[row][col] != 3 and grid_lst[row][col] != 1):\n",
      "                    for r in range(rows):\n",
      "                        if not row_flags[r]:\n",
      "                            new_grid[r][col] = 2\n",
      "    return new_grid\n",
      "\n",
      "def transform(grid: list[list[int]]) -> list[list[int]]:\n",
      "\n",
      "    def find_blocks(grid):\n",
      "        blocks = []\n",
      "        visited = set()\n",
      "        for i in range(len(grid)):\n",
      "            for j in range(len(grid[0])):\n",
      "                if grid[i][j] != 0 and (i, j) not in visited:\n",
      "                    block = set()\n",
      "                    stack = [(i, j)]\n",
      "                    while stack:\n",
      "                        x, y = stack.pop()\n",
      "                        if (x, y) in visited:\n",
      "                            continue\n",
      "                        visited.add((x, y))\n",
      "                        block.add((x, y))\n",
      "                        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
      "                            nx, ny = (x + dx, y + dy)\n",
      "                            if 0 <= nx < len(grid) and 0 <= ny < len(grid[0]) and (grid[nx][ny] == grid[x][y]):\n",
      "                                stack.append((nx, ny))\n",
      "                    blocks.append((block, grid[i][j]))\n",
      "        return blocks\n",
      "\n",
      "    def place_numbers(grid, blocks):\n",
      "        for block, color in blocks:\n",
      "            min_x, min_y = (min(block, key=lambda x: x[0])[0], min(block, key=lambda x: x[1])[1])\n",
      "            max_x, max_y = (max(block, key=lambda x: x[0])[0], max(block, key=lambda x: x[1])[1])\n",
      "            if min_x > 0 and min_y > 0 and (grid[min_x - 1][min_y - 1] == 0):\n",
      "                grid[min_x - 1][min_y - 1] = 1\n",
      "            if min_x > 0 and max_y < len(grid[0]) - 1 and (grid[min_x - 1][max_y + 1] == 0):\n",
      "                grid[min_x - 1][max_y + 1] = 2\n",
      "            if max_x < len(grid) - 1 and min_y > 0 and (grid[max_x + 1][min_y - 1] == 0):\n",
      "                grid[max_x + 1][min_y - 1] = 3\n",
      "            if max_x < len(grid) - 1 and max_y < len(grid[0]) - 1 and (grid[max_x + 1][max_y + 1] == 0):\n",
      "                grid[max_x + 1][max_y + 1] = 4\n",
      "        return grid\n",
      "    blocks = find_blocks(grid)\n",
      "    transformed_grid = place_numbers(grid, blocks)\n",
      "    return transformed_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    colors_to_transform = {1: 1, 4: 4, 7: 7, 8: 8, 5: 5}\n",
      "    for color in colors_to_transform:\n",
      "        positions = [(i, j) for i, row in enumerate(grid) for j, cell in enumerate(row) if cell == color]\n",
      "        if not positions:\n",
      "            continue\n",
      "        min_i = min((pos[0] for pos in positions))\n",
      "        max_i = max((pos[0] for pos in positions))\n",
      "        min_j = min((pos[1] for pos in positions))\n",
      "        max_j = max((pos[1] for pos in positions))\n",
      "        for i in range(min_i, max_i + 1):\n",
      "            for j in range(min_j, max_j + 1):\n",
      "                if (i != min_i and i != max_i) and (j != min_j and j != max_j):\n",
      "                    if (i - min_i) % 2 == (j - min_j) % 2:\n",
      "                        grid[i][j] = 0\n",
      "    return grid\n"
     ]
    }
   ],
   "source": [
    "for file in parquet_files[:1]:\n",
    "    df = read_soar_parquet(file)\n",
    "    print('\\n\\n'.join(df['code'].sample(20, random_state=42).tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trelis-arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
