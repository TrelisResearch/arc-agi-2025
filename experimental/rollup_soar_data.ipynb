{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8af7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ee7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-72B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-7B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-14B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_refinement.pkl\n",
      "/home/lewis/julien_soar/arc_1_train_refinement.pkl\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-32B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Mistral-Large-Instruct-2407_solution.parquet\n"
     ]
    }
   ],
   "source": [
    "for file in Path.home().joinpath(\"julien_soar\").iterdir():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parquet schema inspection\n",
    "for file in Path.home().joinpath(\"julien_soar\").iterdir():\n",
    "    if file.suffix == '.parquet':\n",
    "        print(f\"Schema for {file.name}:\")\n",
    "        if pq is not None:\n",
    "            pf = pq.ParquetFile(file)\n",
    "            print(pf.schema)\n",
    "        else:\n",
    "            print(\"pyarrow not available, falling back to pandas (loads data).\")\n",
    "            df = pd.read_parquet(file)\n",
    "            print(df.dtypes)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llm_python.datasets.collector import generate_unique_hex_id\n",
    "from llm_python.datasets.io import write_soar_parquet\n",
    "\n",
    "def process_refinement_pickle(pkl_path, out_dir):\n",
    "    \"\"\"\n",
    "    Loads a pickle file with dict-of-list structure, merges task_id into each dict, flattens, and writes to parquet.\n",
    "    \"\"\"\n",
    "    obj = pd.read_pickle(pkl_path)\n",
    "    rows = []\n",
    "    for task_id, sample_list in obj.items():\n",
    "        for sample in sample_list:\n",
    "            sample = dict(sample)  # copy to avoid mutating original\n",
    "            sample['task_id'] = task_id\n",
    "            sample['row_id'] = generate_unique_hex_id()\n",
    "            sample['is_transductive'] = False\n",
    "            rows.append(sample)\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_path = Path(out_dir) / (Path(pkl_path).stem + '.parquet')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_soar_parquet(df, out_path)\n",
    "    print(f\"Wrote {out_path} with {len(df)} rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/lewis/julien_soar/parquet/arc_1_val_refinement.parquet with 17408 rows.\n",
      "Wrote /home/lewis/julien_soar/parquet/arc_1_train_refinement.parquet with 19002 rows.\n",
      "Wrote /home/lewis/julien_soar/parquet/arc_1_train_refinement.parquet with 19002 rows.\n"
     ]
    }
   ],
   "source": [
    "pkl_files = [f for f in Path.home().joinpath(\"julien_soar\").iterdir() if f.suffix == '.pkl']\n",
    "for pkl_file in pkl_files:\n",
    "    process_refinement_pickle(pkl_file, Path.home().joinpath(\"julien_soar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.validation import (\n",
    "    validate_soar_dataframe,\n",
    "    validate_soar_dataframe_correctness,\n",
    "    validate_soar_row,\n",
    ")\n",
    "\n",
    "\n",
    "def clean_soar_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the SOAR DataFrame by ensuring required columns are present and correctly typed.\n",
    "    \"\"\"\n",
    "    # We have already checked required columns in previous steps.\n",
    "\n",
    "    print(\"Validating row correctness...\")\n",
    "    df = df.copy()\n",
    "\n",
    "    def validate_and_log_errors(df: pd.DataFrame) -> pd.Series:\n",
    "        errors_set = set()\n",
    "        def validate_row(row):\n",
    "            result = validate_soar_row(row)\n",
    "            if hasattr(result, \"errors\") and result.errors:\n",
    "                errors_set.update(result.errors)\n",
    "            return result.is_valid\n",
    "        is_valid_series = df.apply(validate_row, axis=1)\n",
    "        if errors_set:\n",
    "            print(f\"Unique validation errors encountered: {errors_set}\")\n",
    "        return is_valid_series\n",
    "\n",
    "    df[\"is_valid\"] = validate_and_log_errors(df)\n",
    "    print(f\"After cleaning, {df['is_valid'].sum()} out of {len(df)} rows are valid.\")\n",
    "    df = df[df[\"is_valid\"]]\n",
    "    df = df.drop(columns=[\"is_valid\"])\n",
    "\n",
    "    df = df[~df[\"code\"].str.lower().str.contains(\"random|randbelow|rvs\")]\n",
    "    print(f\"Kept {len(df)}/{len(df)} rows after filtering for randomness.\")\n",
    "\n",
    "    correctness_result = validate_soar_dataframe_correctness(\n",
    "        df, correctness_samples=1000\n",
    "    )\n",
    "    print(correctness_result.summary())\n",
    "    if not correctness_result.is_valid:\n",
    "        raise ValueError(\n",
    "            \"Validation failed: Some programs do not meet the correctness requirements.\"\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8554689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating row correctness...\n",
      "Unique validation errors encountered: {'reasoning must be a string if provided', 'predicted_test_output must be a list', 'correct_test_input must be a list', 'predicted_train_output must be a list', 'correct_train_input must be a list'}\n",
      "After cleaning, 0 out of 19002 rows are valid.\n",
      "Kept 0/0 rows after filtering for randomness.\n",
      "Correctness validation:\n",
      "    Total programs: 0\n",
      "    Sample size: 0\n",
      "    Correctness valid: PASS\n",
      "    Errors: 0\n",
      "Unique validation errors encountered: {'reasoning must be a string if provided', 'predicted_test_output must be a list', 'correct_test_input must be a list', 'predicted_train_output must be a list', 'correct_train_input must be a list'}\n",
      "After cleaning, 0 out of 19002 rows are valid.\n",
      "Kept 0/0 rows after filtering for randomness.\n",
      "Correctness validation:\n",
      "    Total programs: 0\n",
      "    Sample size: 0\n",
      "    Correctness valid: PASS\n",
      "    Errors: 0\n"
     ]
    }
   ],
   "source": [
    "from llm_python.datasets.io import read_soar_parquet\n",
    "\n",
    "\n",
    "parquet_files = [f for f in Path.home().joinpath(\"julien_soar\").iterdir() if f.suffix == '.parquet']\n",
    "\n",
    "for file in parquet_files[:1]:\n",
    "    df = read_soar_parquet(file)\n",
    "    print(df.iloc[0])\n",
    "    df = clean_soar_dataframe(df)\n",
    "    del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trelis-arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
