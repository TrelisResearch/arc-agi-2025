{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8af7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349ee7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-72B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-7B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-14B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_refinement.pkl\n",
      "/home/lewis/julien_soar/arc_1_train_refinement.pkl\n",
      "/home/lewis/julien_soar/arc_1_val_Qwen2.5-Coder-32B-Instruct_solution.parquet\n",
      "/home/lewis/julien_soar/arc_1_val_Mistral-Large-Instruct-2407_solution.parquet\n"
     ]
    }
   ],
   "source": [
    "for file in Path.home().joinpath(\"julien_soar\").iterdir():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdfbe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for arc_1_val_Qwen2.5-72B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x74b549381a00>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-Coder-7B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x74b5a6d4a300>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-Coder-14B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x74b5c4168f00>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Qwen2.5-Coder-32B-Instruct_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x74b54b597a40>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n",
      "Schema for arc_1_val_Mistral-Large-Instruct-2407_solution.parquet:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x74b5495e9040>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 task_id (String);\n",
      "  optional binary field_id=-1 text (String);\n",
      "  optional binary field_id=-1 code (String);\n",
      "  optional group field_id=-1 correct_train_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_train_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 correct_test_input (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional boolean field_id=-1 element;\n",
      "    }\n",
      "  }\n",
      "  optional group field_id=-1 predicted_test_output (List) {\n",
      "    repeated group field_id=-1 list {\n",
      "      optional group field_id=-1 element (List) {\n",
      "        repeated group field_id=-1 list {\n",
      "          optional group field_id=-1 element (List) {\n",
      "            repeated group field_id=-1 list {\n",
      "              optional int64 field_id=-1 element;\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  optional binary field_id=-1 model (String);\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parquet schema inspection\n",
    "for file in Path.home().joinpath(\"julien_soar\").iterdir():\n",
    "    if file.suffix == '.parquet':\n",
    "        print(f\"Schema for {file.name}:\")\n",
    "        if pq is not None:\n",
    "            pf = pq.ParquetFile(file)\n",
    "            print(pf.schema)\n",
    "        else:\n",
    "            print(\"pyarrow not available, falling back to pandas (loads data).\")\n",
    "            df = pd.read_parquet(file)\n",
    "            print(df.dtypes)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b53bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llm_python.datasets.collector import generate_unique_hex_id\n",
    "from llm_python.datasets.io import write_soar_parquet\n",
    "\n",
    "def process_refinement_pickle(pkl_path, out_dir):\n",
    "    \"\"\"\n",
    "    Loads a pickle file with dict-of-list structure, merges task_id into each dict, flattens, and writes to parquet.\n",
    "    \"\"\"\n",
    "    obj = pd.read_pickle(pkl_path)\n",
    "    rows = []\n",
    "    for task_id, sample_list in obj.items():\n",
    "        for sample in sample_list:\n",
    "            sample = dict(sample)  # copy to avoid mutating original\n",
    "            sample['task_id'] = task_id\n",
    "            sample['row_id'] = generate_unique_hex_id()\n",
    "            sample['is_transductive'] = False\n",
    "            rows.append(sample)\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_path = Path(out_dir) / (Path(pkl_path).stem + '.parquet')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_soar_parquet(df, out_path)\n",
    "    print(f\"Wrote {out_path} with {len(df)} rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6586684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_files = [f for f in Path.home().joinpath(\"julien_soar\").iterdir() if f.suffix == '.pkl']\n",
    "# for pkl_file in pkl_files:\n",
    "#     process_refinement_pickle(pkl_file, Path.home().joinpath(\"julien_soar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/code/trelis-arc/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llm_python.datasets.validation import (\n",
    "    CorrectnessRowValidationResult,\n",
    "    validate_soar_dataframe,\n",
    "    validate_soar_dataframe_correctness,\n",
    "    validate_soar_row,\n",
    "    validate_soar_row_correctness,\n",
    ")\n",
    "from llm_python.transduction.code_classifier import CodeTransductionClassifier\n",
    "from llm_python.utils.arc_tester import ArcTester\n",
    "from llm_python.utils.numpy import convert_numpy_types\n",
    "import concurrent.futures\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "arc_tester = ArcTester()\n",
    "\n",
    "def clean_soar_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the SOAR DataFrame by ensuring required columns are present and correctly typed.\n",
    "    \"\"\"\n",
    "    # We have already checked required columns in previous steps.\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    def process_row_initial(row_tuple):\n",
    "        _ , row = row_tuple\n",
    "        row = row.to_dict()\n",
    "        row[\"predicted_train_output\"] = convert_numpy_types(row[\"predicted_train_output\"])\n",
    "        row[\"predicted_test_output\"] = convert_numpy_types(row[\"predicted_test_output\"])\n",
    "        row[\"correct_train_input\"] = convert_numpy_types(row[\"correct_train_input\"])\n",
    "        row[\"correct_test_input\"] = convert_numpy_types(row[\"correct_test_input\"])\n",
    "        row[\"reasoning\"] = str(row[\"reasoning\"])\n",
    "        row[\"row_id\"] = generate_unique_hex_id()\n",
    "        transductive_classifier = CodeTransductionClassifier()\n",
    "        row[\"is_transductive\"] = transductive_classifier.is_transductive(row[\"code\"])[0]\n",
    "        return row\n",
    "\n",
    "    print(\"Computing base fields in parallel...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        processed_rows = list(tqdm(executor.map(process_row_initial, df.iterrows()), total=len(df), desc=\"Computing base fields\"))\n",
    "    \n",
    "    df = pd.DataFrame(processed_rows)\n",
    "\n",
    "    def validate_and_log_errors_parallel(df: pd.DataFrame) -> pd.Series:\n",
    "        errors_set = set()\n",
    "        \n",
    "        def validate_row_wrapper(row_tuple):\n",
    "            _ , row = row_tuple\n",
    "            result = validate_soar_row(row)\n",
    "            return result.is_valid, result.errors\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(validate_row_wrapper, df.iterrows()), total=len(df), desc=\"Validating rows\"))\n",
    "\n",
    "        is_valid_list = []\n",
    "        for is_valid, errors in results:\n",
    "            is_valid_list.append(is_valid)\n",
    "            if errors:\n",
    "                errors_set.update(errors)\n",
    "        \n",
    "        if errors_set:\n",
    "            print(f\"Unique validation errors encountered: {errors_set}\")\n",
    "            \n",
    "        return pd.Series(is_valid_list, index=df.index)\n",
    "\n",
    "    print(\"Validating rows in parallel...\")\n",
    "    df[\"is_valid\"] = validate_and_log_errors_parallel(df)\n",
    "    print(f\"After cleaning, {df['is_valid'].sum()} out of {len(df)} rows are valid.\")\n",
    "    df = df[df[\"is_valid\"]]\n",
    "    df = df.drop(columns=[\"is_valid\"])\n",
    "\n",
    "    df = df[~df[\"code\"].str.lower().str.contains(\"random|randbelow|rvs\")]\n",
    "    print(f\"Kept {len(df)}/{len(df)} rows after filtering for randomness.\")\n",
    "\n",
    "    print(f\"{len(df[df['is_transductive']])} out of {len(df)} rows are transductive.\")\n",
    "\n",
    "    print(\"Validating row correctness with ThreadPoolExecutor...\")\n",
    "    \n",
    "    def validate_correctness_wrapper(row_tuple):\n",
    "        _, row = row_tuple\n",
    "        return validate_soar_row_correctness(row, arc_tester)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        correctness_results = list(tqdm(executor.map(validate_correctness_wrapper, df.iterrows()), total=len(df), desc=\"Validating correctness\"))\n",
    "\n",
    "    cleaned_rows = []\n",
    "    original_rows = df.to_dict('records')\n",
    "    for i, (correctness_result) in enumerate(correctness_results):\n",
    "        row = original_rows[i]\n",
    "        if not correctness_result.correctness_valid:\n",
    "            # For safety, check if counts change\n",
    "            orig_true_count = sum(row[\"correct_train_input\"]) if isinstance(row[\"correct_train_input\"], list) else 0\n",
    "            new_true_count = sum(correctness_result.new_correct_train_input) if isinstance(correctness_result.new_correct_train_input, list) else 0\n",
    "            if orig_true_count != new_true_count:\n",
    "                print(\n",
    "                    f\"Warning: correct_train_input True count changed from {orig_true_count} to {new_true_count} for row_id {row.get('row_id', '<unknown>')}\"\n",
    "                )\n",
    "\n",
    "            row[\"predicted_train_output\"] = correctness_result.new_predicted_train_output\n",
    "            row[\"predicted_test_output\"] = correctness_result.new_predicted_test_output\n",
    "            row[\"correct_train_input\"] = correctness_result.new_correct_train_input\n",
    "            row[\"correct_test_input\"] = correctness_result.new_correct_test_input\n",
    "        \n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(cleaned_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8554689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/lewis/julien_soar/arc_1_val_Qwen2.5-72B-Instruct_solution.parquet\n",
      "Computing base fields in parallel...\n",
      "Computing base fields in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing base fields:   0%|          | 0/1289137 [00:01<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'clean_soar_dataframe.<locals>.process_row_initial'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/lewis/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/multiprocessing/queues.py\", line 244, in _feed\n    obj = _ForkingPickler.dumps(obj)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lewis/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\nAttributeError: Can't pickle local object 'clean_soar_dataframe.<locals>.process_row_initial'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m df = read_soar_parquet(file)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# df = df.head(1000) # Testing\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(df.dtypes)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# print(df.iloc[0])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mclean_soar_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m out_dir = Path.home().joinpath(\u001b[33m\"\u001b[39m\u001b[33mjulien_soar_cleaned\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m out_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mclean_soar_dataframe\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing base fields in parallel...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ProcessPoolExecutor(max_workers=max_procs) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     processed_rows = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_row_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mComputing base fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m df = pd.DataFrame(processed_rows)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_and_log_errors_parallel\u001b[39m(df: pd.DataFrame) -> pd.Series:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/trelis-arc/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/process.py:620\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    615\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    617\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    618\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/multiprocessing/queues.py:244\u001b[39m, in \u001b[36mQueue._feed\u001b[39m\u001b[34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[39m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m obj = \u001b[43m_ForkingPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wacquire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    246\u001b[39m     send_bytes(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/multiprocessing/reduction.py:51\u001b[39m, in \u001b[36mForkingPickler.dumps\u001b[39m\u001b[34m(cls, obj, protocol)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     50\u001b[39m     buf = io.BytesIO()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m buf.getbuffer()\n",
      "\u001b[31mAttributeError\u001b[39m: Can't pickle local object 'clean_soar_dataframe.<locals>.process_row_initial'"
     ]
    }
   ],
   "source": [
    "from llm_python.datasets.io import read_soar_parquet\n",
    "\n",
    "\n",
    "parquet_files = [f for f in Path.home().joinpath(\"julien_soar\").iterdir() if f.suffix == '.parquet']\n",
    "\n",
    "for file in parquet_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    df = read_soar_parquet(file)\n",
    "    # df = df.head(1000) # Testing\n",
    "    # print(df.dtypes)\n",
    "    # print(df.iloc[0])\n",
    "    df = clean_soar_dataframe(df)\n",
    "    out_dir = Path.home().joinpath(\"julien_soar_cleaned\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    write_soar_parquet(df, out_dir / file.name)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f215744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = grid_lst[0]\n",
      "    n = len(grid)\n",
      "    output_size = n * 2 - 1\n",
      "    output_grid = [[0 for _ in range(output_size)] for _ in range(output_size)]\n",
      "    for i in range(n):\n",
      "        if grid[i] != 0:\n",
      "            for j in range(output_size):\n",
      "                if i + j < output_size:\n",
      "                    output_grid[i + j][output_size - 1 - j] = grid[i]\n",
      "                if i - j >= 0:\n",
      "                    output_grid[i - j][j] = grid[i]\n",
      "                if i + j < output_size:\n",
      "                    output_grid[output_size - 1 - i - j][j] = 1\n",
      "                if i - j >= 0:\n",
      "                    output_grid[j][i - j] = 1\n",
      "    return output_grid\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    non_zero_indices = np.argwhere(grid != 0)\n",
      "    if non_zero_indices.size == 0:\n",
      "        return grid.tolist()\n",
      "    min_row, min_col = non_zero_indices.min(axis=0)\n",
      "    max_row, max_col = non_zero_indices.max(axis=0)\n",
      "    new_grid = np.zeros_like(grid)\n",
      "    for row in range(min_row, max_row + 1):\n",
      "        for col in range(min_col, max_col + 1):\n",
      "            if grid[row, col] != 0:\n",
      "                new_grid[row, col] = grid[row, col]\n",
      "                new_grid[row, min_col] = grid[row, col]\n",
      "                new_grid[row, max_col] = grid[row, col]\n",
      "                new_grid[min_row, col] = grid[row, col]\n",
      "                new_grid[max_row, col] = grid[row, col]\n",
      "                new_grid[min_row, min_col] = grid[row, col]\n",
      "                new_grid[min_row, max_col] = grid[row, col]\n",
      "                new_grid[max_row, min_col] = grid[row, col]\n",
      "                new_grid[max_row, max_col] = grid[row, col]\n",
      "    return new_grid.tolist()\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    rows, cols = (len(grid), len(grid[0]))\n",
      "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid[row][col] == 0:\n",
      "                continue\n",
      "            if grid[row][col] == 1 or grid[row][col] == 2:\n",
      "                for dr, dc in directions:\n",
      "                    new_row, new_col = (row + dr, col + dc)\n",
      "                    if 0 <= new_row < rows and 0 <= new_col < cols:\n",
      "                        if grid[new_row][new_col] == 8:\n",
      "                            grid[row][col] = 3 if grid[row][col] == 1 else 4\n",
      "                            break\n",
      "                else:\n",
      "                    for dr, dc in directions:\n",
      "                        new_row, new_col = (row + dr, col + dc)\n",
      "                        if 0 <= new_row < rows and 0 <= new_col < cols:\n",
      "                            if grid[new_row][new_col] == 1:\n",
      "                                grid[row][col] = 2\n",
      "                                break\n",
      "                            elif grid[new_row][new_col] == 2:\n",
      "                                grid[row][col] = 1\n",
      "                                break\n",
      "            elif grid[row][col] == 3 or grid[row][col] == 4:\n",
      "                for dr, dc in directions:\n",
      "                    new_row, new_col = (row + dr, col + dc)\n",
      "                    if 0 <= new_row < rows and 0 <= new_col < cols:\n",
      "                        if grid[new_row][new_col] == 8:\n",
      "                            grid[row][col] = 6 if grid[row][col] == 3 else 2\n",
      "                            break\n",
      "                else:\n",
      "                    for dr, dc in directions:\n",
      "                        new_row, new_col = (row + dr, col + dc)\n",
      "                        if 0 <= new_row < rows and 0 <= new_col < cols:\n",
      "                            if grid[new_row][new_col] == 3:\n",
      "                                grid[row][col] = 4\n",
      "                                break\n",
      "                            elif grid[new_row][new_col] == 4:\n",
      "                                grid[row][col] = 3\n",
      "                                break\n",
      "    return grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    rows = len(grid_lst)\n",
      "    cols = len(grid_lst[0])\n",
      "    new_grid = [row[:] for row in grid_lst]\n",
      "    for i in range(rows):\n",
      "        for j in range(cols):\n",
      "            if grid_lst[i][j] == 1:\n",
      "                if i + 1 < rows and j + 1 < cols and (grid_lst[i + 1][j] == 1) and (grid_lst[i][j + 1] == 1) and (grid_lst[i + 1][j + 1] == 1):\n",
      "                    new_grid[i + 1][j] = 3\n",
      "                    new_grid[i][j + 1] = 3\n",
      "                    new_grid[i + 1][j + 1] = 3\n",
      "                elif i + 1 < rows and grid_lst[i + 1][j] == 1:\n",
      "                    new_grid[i + 1][j] = 2\n",
      "                elif j + 1 < cols and grid_lst[i][j + 1] == 1:\n",
      "                    new_grid[i][j + 1] = 2\n",
      "    return new_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "\n",
      "    def map_to_7x7(x, y):\n",
      "        if x < 3:\n",
      "            new_x = 0 if x == 0 else 1 if x == 1 else 2\n",
      "        elif x > 13:\n",
      "            new_x = 4 if x == 14 else 5 if x == 15 else 6\n",
      "        else:\n",
      "            new_x = 3\n",
      "        if y < 3:\n",
      "            new_y = 0 if y == 0 else 1 if y == 1 else 2\n",
      "        elif y > 12:\n",
      "            new_y = 4 if y == 13 else 5 if y == 14 else 6\n",
      "        else:\n",
      "            new_y = 3\n",
      "        return (new_x, new_y)\n",
      "    grid = grid_lst\n",
      "    output_grid = [[0] * 7 for _ in range(7)]\n",
      "    for i in range(len(grid)):\n",
      "        for j in range(len(grid[0])):\n",
      "            if grid[i][j] != 0:\n",
      "                new_i, new_j = map_to_7x7(i, j)\n",
      "                if output_grid[new_i][new_j] == 0:\n",
      "                    output_grid[new_i][new_j] = grid[i][j]\n",
      "    return output_grid\n",
      "\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    non_zero_grid = grid[grid != 0]\n",
      "    if non_zero_grid.size == 0:\n",
      "        return [[0]]\n",
      "    counts = Counter(non_zero_grid)\n",
      "    most_frequent_color = counts.most_common(1)[0][0]\n",
      "    if most_frequent_color == 8:\n",
      "        output_shape = (8, 9)\n",
      "    elif most_frequent_color == 4:\n",
      "        output_shape = (6, 8)\n",
      "    elif most_frequent_color == 3:\n",
      "        output_shape = (6, 9)\n",
      "    elif most_frequent_color == 6:\n",
      "        output_shape = (8, 8)\n",
      "    elif most_frequent_color == 2:\n",
      "        output_shape = (8, 10)\n",
      "    else:\n",
      "        output_shape = (8, 10)\n",
      "    output_grid = np.zeros(output_shape, dtype=int)\n",
      "    sorted_colors = sorted(counts.items(), key=lambda x: (-x[1], x[0]))\n",
      "    color_index = 0\n",
      "    for i in range(output_shape[0]):\n",
      "        for j in range(output_shape[1]):\n",
      "            if color_index < len(sorted_colors):\n",
      "                output_grid[i, j] = sorted_colors[color_index][0]\n",
      "                if j < output_shape[1] - 1:\n",
      "                    color_index += 1\n",
      "            else:\n",
      "                color_index = 0\n",
      "                output_grid[i, j] = sorted_colors[color_index][0]\n",
      "    return output_grid.tolist()\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    rows, cols = grid.shape\n",
      "    orange_color = 7\n",
      "    pink_color = 6\n",
      "    orange_cells = np.argwhere(grid == orange_color)\n",
      "    if len(orange_cells) == 0:\n",
      "        return grid_lst\n",
      "    for row, col in orange_cells:\n",
      "        for i in range(col + 4, cols, 4):\n",
      "            if grid[row, i] == 0:\n",
      "                grid[row, i] = pink_color\n",
      "            else:\n",
      "                break\n",
      "        for i in range(col - 4, -1, -4):\n",
      "            if grid[row, i] == 0:\n",
      "                grid[row, i] = pink_color\n",
      "            else:\n",
      "                break\n",
      "        for i in range(row + 4, rows, 4):\n",
      "            if grid[i, col] == 0:\n",
      "                grid[i, col] = pink_color\n",
      "            else:\n",
      "                break\n",
      "        for i in range(row - 4, -1, -4):\n",
      "            if grid[i, col] == 0:\n",
      "                grid[i, col] = pink_color\n",
      "            else:\n",
      "                break\n",
      "    for row, col in orange_cells:\n",
      "        for dr, dc in [(0, 4), (0, -4), (4, 0), (-4, 0)]:\n",
      "            new_row, new_col = (row + dr, col + dc)\n",
      "            if 0 <= new_row < rows and 0 <= new_col < cols and (grid[new_row, new_col] == pink_color):\n",
      "                if dr == 0 and dc == 4:\n",
      "                    for i in range(col, col + 4):\n",
      "                        if grid[row, i] == 0:\n",
      "                            grid[row, i] = orange_color\n",
      "                elif dr == 0 and dc == -4:\n",
      "                    for i in range(col - 3, col + 1):\n",
      "                        if grid[row, i] == 0:\n",
      "                            grid[row, i] = orange_color\n",
      "                elif dr == 4 and dc == 0:\n",
      "                    for i in range(row, row + 4):\n",
      "                        if grid[i, col] == 0:\n",
      "                            grid[i, col] = orange_color\n",
      "                elif dr == -4 and dc == 0:\n",
      "                    for i in range(row - 3, row + 1):\n",
      "                        if grid[i, col] == 0:\n",
      "                            grid[i, col] = orange_color\n",
      "    return grid.tolist()\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    rows, cols = (len(grid), len(grid[0]))\n",
      "    sequence = []\n",
      "    for i in range(cols):\n",
      "        if grid[0][i] != 0:\n",
      "            if not sequence or grid[0][i] == (sequence[-1] + 1) % 10:\n",
      "                sequence.append(grid[0][i])\n",
      "            else:\n",
      "                break\n",
      "        else:\n",
      "            break\n",
      "    for i in range(1, rows):\n",
      "        for j in range(cols):\n",
      "            if grid[i][j] != 0:\n",
      "                expected_value = sequence[(j + i) % len(sequence)]\n",
      "                if grid[i][j] != expected_value:\n",
      "                    grid[i][j] = expected_value\n",
      "    return grid\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    rows, cols = grid.shape\n",
      "    new_grid = np.copy(grid)\n",
      "    temp_grid = np.zeros_like(grid)\n",
      "\n",
      "    def fill_between(val1, val2, row, col, end_row, end_col):\n",
      "        if row == end_row:\n",
      "            start_col = min(col, end_col)\n",
      "            end_col = max(col, end_col)\n",
      "            for c in range(start_col, end_col + 1):\n",
      "                if new_grid[row, c] == 0:\n",
      "                    new_grid[row, c] = val2\n",
      "                    temp_grid[row, c] = val2\n",
      "        elif col == end_col:\n",
      "            start_row = min(row, end_row)\n",
      "            end_row = max(row, end_row)\n",
      "            for r in range(start_row, end_row + 1):\n",
      "                if new_grid[r, col] == 0:\n",
      "                    new_grid[r, col] = val2\n",
      "                    temp_grid[r, col] = val2\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid[row, col] == 4:\n",
      "                for end_col in range(cols):\n",
      "                    if grid[row, end_col] == 8:\n",
      "                        fill_between(4, 3, row, col, row, end_col)\n",
      "                        break\n",
      "                for end_row in range(rows):\n",
      "                    if grid[end_row, col] == 8:\n",
      "                        fill_between(4, 3, row, col, end_row, col)\n",
      "                        break\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid[row, col] == 8:\n",
      "                for end_col in range(cols):\n",
      "                    if grid[row, end_col] == 2:\n",
      "                        fill_between(8, 8, row, col, row, end_col)\n",
      "                        break\n",
      "                for end_row in range(rows):\n",
      "                    if grid[end_row, col] == 2:\n",
      "                        fill_between(8, 8, row, col, end_row, col)\n",
      "                        break\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid[row, col] == 2:\n",
      "                for end_col in range(cols):\n",
      "                    if grid[row, end_col] == 8:\n",
      "                        fill_between(2, 2, row, col, row, end_col)\n",
      "                        break\n",
      "                for end_row in range(rows):\n",
      "                    if grid[end_row, col] == 8:\n",
      "                        fill_between(2, 2, row, col, end_row, col)\n",
      "                        break\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if temp_grid[row, col] == 3:\n",
      "                for end_col in range(cols):\n",
      "                    if grid[row, end_col] == 4:\n",
      "                        fill_between(3, 4, row, col, row, end_col)\n",
      "                        break\n",
      "                for end_row in range(rows):\n",
      "                    if grid[end_row, col] == 4:\n",
      "                        fill_between(3, 4, row, col, end_row, col)\n",
      "                        break\n",
      "    return new_grid.tolist()\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    rows, cols = grid.shape\n",
      "    out_grid = np.zeros((rows, cols), dtype=int)\n",
      "    for row in range(rows):\n",
      "        for col in range(cols):\n",
      "            if grid[row, col] == 3:\n",
      "                out_grid[row, col] = 3\n",
      "                for i in range(1, min(rows, cols)):\n",
      "                    if row + i < rows and grid[row + i, col] == 0:\n",
      "                        out_grid[row + i, col] = 3\n",
      "                        break\n",
      "                    if row - i >= 0 and grid[row - i, col] == 0:\n",
      "                        out_grid[row - i, col] = 3\n",
      "                        break\n",
      "                    if col + i < cols and grid[row, col + i] == 0:\n",
      "                        out_grid[row, col + i] = 3\n",
      "                        break\n",
      "                    if col - i >= 0 and grid[row, col - i] == 0:\n",
      "                        out_grid[row, col - i] = 3\n",
      "                        break\n",
      "    return out_grid.tolist()\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = grid_lst\n",
      "    rows, cols = (len(grid), len(grid[0]))\n",
      "    out_grid = [[0] * cols for _ in range(3)]\n",
      "    for j in range(cols):\n",
      "        for i in range(rows):\n",
      "            if grid[i][j] != 0 and grid[i][j] != 5:\n",
      "                out_grid[1][j] = grid[i][j]\n",
      "                break\n",
      "    for j in range(cols):\n",
      "        found = False\n",
      "        for i in range(rows):\n",
      "            if grid[i][j] != 0 and grid[i][j] != 5:\n",
      "                out_grid[1][j] = grid[i][j]\n",
      "                found = True\n",
      "                break\n",
      "        if not found:\n",
      "            for i in range(rows):\n",
      "                if grid[i][j] != 0:\n",
      "                    out_grid[0][j] = grid[i][j]\n",
      "                    break\n",
      "            else:\n",
      "                for i in range(rows - 1, -1, -1):\n",
      "                    if grid[i][j] != 0:\n",
      "                        out_grid[2][j] = grid[i][j]\n",
      "                        break\n",
      "    return out_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    flat_grid = [cell for row in grid_lst for cell in row]\n",
      "    unique_values = set(flat_grid)\n",
      "    value_counts = {value: flat_grid.count(value) for value in unique_values}\n",
      "    sorted_values = sorted(unique_values, key=lambda x: (value_counts[x], x))\n",
      "    output_shape = (len(sorted_values), len(sorted_values))\n",
      "    output_grid = [[0] * output_shape[1] for _ in range(output_shape[0])]\n",
      "    for i, value in enumerate(sorted_values):\n",
      "        output_grid[i][i] = value\n",
      "    return output_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    rows, cols = (len(grid), len(grid[0]))\n",
      "    for i in range(rows):\n",
      "        for j in range(cols):\n",
      "            if grid[i][j] == 5:\n",
      "                for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
      "                    ni, nj = (i + dx, j + dy)\n",
      "                    if 0 <= ni < rows and 0 <= nj < cols and (grid[ni][nj] == 8):\n",
      "                        for ci in range(max(0, i - 2), min(rows, i + 3)):\n",
      "                            for cj in range(max(0, j - 2), min(cols, j + 3)):\n",
      "                                if grid[ci][cj] == 0:\n",
      "                                    grid[ci][cj] = 4\n",
      "                        break\n",
      "                else:\n",
      "                    for ci in range(max(0, i - 2), min(rows, i + 3)):\n",
      "                        for cj in range(max(0, j - 2), min(cols, j + 3)):\n",
      "                            if grid[ci][cj] == 0:\n",
      "                                grid[ci][cj] = 2\n",
      "    return grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    top_left_subgrid = [row[:2] for row in grid_lst[:2]]\n",
      "    bottom_right_subgrid = [row[-2:] for row in grid_lst[-2:]]\n",
      "    output_grid = [[top_left_subgrid[0][1], bottom_right_subgrid[0][1]], [bottom_right_subgrid[1][0], top_left_subgrid[1][0]]]\n",
      "    return output_grid\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    rows, cols = grid.shape\n",
      "    output_grid = np.copy(grid)\n",
      "    non_one_positions = np.argwhere(grid != 1)\n",
      "    for pos in non_one_positions:\n",
      "        x, y = pos\n",
      "        value = grid[x, y]\n",
      "        if value in [2, 3, 4, 6, 8]:\n",
      "            for i in range(1, min(rows, cols)):\n",
      "                if x + i < rows and output_grid[x + i, y] == 1:\n",
      "                    output_grid[x + i, y] = value\n",
      "                    break\n",
      "                if x - i >= 0 and output_grid[x - i, y] == 1:\n",
      "                    output_grid[x - i, y] = value\n",
      "                    break\n",
      "                if y + i < cols and output_grid[x, y + i] == 1:\n",
      "                    output_grid[x, y + i] = value\n",
      "                    break\n",
      "                if y - i >= 0 and output_grid[x, y - i] == 1:\n",
      "                    output_grid[x, y - i] = value\n",
      "                    break\n",
      "    return output_grid.tolist()\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    n = len(grid)\n",
      "    m = len(grid[0])\n",
      "    top_left_rows = (n + 2) // 3\n",
      "    top_left_cols = (m + 2) // 3\n",
      "    for i in range(n):\n",
      "        for j in range(m):\n",
      "            if i < top_left_rows and j < top_left_cols:\n",
      "                continue\n",
      "            elif i < top_left_rows and j >= top_left_cols:\n",
      "                grid[i][j] = grid[i][m - j - 1]\n",
      "            elif i >= top_left_rows and j < top_left_cols:\n",
      "                grid[i][j] = grid[n - i - 1][j]\n",
      "            else:\n",
      "                grid[i][j] = grid[n - i - 1][m - j - 1]\n",
      "    return grid\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = np.array(grid_lst)\n",
      "    new_grid = np.copy(grid)\n",
      "    unique_colors = np.setdiff1d(np.unique(grid), [0])\n",
      "    if len(unique_colors) < 2:\n",
      "        return grid_lst\n",
      "    color1 = unique_colors[0]\n",
      "    color2 = unique_colors[1] if len(unique_colors) > 1 else None\n",
      "    color3 = unique_colors[2] if len(unique_colors) > 2 else None\n",
      "    color4 = unique_colors[3] if len(unique_colors) > 3 else None\n",
      "    coords1 = np.argwhere(grid == color1)\n",
      "    coords2 = np.argwhere(grid == color2) if color2 is not None else np.array([])\n",
      "    coords3 = np.argwhere(grid == color3) if color3 is not None else np.array([])\n",
      "    coords4 = np.argwhere(grid == color4) if color4 is not None else np.array([])\n",
      "    for coord in coords2:\n",
      "        closest1 = min(coords1, key=lambda x: np.linalg.norm(x - coord))\n",
      "        if np.linalg.norm(coord - closest1) == 1:\n",
      "            new_grid[coord[0], coord[1]] = color1\n",
      "    for coord in coords3:\n",
      "        closest1 = min(coords1, key=lambda x: np.linalg.norm(x - coord))\n",
      "        closest2 = min(coords2, key=lambda x: np.linalg.norm(x - coord))\n",
      "        if np.linalg.norm(coord - closest1) == 1:\n",
      "            new_grid[coord[0], coord[1]] = color1\n",
      "        elif np.linalg.norm(coord - closest2) == 1:\n",
      "            new_grid[coord[0], coord[1]] = color2\n",
      "    for coord in coords4:\n",
      "        closest1 = min(coords1, key=lambda x: np.linalg.norm(x - coord))\n",
      "        closest2 = min(coords2, key=lambda x: np.linalg.norm(x - coord))\n",
      "        closest3 = min(coords3, key=lambda x: np.linalg.norm(x - coord))\n",
      "        if np.linalg.norm(coord - closest1) == 1:\n",
      "            new_grid[coord[0], coord[1]] = color1\n",
      "        elif np.linalg.norm(coord - closest2) == 1:\n",
      "            new_grid[coord[0], coord[1]] = color2\n",
      "        elif np.linalg.norm(coord - closest3) == 1:\n",
      "            new_grid[coord[0], coord[1]] = color3\n",
      "    return new_grid.tolist()\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    rows, cols = (len(grid), len(grid[0]))\n",
      "    max_val = max((max(row) for row in grid))\n",
      "\n",
      "    def next_value(val):\n",
      "        return val % max_val + 1 if max_val > 1 else 0\n",
      "\n",
      "    def is_valid(i, j):\n",
      "        return 0 <= i < rows and 0 <= j < cols\n",
      "\n",
      "    def get_next_non_zero(i, j):\n",
      "        if is_valid(i, j + 1) and grid[i][j + 1] != 0:\n",
      "            return grid[i][j + 1]\n",
      "        elif is_valid(i + 1, j) and grid[i + 1][j] != 0:\n",
      "            return grid[i + 1][j]\n",
      "        elif is_valid(i - 1, j) and grid[i - 1][j] != 0:\n",
      "            return grid[i - 1][j]\n",
      "        elif is_valid(i, j - 1) and grid[i][j - 1] != 0:\n",
      "            return grid[i][j - 1]\n",
      "        return 0\n",
      "    for i in range(rows):\n",
      "        for j in range(cols):\n",
      "            if grid[i][j] == 0:\n",
      "                next_non_zero = get_next_non_zero(i, j)\n",
      "                if next_non_zero != 0:\n",
      "                    grid[i][j] = next_value(next_non_zero)\n",
      "                else:\n",
      "                    grid[i][j] = next_value(max_val)\n",
      "    return grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    grey_color = 5\n",
      "    grey_column = next((i for i, cell in enumerate(grid[0]) if cell == grey_color), None)\n",
      "    if grey_column is None:\n",
      "        return grid\n",
      "    new_grid = [[0 for _ in range(len(grid[0]))] for _ in range(len(grid))]\n",
      "    for i, row in enumerate(grid):\n",
      "        for j, cell in enumerate(row):\n",
      "            if cell != grey_color:\n",
      "                new_grid[i][j] = cell\n",
      "    output_grid = [[0 for _ in range(len(grid[0]))] for _ in range(len(grid))]\n",
      "    for i, row in enumerate(new_grid):\n",
      "        for j, cell in enumerate(row):\n",
      "            if cell != 0:\n",
      "                if j < grey_column:\n",
      "                    output_grid[i][j] = cell\n",
      "                elif j > grey_column:\n",
      "                    output_grid[i][j - 1] = cell\n",
      "    for i, row in enumerate(output_grid):\n",
      "        for j, cell in enumerate(row):\n",
      "            if cell != 0:\n",
      "                if j < grey_column:\n",
      "                    output_grid[i][grey_column - 1] = cell\n",
      "                elif j > grey_column:\n",
      "                    output_grid[i][grey_column + 1] = cell\n",
      "    for i, row in enumerate(output_grid):\n",
      "        for j, cell in enumerate(row):\n",
      "            if cell != 0:\n",
      "                if j < grey_column:\n",
      "                    output_grid[i][grey_column - 1] = cell\n",
      "                elif j > grey_column:\n",
      "                    output_grid[i][grey_column + 1] = cell\n",
      "    for i, row in enumerate(output_grid):\n",
      "        for j, cell in enumerate(row):\n",
      "            if cell != 0:\n",
      "                if j < grey_column:\n",
      "                    output_grid[i][grey_column - 1] = cell\n",
      "                elif j > grey_column:\n",
      "                    output_grid[i][grey_column + 1] = cell\n",
      "    reduced_grid = [[0 for _ in range(10)] for _ in range(10)]\n",
      "    for i in range(10):\n",
      "        for j in range(10):\n",
      "            reduced_grid[i][j] = output_grid[i][j]\n",
      "    return reduced_grid\n",
      "\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    rows = len(grid_lst)\n",
      "    cols = len(grid_lst[0])\n",
      "    colored_rows = [row for row in grid_lst if any((cell != 0 for cell in row))]\n",
      "    output_grid = [[0] * (2 * cols) for _ in range(2 * rows)]\n",
      "    for i, row in enumerate(colored_rows):\n",
      "        for j, cell in enumerate(row):\n",
      "            if cell != 0:\n",
      "                output_grid[i][j] = cell\n",
      "                output_grid[i + rows][j + cols] = cell\n",
      "    for i in range(2 * rows):\n",
      "        if all((output_grid[i][j] == 0 for j in range(2 * cols))):\n",
      "            output_grid[i] = [3] * (2 * cols)\n",
      "    return output_grid\n"
     ]
    }
   ],
   "source": [
    "for file in parquet_files[:1]:\n",
    "    df = read_soar_parquet(file)\n",
    "    print('\\n\\n'.join(df['code'].sample(20, random_state=42).tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trelis-arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
