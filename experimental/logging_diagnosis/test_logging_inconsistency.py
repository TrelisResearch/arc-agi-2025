#!/usr/bin/env python3
"""
Test suite to diagnose the logging inconsistency between train categorization and best attempt selection.

Issue: We see "10 train-partial" but "best: 0.0% train" which seems impossible given the logic.

Hypothesis 1: All partial programs have outputs_valid=False
Hypothesis 2: A test-correct program with 0% train beats all partials
Hypothesis 3: Bug in the logic somewhere
"""

import sys
from typing import List, Dict, Any
from pathlib import Path

# Add the project root to the path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def simulate_logging_logic(attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Simulate the exact logging logic from run_arc_tasks_soar.py lines 1795-1821
    """
    # Initialize counters (matching the actual code)
    train_perfect_total = train_perfect_trans = 0
    train_partial_total = train_partial_trans = 0
    train_incorrect_total = train_incorrect_trans = 0

    # Train categorization logic (lines 1798-1815)
    for att in attempts:
        if not att.get("outputs_valid", False):
            continue  # Skip invalid attempts
        is_trans = att.get("is_transductive", False)
        train_acc = att.get("train_accuracy", 0.0)

        if train_acc == 1.0:
            train_perfect_total += 1
            if is_trans:
                train_perfect_trans += 1
        elif train_acc > 0:
            train_partial_total += 1
            if is_trans:
                train_partial_trans += 1
        else:
            train_incorrect_total += 1
            if is_trans:
                train_incorrect_trans += 1

    # Best attempt selection logic (lines 1818-1821)
    best_attempt = max(
        attempts,
        key=lambda x: (x.get("test_correct", False), x.get("train_accuracy", 0.0)),
    )

    return {
        "train_perfect_total": train_perfect_total,
        "train_partial_total": train_partial_total,
        "train_incorrect_total": train_incorrect_total,
        "best_attempt": best_attempt,
        "best_train_accuracy": best_attempt.get("train_accuracy", 0.0),
        "best_test_correct": best_attempt.get("test_correct", False),
        "best_outputs_valid": best_attempt.get("outputs_valid", False),
    }


def test_hypothesis_1_all_partials_invalid():
    """
    Hypothesis 1: All partial programs have outputs_valid=False
    This would explain why we see 0 train-partial but best > 0%
    """
    print("=== Testing Hypothesis 1: All partials have invalid outputs ===")

    attempts = [
        # Partial programs with invalid outputs
        {"train_accuracy": 0.3, "test_correct": False, "outputs_valid": False, "is_transductive": False},
        {"train_accuracy": 0.5, "test_correct": False, "outputs_valid": False, "is_transductive": False},
        {"train_accuracy": 0.7, "test_correct": False, "outputs_valid": False, "is_transductive": False},

        # Programs with valid outputs but 0% train
        {"train_accuracy": 0.0, "test_correct": False, "outputs_valid": True, "is_transductive": False},
        {"train_accuracy": 0.0, "test_correct": False, "outputs_valid": True, "is_transductive": False},
    ]

    result = simulate_logging_logic(attempts)

    print(f"Train partial count: {result['train_partial_total']}")
    print(f"Best train accuracy: {result['best_train_accuracy']:.1%}")
    print(f"Best test correct: {result['best_test_correct']}")
    print(f"Best outputs valid: {result['best_outputs_valid']}")

    # This should show 0 train-partial but best = 70% train
    expected_partials = 0  # All partials have invalid outputs
    expected_best = 0.7    # Highest train accuracy among all attempts

    assert result['train_partial_total'] == expected_partials, f"Expected {expected_partials} partials, got {result['train_partial_total']}"
    assert result['best_train_accuracy'] == expected_best, f"Expected {expected_best} best, got {result['best_train_accuracy']}"

    print("‚úÖ Hypothesis 1 CONFIRMED: All partials invalid -> 0 partial count but >0% best")
    print()


def test_hypothesis_2_test_correct_wins():
    """
    Hypothesis 2: A test-correct program with 0% train beats all partials
    The sorting key prioritizes test_correct over train_accuracy
    """
    print("=== Testing Hypothesis 2: Test-correct program wins despite 0% train ===")

    attempts = [
        # Partial programs with valid outputs
        {"train_accuracy": 0.3, "test_correct": False, "outputs_valid": True, "is_transductive": False},
        {"train_accuracy": 0.5, "test_correct": False, "outputs_valid": True, "is_transductive": False},
        {"train_accuracy": 0.7, "test_correct": False, "outputs_valid": True, "is_transductive": False},

        # Test-correct program with 0% train (lucky guess?)
        {"train_accuracy": 0.0, "test_correct": True, "outputs_valid": True, "is_transductive": False},
    ]

    result = simulate_logging_logic(attempts)

    print(f"Train partial count: {result['train_partial_total']}")
    print(f"Best train accuracy: {result['best_train_accuracy']:.1%}")
    print(f"Best test correct: {result['best_test_correct']}")
    print(f"Best outputs valid: {result['best_outputs_valid']}")

    # This should show 3 train-partial but best = 0% train (because test_correct=True wins)
    expected_partials = 3  # Three valid partial programs
    expected_best = 0.0    # Test-correct program has 0% train but wins due to sort key

    assert result['train_partial_total'] == expected_partials, f"Expected {expected_partials} partials, got {result['train_partial_total']}"
    assert result['best_train_accuracy'] == expected_best, f"Expected {expected_best} best, got {result['best_train_accuracy']}"
    assert result['best_test_correct'] == True, "Expected best attempt to have test_correct=True"

    print("‚úÖ Hypothesis 2 CONFIRMED: Test-correct with 0% train beats partials")
    print()


def test_hypothesis_3_edge_case_bug():
    """
    Hypothesis 3: Look for edge cases in the logic that could cause bugs
    """
    print("=== Testing Hypothesis 3: Edge cases and potential bugs ===")

    # Test with empty attempts list
    print("Testing empty attempts list...")
    try:
        result = simulate_logging_logic([])
        print("‚ùå BUG: Empty list should fail but didn't!")
    except ValueError as e:
        print(f"‚úÖ Empty list properly raises error: {e}")

    # Test with all attempts having missing fields
    print("Testing attempts with missing fields...")
    attempts = [
        {},  # Completely empty
        {"train_accuracy": 0.5},  # Missing other fields
        {"outputs_valid": True},  # Missing train_accuracy
    ]

    result = simulate_logging_logic(attempts)
    print(f"Result with missing fields: {result}")

    # Test with negative train_accuracy (shouldn't happen but let's see)
    print("Testing negative train accuracy...")
    attempts = [
        {"train_accuracy": -0.1, "test_correct": False, "outputs_valid": True, "is_transductive": False},
        {"train_accuracy": 0.5, "test_correct": False, "outputs_valid": True, "is_transductive": False},
    ]

    result = simulate_logging_logic(attempts)
    print(f"Negative train accuracy result: {result}")

    print()


def test_real_world_scenario():
    """
    Test the exact scenario from the log: 10 train-partial but best: 0.0% train
    """
    print("=== Testing Real-World Scenario: 10 partials, best 0% ===")

    # Create a scenario that could produce the observed behavior
    attempts = []

    # Add 10 partial programs with invalid outputs (explaining the count discrepancy)
    for i in range(10):
        attempts.append({
            "train_accuracy": 0.2 + i * 0.05,  # 20% to 65%
            "test_correct": False,
            "outputs_valid": False,  # Invalid outputs!
            "is_transductive": i < 3,  # First 3 are transductive
        })

    # Add 47 programs with 0% train and valid outputs
    for i in range(47):
        attempts.append({
            "train_accuracy": 0.0,
            "test_correct": False,
            "outputs_valid": True,
            "is_transductive": i < 4,  # First 4 are transductive
        })

    # Add 2 test-perfect programs (but 0% train - lucky guesses)
    for i in range(2):
        attempts.append({
            "train_accuracy": 0.0,  # 0% train but test correct!
            "test_correct": True,
            "outputs_valid": True,
            "is_transductive": False,
        })

    result = simulate_logging_logic(attempts)

    print(f"Train partial count: {result['train_partial_total']}")
    print(f"Train incorrect count: {result['train_incorrect_total']}")
    print(f"Best train accuracy: {result['best_train_accuracy']:.1%}")
    print(f"Best test correct: {result['best_test_correct']}")
    print(f"Best outputs valid: {result['best_outputs_valid']}")

    # This should match the observed behavior
    assert result['train_partial_total'] == 0, "Partials should be 0 due to invalid outputs"
    assert result['train_incorrect_total'] == 49, f"Should have train-incorrect with valid outputs, got {result['train_incorrect_total']}"
    assert result['best_train_accuracy'] == 0.0, "Best should be 0% train"
    assert result['best_test_correct'] == True, "Best should be test-correct"

    print("‚úÖ Real-world scenario EXPLAINED: Partials have invalid outputs, test-correct programs win")
    print()


def test_fix_suggestion():
    """
    Test what the logging would look like with consistent filtering
    """
    print("=== Testing Fix: Consistent Filtering ===")

    attempts = [
        # Partial programs with invalid outputs
        {"train_accuracy": 0.3, "test_correct": False, "outputs_valid": False, "is_transductive": False},
        {"train_accuracy": 0.5, "test_correct": False, "outputs_valid": False, "is_transductive": False},

        # Programs with valid outputs
        {"train_accuracy": 0.0, "test_correct": False, "outputs_valid": True, "is_transductive": False},
        {"train_accuracy": 0.0, "test_correct": True, "outputs_valid": True, "is_transductive": False},
    ]

    # Current logic (inconsistent)
    current_result = simulate_logging_logic(attempts)

    # Fixed logic (consistent filtering)
    valid_attempts = [att for att in attempts if att.get("outputs_valid", False)]
    fixed_best = max(
        valid_attempts,
        key=lambda x: (x.get("test_correct", False), x.get("train_accuracy", 0.0)),
    ) if valid_attempts else {"train_accuracy": 0.0}

    print("Current (inconsistent) logic:")
    print(f"  Train partial count: {current_result['train_partial_total']}")
    print(f"  Best train accuracy: {current_result['best_train_accuracy']:.1%}")

    print("Fixed (consistent) logic:")
    print(f"  Train partial count: {current_result['train_partial_total']}")  # Same
    print(f"  Best train accuracy: {fixed_best.get('train_accuracy', 0.0):.1%}")

    print("‚úÖ Fix would make the logging consistent")
    print()


if __name__ == "__main__":
    print("Diagnosing train categorization vs best attempt logging inconsistency")
    print("=" * 70)
    print()

    try:
        test_hypothesis_1_all_partials_invalid()
        test_hypothesis_2_test_correct_wins()
        test_hypothesis_3_edge_case_bug()
        test_real_world_scenario()
        test_fix_suggestion()

        print("üéØ DIAGNOSIS COMPLETE")
        print("=" * 70)
        print("KEY FINDINGS:")
        print("1. ‚úÖ Hypothesis 1: Programs with partial train success can have invalid outputs")
        print("2. ‚úÖ Hypothesis 2: Test-correct programs with 0% train beat all partials in sort")
        print("3. ‚úÖ Real scenario: Mix of both causes the observed behavior")
        print("4. ‚úÖ Fix: Use consistent filtering for both metrics")

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()