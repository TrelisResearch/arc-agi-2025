#!/usr/bin/env python3
"""
ARC Diffusion Backbone Training with JSON Config Support

Trains the diffusion model with integrated size prediction head.

Usage:
    uv run python experimental/diffusion/train_diffusion_backbone.py --config experimental/diffusion/configs/smol_config.json
    uv run python experimental/diffusion/train_diffusion_backbone.py --config experimental/diffusion/configs/smol_config.json --no-wandb

Config file structure:
    {
        "model": {d_model, nhead, num_layers, max_size, etc.},
        "training": {batch_size, learning_rate, optimizer_steps, etc.},
        "data": {data_dir, datasets, etc.},
        "auxiliary_loss": {include_size_head, size_head_hidden_dim, etc.},
        "output": {output_dir, use_wandb, etc.}
    }
"""
import json
import argparse
import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from experimental.diffusion.src.training import train_arc_diffusion


def load_and_flatten_config(config_path: str) -> dict:
    """
    Load configuration from JSON file and flatten nested structure.

    The training function expects a flat dictionary, so we flatten the nested
    config structure while preserving the auxiliary_loss section.
    """
    with open(config_path, 'r') as f:
        config = json.load(f)

    # Flatten nested config sections
    flat_config = {}

    # Copy top-level fields (model_version, tag, etc.)
    for key in ['model_version', 'tag']:
        if key in config:
            flat_config[key] = config[key]

    # Flatten model, training, data, and output sections
    for section in ['model', 'training', 'data', 'output']:
        if section in config:
            flat_config.update(config[section])

    # Keep auxiliary_loss as nested dict (expected by training.py)
    if 'auxiliary_loss' in config:
        flat_config['auxiliary_loss'] = config['auxiliary_loss']

    return flat_config


def main():
    parser = argparse.ArgumentParser(
        description="Train ARC Diffusion Model with JSON Config",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--config",
        required=True,
        help="Path to JSON config file (see configs/ for examples)"
    )
    parser.add_argument(
        "--no-wandb",
        action="store_true",
        help="Disable wandb logging (overrides config)"
    )
    parser.add_argument(
        "--profile",
        action="store_true",
        help="Run profiling mode: execute 20 steps with PyTorch profiler and output timing breakdown"
    )

    args = parser.parse_args()

    # Validate config file exists
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"‚ùå Config file not found: {config_path}")
        sys.exit(1)

    # Load and flatten config
    try:
        config = load_and_flatten_config(args.config)
    except Exception as e:
        print(f"‚ùå Failed to load config: {e}")
        sys.exit(1)

    # Override wandb setting if requested
    if args.no_wandb:
        config['use_wandb'] = False

    # Add profiling flag to config
    config['profile_mode'] = args.profile

    print(f"‚ú® Training ARC Diffusion Model")
    print(f"üìÅ Config: {args.config}")
    print(f"üìä Model: {config['d_model']}d, {config['num_layers']} layers")
    print(f"üéØ Training: {config['optimizer_steps']} steps, batch_size={config['batch_size']}")
    print(f"üíæ Output: {config['output_dir']}")
    if args.profile:
        print(f"üî¨ Profiling mode: Will run 20 steps with PyTorch profiler")

    # Train model
    try:
        model = train_arc_diffusion(config=config)
        print("‚úÖ Training completed successfully!")
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Training interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()