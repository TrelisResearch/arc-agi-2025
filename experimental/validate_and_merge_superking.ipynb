{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0638b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "from llm_python.datasets.io import read_soar_parquet\n",
    "\n",
    "# Get all parquet file paths\n",
    "parquet_files = glob(\"/tmp/superking_snapshot/*.parquet\")\n",
    "\n",
    "# Read each parquet file into a dataframe using read_soar_parquet()\n",
    "dfs = []\n",
    "for f in parquet_files:\n",
    "    try:\n",
    "        df = read_soar_parquet(f)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "# Merge all dataframes into a single dataframe\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Merged {len(parquet_files)} parquet files with total {len(merged_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c434e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.transduction.code_classifier import CodeTransductionClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "transduction_classifier = CodeTransductionClassifier()\n",
    "\n",
    "print(\"Applying transduction classifier...\")\n",
    "merged_df[\"is_transductive\"] = [\n",
    "    transduction_classifier.is_transductive(row[\"code\"])[0]\n",
    "    for _, row in tqdm(merged_df.iterrows(), total=len(merged_df))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[0].info())\n",
    "print(dfs[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff01911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.io import validate_soar_dataframe_schema\n",
    "from llm_python.utils.numpy import convert_numpy_types\n",
    "\n",
    "merged_df[\"predicted_train_output\"] = merged_df[\"predicted_train_output\"].apply(convert_numpy_types)\n",
    "merged_df[\"correct_train_input\"] = merged_df[\"correct_train_input\"].apply(convert_numpy_types)\n",
    "merged_df[\"predicted_test_output\"] = merged_df[\"predicted_test_output\"].apply(convert_numpy_types)\n",
    "merged_df[\"correct_test_input\"] = merged_df[\"correct_test_input\"].apply(convert_numpy_types)\n",
    "validate_soar_dataframe_schema(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171de997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.validation import validate_soar_row\n",
    "\n",
    "print(\"Validating rows formats...\")\n",
    "valid_mask = []\n",
    "errors = []\n",
    "for i, row in tqdm(merged_df.iterrows(), total=len(merged_df)):\n",
    "    result = validate_soar_row(row)\n",
    "    valid_mask.append(result.is_valid)\n",
    "    if not result.is_valid:\n",
    "        errors.append((i, result.errors))\n",
    "\n",
    "# Filter out invalid rows\n",
    "merged_df = merged_df[pd.Series(valid_mask, index=merged_df.index)]\n",
    "print(f\"Filtered dataframe: {len(merged_df)} valid rows out of {len(valid_mask)} total.\")\n",
    "print(f\"Total invalid rows: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.validation import validate_soar_dataframe_correctness\n",
    "\n",
    "\n",
    "print(\"Validating row correctness...\")\n",
    "\n",
    "correctness_result = validate_soar_dataframe_correctness(merged_df, correctness_samples=1000)\n",
    "print(correctness_result.summary())\n",
    "if not correctness_result.is_valid:\n",
    "    raise ValueError(\n",
    "        \"Validation failed: Some programs do not meet the correctness requirements.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.utils.arc_tester import ArcTester\n",
    "from llm_python.utils.task_loader import get_task_loader\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Original columns: {df.columns.tolist()}\")\n",
    "\n",
    "task_loader = get_task_loader()\n",
    "\n",
    "\n",
    "def process_single_row(row_data):\n",
    "    \"\"\"Process a single row - this function will be called in parallel\"\"\"\n",
    "    idx, row = row_data\n",
    "    try:\n",
    "        # Create instances inside the worker process\n",
    "        arc_tester = ArcTester()\n",
    "\n",
    "        result = arc_tester.test_program(\n",
    "            row[\"code\"], task_loader.get_task(row[\"task_id\"])\n",
    "        )\n",
    "\n",
    "        # Create corrected row with actual values from arc_tester\n",
    "        corrected_row = row.copy()\n",
    "        corrected_row[\"predicted_train_output\"] = result.train_outputs\n",
    "        corrected_row[\"predicted_test_output\"] = result.test_outputs\n",
    "        corrected_row[\"correct_train_input\"] = result.correct_train_input\n",
    "        corrected_row[\"correct_test_input\"] = result.correct_test_input\n",
    "\n",
    "        return (\"success\", idx, corrected_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"failed\", idx, str(e))\n",
    "\n",
    "\n",
    "# Determine optimal number of workers\n",
    "num_workers = min(mp.cpu_count() - 2, 8)  # Don't use too many to avoid memory issues\n",
    "print(f\"Using {num_workers} parallel workers\")\n",
    "\n",
    "total_rows = len(merged_df)\n",
    "all_corrected_rows = []\n",
    "all_failed_indices = []\n",
    "\n",
    "print(f\"Processing {total_rows}...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "corrected_rows = []\n",
    "failed_indices = []\n",
    "\n",
    "# Process batch in parallel with progress bar\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    futures = {\n",
    "        executor.submit(process_single_row, row): row[0] for row in merged_df.iterrows()\n",
    "    }\n",
    "    corrected_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=\"Processing Rows\",\n",
    "        unit=\" row\",\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\",\n",
    "    )\n",
    "\n",
    "    for future in pbar:\n",
    "        try:\n",
    "            result_type, idx, result_data = future.result()\n",
    "            if result_type == \"success\":\n",
    "                corrected_rows.append(result_data)\n",
    "                corrected_count += 1\n",
    "            else:\n",
    "                failed_indices.append(idx)\n",
    "                failed_count += 1\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            failed_idx = futures[future]\n",
    "            failed_indices.append(failed_idx)\n",
    "            print(f\"\\nAn error occurred processing row {failed_idx}: {e}\")\n",
    "\n",
    "        pbar.set_postfix(completed=corrected_count, failed=failed_count, refresh=True)\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Total successful rows: {len(corrected_rows)}\")\n",
    "print(f\"Total failed rows: {len(failed_indices)}\")\n",
    "print(f\"Total time: {(time.time() - start_time) / 60:.1f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_df = pd.DataFrame(corrected_rows)\n",
    "fixed_df = fixed_df[~fixed_df[\"code\"].str.lower().str.contains(\"random|randbelow|rvs\")]\n",
    "print(f\"Kept {len(fixed_df)}/{len(corrected_rows)} rows after filtering for randomness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.validation import validate_soar_dataframe_correctness\n",
    "\n",
    "\n",
    "print(\"Validating row correctness...\")\n",
    "\n",
    "correctness_result = validate_soar_dataframe_correctness(fixed_df, correctness_samples=10000, seed=41)\n",
    "print(correctness_result.summary())\n",
    "if not correctness_result.is_valid:\n",
    "    raise ValueError(\n",
    "        \"Validation failed: Some programs do not meet the correctness requirements.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9206703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_python.datasets.io import write_soar_parquet\n",
    "\n",
    "\n",
    "write_soar_parquet(fixed_df, \"/tmp/superking_merged_and_cleaned.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trelis-arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
