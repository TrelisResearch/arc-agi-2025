{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a4ba49",
   "metadata": {},
   "source": [
    "# Transduction Detection Analysis: Old vs New Methods\n",
    "\n",
    "This notebook compares the old hardcoded-value transduction detection with the new augmentation-invariance method on the superking dataset.\n",
    "\n",
    "## Overview\n",
    "- **Old method**: Detects hardcoded training/test outputs by string matching\n",
    "- **New method**: Tests augmentation invariance (vertical flip, horizontal flip, color rotation)\n",
    "- **Dataset**: BigQuery superking dataset (`trelis-arc.arc.superking`)\n",
    "\n",
    "## Analysis Goals\n",
    "1. Download all programs and task_ids from superking dataset\n",
    "2. Apply both transduction detection methods\n",
    "3. Compare classification differences and analyze patterns\n",
    "4. Identify cases where methods disagree and understand why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063a0e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n",
      "âœ“ Project root: /home/lewis/code/trelis-arc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/code/trelis-arc/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Setup project path and imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import our transduction detection functions\n",
    "from llm_python.utils.transduction import detect_transduction, detect_transduction_augmentation\n",
    "from llm_python.utils.arc_tester import ArcTester\n",
    "from llm_python.utils.task_loader import TaskData\n",
    "from llm_python.datasets.bigquery_export import load_bigquery_table_as_dataframe\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"âœ“ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a16c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading superking dataset from BigQuery...\n",
      "Table: trelis-arc.arc.superking\n",
      "Exporting BigQuery table 'trelis-arc.arc.superking' to GCS with sharding...\n",
      "Waiting for BigQuery export to complete...\n",
      "âŒ Error downloading data: 400 trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.\n",
      "Waiting for BigQuery export to complete...\n",
      "âŒ Error downloading data: 400 trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequest\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Use our reusable function to load the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     raw_data = \u001b[43mload_bigquery_table_as_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use sharding in case dataset is large\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Downloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m programs from superking dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(raw_data.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/trelis-arc/llm_python/datasets/bigquery_export.py:194\u001b[39m, in \u001b[36mload_bigquery_table_as_dataframe\u001b[39m\u001b[34m(client, table_name, gcs_bucket, gcs_prefix, use_sharding)\u001b[39m\n\u001b[32m    191\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Use BigQuery's built-in sharding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m local_path = \u001b[43mexport_bigquery_table_to_local\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgcs_bucket\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgcs_bucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgcs_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgcs_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReading combined parquet file...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m df = pd.read_parquet(local_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/trelis-arc/llm_python/datasets/bigquery_export.py:65\u001b[39m, in \u001b[36mexport_bigquery_table_to_local\u001b[39m\u001b[34m(client, table_name, local_path, gcs_bucket, gcs_prefix, use_sharding)\u001b[39m\n\u001b[32m     58\u001b[39m extract_job = client.extract_table(\n\u001b[32m     59\u001b[39m     table_name,\n\u001b[32m     60\u001b[39m     gcs_uri,\n\u001b[32m     61\u001b[39m     job_config=export_job_config\n\u001b[32m     62\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWaiting for BigQuery export to complete...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mextract_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for export to complete\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Export to GCS completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Download all sharded files\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/trelis-arc/.venv/lib/python3.11/site-packages/google/cloud/bigquery/job/base.py:1001\u001b[39m, in \u001b[36m_AsyncJob.result\u001b[39m\u001b[34m(self, retry, timeout)\u001b[39m\n\u001b[32m    998\u001b[39m     \u001b[38;5;28mself\u001b[39m._begin(retry=retry, timeout=timeout)\n\u001b[32m   1000\u001b[39m kwargs = {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mretry\u001b[39m\u001b[33m\"\u001b[39m: retry}\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/trelis-arc/.venv/lib/python3.11/site-packages/google/api_core/future/polling.py:261\u001b[39m, in \u001b[36mPollingFuture.result\u001b[39m\u001b[34m(self, timeout, retry, polling)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._blocking_poll(timeout=timeout, retry=retry, polling=polling)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[31mBadRequest\u001b[39m: 400 trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL.; reason: invalid, message: trelis-arc:arc.superking is not allowed for this operation because it currently has type EXTERNAL."
     ]
    }
   ],
   "source": [
    "# Step 1: Download superking dataset from Google Cloud Storage\n",
    "from google.cloud import storage\n",
    "import glob\n",
    "\n",
    "# GCS bucket and path\n",
    "gcs_bucket = \"trelis-arc\"\n",
    "gcs_prefix = \"datasets/superking\"\n",
    "local_download_dir = \"/tmp/superking_data\"\n",
    "output_path = \"/tmp/superking_analysis.parquet\"\n",
    "\n",
    "print(\"Downloading superking dataset from Google Cloud Storage...\")\n",
    "print(f\"Bucket: gs://{gcs_bucket}/{gcs_prefix}\")\n",
    "\n",
    "# Create local directory\n",
    "Path(local_download_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(gcs_bucket)\n",
    "    \n",
    "    # List all parquet files in the superking dataset\n",
    "    blobs = list(bucket.list_blobs(prefix=gcs_prefix))\n",
    "    parquet_blobs = [blob for blob in blobs if blob.name.endswith('.parquet')]\n",
    "    \n",
    "    print(f\"âœ“ Found {len(parquet_blobs)} parquet files in {gcs_prefix}\")\n",
    "    \n",
    "    # Download all parquet files\n",
    "    local_files = []\n",
    "    for blob in parquet_blobs:\n",
    "        filename = blob.name.split('/')[-1]  # Get just the filename\n",
    "        local_path = f\"{local_download_dir}/{filename}\"\n",
    "        \n",
    "        print(f\"  Downloading {blob.name} -> {local_path}\")\n",
    "        blob.download_to_filename(local_path)\n",
    "        local_files.append(local_path)\n",
    "    \n",
    "    print(f\"âœ“ Downloaded {len(local_files)} files\")\n",
    "    \n",
    "    # Combine all parquet files into a single DataFrame\n",
    "    print(\"Combining parquet files...\")\n",
    "    dataframes = []\n",
    "    for file_path in local_files:\n",
    "        df_part = pd.read_parquet(file_path)\n",
    "        print(f\"  {file_path}: {len(df_part):,} rows\")\n",
    "        dataframes.append(df_part)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    raw_data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"âœ“ Combined dataset: {len(raw_data):,} programs\")\n",
    "    print(f\"âœ“ Columns: {list(raw_data.columns)}\")\n",
    "    \n",
    "    # Save combined dataset to local parquet for analysis\n",
    "    raw_data.to_parquet(output_path, index=False)\n",
    "    print(f\"âœ“ Saved combined dataset to: {output_path}\")\n",
    "    \n",
    "    # Show sample of the data\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(raw_data.head())\n",
    "    \n",
    "    # Clean up individual downloaded files\n",
    "    import os\n",
    "    for file_path in local_files:\n",
    "        os.remove(file_path)\n",
    "    os.rmdir(local_download_dir)\n",
    "    print(f\"âœ“ Cleaned up temporary files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error downloading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Examine data structure and prepare for analysis\n",
    "df = pd.read_parquet(output_path)\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"- Total programs: {len(df):,}\")\n",
    "print(f\"- Unique tasks: {df['task_id'].nunique():,}\")\n",
    "print(f\"- Columns: {list(df.columns)}\")\n",
    "print(f\"- Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Check what columns we have for task data\n",
    "print(f\"\\nColumn details:\")\n",
    "for col in df.columns:\n",
    "    print(f\"- {col}: {df[col].dtype}\")\n",
    "    if col in ['predicted_train_output', 'predicted_test_output']:\n",
    "        print(f\"  Sample value type: {type(df[col].iloc[0])}\")\n",
    "\n",
    "# Look at a sample program\n",
    "print(f\"\\nSample program:\")\n",
    "sample_row = df.iloc[0]\n",
    "print(f\"Task ID: {sample_row['task_id']}\")\n",
    "print(f\"Code length: {len(sample_row['code'])} characters\")\n",
    "print(f\"Code preview:\\n{sample_row['code'][:200]}...\")\n",
    "\n",
    "# Check if we have task data or need to load it separately\n",
    "if 'task_data' in df.columns:\n",
    "    print(\"âœ“ Task data is included in the dataset\")\n",
    "else:\n",
    "    print(\"âš ï¸  Task data not included - we'll need to load it separately\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f87741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load ARC task data using TaskLoader\n",
    "from llm_python.utils.task_loader import TaskLoader\n",
    "\n",
    "# Initialize the task loader - this will automatically find and load all ARC data\n",
    "print(\"Initializing TaskLoader...\")\n",
    "task_loader = TaskLoader()\n",
    "\n",
    "print(f\"âœ“ TaskLoader initialized with {len(task_loader.tasks)} tasks available\")\n",
    "print(f\"âœ“ Available subsets: {list(task_loader.subsets.keys())[:10]}...\")  # Show first 10\n",
    "\n",
    "# Check coverage of our dataset\n",
    "unique_task_ids = set(df['task_id'].unique())\n",
    "available_tasks = set(task_loader.tasks.keys())\n",
    "covered_tasks = unique_task_ids.intersection(available_tasks)\n",
    "missing_tasks = unique_task_ids - available_tasks\n",
    "\n",
    "print(f\"âœ“ Task coverage: {len(covered_tasks)}/{len(unique_task_ids)} ({100*len(covered_tasks)/len(unique_task_ids):.1f}%)\")\n",
    "if missing_tasks:\n",
    "    print(f\"âš ï¸  Missing {len(missing_tasks)} tasks: {list(missing_tasks)[:5]}...\")\n",
    "\n",
    "# Filter to only programs with available task data\n",
    "df_filtered = df[df['task_id'].isin(available_tasks)].copy()\n",
    "print(f\"âœ“ Filtered to {len(df_filtered)} programs with task data available\")\n",
    "\n",
    "# Helper function to get task data for our analysis\n",
    "def get_task_data(task_id: str) -> TaskData:\n",
    "    \"\"\"Get task data for a given task ID\"\"\"\n",
    "    try:\n",
    "        return task_loader.get_task(task_id)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸  Task {task_id} not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run both transduction detection methods on the dataset\n",
    "# Initialize ArcTester for the new method\n",
    "arc_tester = ArcTester(timeout=2)\n",
    "\n",
    "def analyze_program_transduction(row):\n",
    "    \"\"\"Apply both old and new transduction detection to a single program\"\"\"\n",
    "    task_id = row['task_id']\n",
    "    program = row['code']\n",
    "    \n",
    "    # Get task data\n",
    "    task_data = get_task_data(task_id)\n",
    "    if task_data is None:\n",
    "        return {\n",
    "            'old_train_transductive': None,\n",
    "            'old_train_reason': 'Task not found',\n",
    "            'old_test_transductive': None, \n",
    "            'old_test_reason': 'Task not found',\n",
    "            'new_transductive': None,\n",
    "            'new_reason': 'Task not found',\n",
    "            'error': 'Task not found'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Old method: detect_transduction (hardcoded values)\n",
    "        old_train_transductive, old_train_reason, old_test_transductive, old_test_reason = detect_transduction(\n",
    "            program, task_data, debug=False\n",
    "        )\n",
    "        \n",
    "        # New method: detect_transduction_augmentation (augmentation invariance)\n",
    "        new_transductive, new_reason = detect_transduction_augmentation(\n",
    "            program, task_data, arc_tester, debug=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'old_train_transductive': old_train_transductive,\n",
    "            'old_train_reason': old_train_reason,\n",
    "            'old_test_transductive': old_test_transductive,\n",
    "            'old_test_reason': old_test_reason, \n",
    "            'new_transductive': new_transductive,\n",
    "            'new_reason': new_reason,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'old_train_transductive': None,\n",
    "            'old_train_reason': f'Error: {str(e)}',\n",
    "            'old_test_transductive': None,\n",
    "            'old_test_reason': f'Error: {str(e)}',\n",
    "            'new_transductive': None,\n",
    "            'new_reason': f'Error: {str(e)}',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"Starting transduction analysis...\")\n",
    "print(f\"Analyzing {len(df_filtered)} programs...\")\n",
    "\n",
    "# Sample a smaller subset for initial testing (can increase later)\n",
    "sample_size = min(1000, len(df_filtered))  # Start with 1000 programs\n",
    "df_sample = df_filtered.sample(n=sample_size, random_state=42).copy()\n",
    "print(f\"Using sample of {len(df_sample)} programs for analysis\")\n",
    "\n",
    "# Apply analysis to each program\n",
    "tqdm.pandas(desc=\"Analyzing programs\")\n",
    "results = df_sample.progress_apply(analyze_program_transduction, axis=1, result_type='expand')\n",
    "\n",
    "# Combine results with original data\n",
    "df_analysis = pd.concat([df_sample.reset_index(drop=True), results], axis=1)\n",
    "\n",
    "print(f\"âœ“ Analysis complete for {len(df_analysis)} programs\")\n",
    "print(f\"âœ“ Success rate: {(df_analysis['error'].isna()).sum()}/{len(df_analysis)} ({100*(df_analysis['error'].isna()).sum()/len(df_analysis):.1f}%)\")\n",
    "\n",
    "# Clean up ArcTester\n",
    "ArcTester.cleanup_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Analyze differences between old and new methods\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out error cases for analysis\n",
    "df_valid = df_analysis[df_analysis['error'].isna()].copy()\n",
    "print(f\"Valid results: {len(df_valid)}/{len(df_analysis)} programs\")\n",
    "\n",
    "# Create comparison categories\n",
    "df_valid['old_any_transductive'] = df_valid['old_train_transductive'] | df_valid['old_test_transductive'] \n",
    "df_valid['agreement'] = df_valid['old_any_transductive'] == df_valid['new_transductive']\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRANSDUCTION DETECTION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOLD METHOD (Hardcoded Detection):\")\n",
    "print(f\"  - Train transductive: {df_valid['old_train_transductive'].sum():,} ({100*df_valid['old_train_transductive'].mean():.1f}%)\")\n",
    "print(f\"  - Test transductive: {df_valid['old_test_transductive'].sum():,} ({100*df_valid['old_test_transductive'].mean():.1f}%)\")\n",
    "print(f\"  - Any transductive: {df_valid['old_any_transductive'].sum():,} ({100*df_valid['old_any_transductive'].mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nNEW METHOD (Augmentation Invariance):\")\n",
    "print(f\"  - Transductive: {df_valid['new_transductive'].sum():,} ({100*df_valid['new_transductive'].mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nAGREEMENT:\")\n",
    "agreement_rate = df_valid['agreement'].mean()\n",
    "print(f\"  - Methods agree: {df_valid['agreement'].sum():,}/{len(df_valid)} ({100*agreement_rate:.1f}%)\")\n",
    "\n",
    "# Disagreement analysis\n",
    "disagreement = df_valid[~df_valid['agreement']].copy()\n",
    "print(f\"  - Methods disagree: {len(disagreement):,} ({100*(1-agreement_rate):.1f}%)\")\n",
    "\n",
    "if len(disagreement) > 0:\n",
    "    old_yes_new_no = disagreement[disagreement['old_any_transductive'] & ~disagreement['new_transductive']]\n",
    "    old_no_new_yes = disagreement[~disagreement['old_any_transductive'] & disagreement['new_transductive']]\n",
    "    \n",
    "    print(f\"    â€¢ Old=Transductive, New=Non-transductive: {len(old_yes_new_no):,}\")\n",
    "    print(f\"    â€¢ Old=Non-transductive, New=Transductive: {len(old_no_new_yes):,}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Overall comparison\n",
    "agreement_counts = df_valid['agreement'].value_counts()\n",
    "axes[0,0].pie(agreement_counts.values, labels=['Disagree', 'Agree'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0,0].set_title('Method Agreement Rate')\n",
    "\n",
    "# 2. Transduction rates by method\n",
    "methods = ['Old (Any)', 'New (Augmentation)']\n",
    "rates = [df_valid['old_any_transductive'].mean(), df_valid['new_transductive'].mean()]\n",
    "axes[0,1].bar(methods, rates, color=['skyblue', 'lightcoral'])\n",
    "axes[0,1].set_ylabel('Transduction Rate')\n",
    "axes[0,1].set_title('Transduction Detection Rates')\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "# 3. Disagreement breakdown\n",
    "if len(disagreement) > 0:\n",
    "    disagreement_types = ['Old=T, New=NT', 'Old=NT, New=T']\n",
    "    disagreement_counts = [len(old_yes_new_no), len(old_no_new_yes)]\n",
    "    axes[1,0].bar(disagreement_types, disagreement_counts, color=['orange', 'purple'])\n",
    "    axes[1,0].set_ylabel('Number of Programs')\n",
    "    axes[1,0].set_title('Types of Disagreement')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Task distribution\n",
    "task_counts = df_valid['task_id'].value_counts().head(20)\n",
    "axes[1,1].bar(range(len(task_counts)), task_counts.values)\n",
    "axes[1,1].set_xlabel('Top 20 Tasks (by program count)')\n",
    "axes[1,1].set_ylabel('Number of Programs')\n",
    "axes[1,1].set_title('Programs per Task Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Analysis visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ffe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Examine specific disagreement cases\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED DISAGREEMENT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(disagreement) > 0:\n",
    "    print(f\"\\nAnalyzing {len(disagreement)} disagreement cases...\")\n",
    "    \n",
    "    # Case 1: Old method says transductive, new method says non-transductive\n",
    "    if len(old_yes_new_no) > 0:\n",
    "        print(f\"\\nğŸ” CASE 1: Old=Transductive, New=Non-transductive ({len(old_yes_new_no)} cases)\")\n",
    "        print(\"These programs have hardcoded values but show augmentation invariance\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Show a few examples\n",
    "        for i, (idx, row) in enumerate(old_yes_new_no.head(3).iterrows()):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Task ID: {row['task_id']}\")\n",
    "            print(f\"Old train reason: {row['old_train_reason']}\")\n",
    "            print(f\"Old test reason: {row['old_test_reason']}\")\n",
    "            print(f\"New reason: {row['new_reason']}\")\n",
    "            print(f\"Code (first 200 chars):\")\n",
    "            print(row['code'][:200] + \"...\" if len(row['code']) > 200 else row['code'])\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    # Case 2: Old method says non-transductive, new method says transductive  \n",
    "    if len(old_no_new_yes) > 0:\n",
    "        print(f\"\\nğŸ” CASE 2: Old=Non-transductive, New=Transductive ({len(old_no_new_yes)} cases)\")\n",
    "        print(\"These programs don't have obvious hardcoded values but fail augmentation invariance\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Show a few examples\n",
    "        for i, (idx, row) in enumerate(old_no_new_yes.head(3).iterrows()):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Task ID: {row['task_id']}\")\n",
    "            print(f\"Old train reason: {row['old_train_reason']}\")\n",
    "            print(f\"Old test reason: {row['old_test_reason']}\")\n",
    "            print(f\"New reason: {row['new_reason']}\")\n",
    "            print(f\"Code (first 200 chars):\")\n",
    "            print(row['code'][:200] + \"...\" if len(row['code']) > 200 else row['code'])\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "# Save results for further analysis\n",
    "output_file = \"/tmp/transduction_analysis_results.parquet\"\n",
    "df_analysis.to_parquet(output_file, index=False)\n",
    "print(f\"\\nâœ“ Results saved to: {output_file}\")\n",
    "\n",
    "print(f\"\\nâœ… ANALYSIS COMPLETE\")\n",
    "print(f\"ğŸ“Š Analyzed {len(df_valid)} valid programs\")\n",
    "print(f\"ğŸ¤ Agreement rate: {100*agreement_rate:.1f}%\")\n",
    "print(f\"ğŸ“ Results saved for further investigation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
