{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c913ef65",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.037384,
     "end_time": "2025-09-14T19:17:30.040804",
     "exception": false,
     "start_time": "2025-09-14T19:17:30.003420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Local/RunPod model path:\n",
      "   Model: Trelis/Soar-qwen-14b-FP8-Dynamic\n",
      "ðŸ–¥ï¸ Runpod/local environment\n",
      "ðŸ§ª ENABLE_REFINEMENT ENABLED\n",
      "   â†’ Will run: Sampling â†’ Refinement\n",
      "   â†’ Sampling: 128 attempts, 128 workers, 18000s timeout\n",
      "   â†’ Refinement: 64 attempts, 128 workers, 18000s timeout\n",
      "Mode summary â†’ IS_KAGGLE=False | IS_RERUN=False | ENABLE_REFINEMENT=True |\n",
      "TEST_INFERENCE=True | SCORE=True | SUBMIT=true | MODEL=Trelis/Soar-qwen-14b-FP8-Dynamic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Single model for both initial inference and refinement\n",
    "MODEL_HF = \"Trelis/Soar-qwen-14b-FP8-Dynamic\"  # For local/RunPod\n",
    "MODEL_KAGGLE = \"arc-1-fake-ttt-blended-c802-dataset\"  # Kaggle dataset name\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Sampling attempts (initial inference)\n",
    "SAMPLING_ATTEMPTS = 128     # Number of attempts for sampling phase\n",
    "# Refinement attempts (second inference)\n",
    "REFINEMENT_ATTEMPTS = 64    # Number of attempts for refinement phase\n",
    "\n",
    "# Global workers setting\n",
    "MAX_WORKERS = 64            # Number of workers for all inference phases\n",
    "\n",
    "# ============================================================================\n",
    "# Other Configuration\n",
    "# ============================================================================\n",
    "\n",
    "DATASET = \"arc-prize-2025\"\n",
    "\n",
    "# ---- Config flags (single source of truth) ----\n",
    "START_SERVER = True\n",
    "TEST_INFERENCE = True\n",
    "SCORE = True                   # default; overridden below, depending on flags\n",
    "\n",
    "# Refinement mode?\n",
    "ENABLE_REFINEMENT=True\n",
    "\n",
    "# Env-backed flags\n",
    "IS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "IS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# String env flag for external tools\n",
    "os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# ---- Paths ----\n",
    "if IS_KAGGLE:\n",
    "    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n",
    "    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n",
    "    SUBMIT_DIR      = Path(\"/kaggle/working\")\n",
    "    ARC_PROGRAMS_PARQUET = SUBMIT_DIR\n",
    "\n",
    "    print(\"ðŸ” Searching for models in Kaggle environment...\")\n",
    "\n",
    "    # Auto-find model path in Kaggle's dataset structure\n",
    "    model_dataset_path = ARC_DATA_ROOT / MODEL_KAGGLE\n",
    "    print(f\"   Looking for model dataset: {model_dataset_path}\")\n",
    "\n",
    "    if model_dataset_path.exists() and model_dataset_path.is_dir():\n",
    "        # Kaggle datasets have version folders, find the first subdirectory\n",
    "        subdirs = [d for d in model_dataset_path.iterdir() if d.is_dir()]\n",
    "        if subdirs:\n",
    "            MODEL_PATH = subdirs[0]  # Use the first (usually only) version folder\n",
    "            print(f\"   âœ… Found model at: {MODEL_PATH}\")\n",
    "            # List what's inside to confirm it's right\n",
    "            model_contents = list(MODEL_PATH.iterdir())[:5]  # Show first 5 items\n",
    "            print(f\"      Contents: {[f.name for f in model_contents]}\")\n",
    "        else:\n",
    "            # Fallback if no subdirectory found\n",
    "            MODEL_PATH = model_dataset_path\n",
    "            print(f\"   âš ï¸ No version folder found for model, using: {MODEL_PATH}\")\n",
    "    else:\n",
    "        MODEL_PATH = model_dataset_path\n",
    "        print(f\"   âŒ Model dataset not found at: {MODEL_PATH}\")\n",
    "        print(f\"      Available datasets: {[d.name for d in ARC_DATA_ROOT.iterdir() if d.is_dir()][:10]}\")\n",
    "\n",
    "    print(f\"\\nðŸ“¦ Final model path:\")\n",
    "    print(f\"   Model: {MODEL_PATH}\")\n",
    "\n",
    "else:\n",
    "    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n",
    "    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n",
    "    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n",
    "    ARC_PROGRAMS_PARQUET = Path(\"/workspace/arc-agi-2025/llm_python/datasets/inference\")\n",
    "\n",
    "    # Use local/RunPod model path\n",
    "    MODEL_PATH = MODEL_HF\n",
    "\n",
    "    print(f\"ðŸ“¦ Local/RunPod model path:\")\n",
    "    print(f\"   Model: {MODEL_PATH}\")\n",
    "\n",
    "# Set up paths - parquet files are saved by task runner in different locations\n",
    "if IS_KAGGLE:\n",
    "    # On Kaggle, parquet files are saved directly in /kaggle/working by task runner\n",
    "    inference_dir = \"/kaggle/working\"\n",
    "else:\n",
    "    # On RunPod/local, parquet files are saved in llm_python/datasets/inference\n",
    "    inference_dir = \"llm_python/datasets/inference\"\n",
    "\n",
    "# Export envs for downstream processes\n",
    "os.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\n",
    "os.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\n",
    "os.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\n",
    "os.environ[\"ARC_PROGRAMS_PARQUET\"] = str(ARC_PROGRAMS_PARQUET)\n",
    "os.environ[\"MODEL_PATH\"] = str(MODEL_PATH)\n",
    "\n",
    "# Export config flags for subprocess use\n",
    "os.environ[\"IS_KAGGLE\"] = str(IS_KAGGLE).lower()\n",
    "os.environ[\"IS_RERUN\"] = str(IS_RERUN).lower()\n",
    "os.environ[\"DATASET\"] = DATASET\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure based on environment\n",
    "if IS_RERUN:\n",
    "    # Kaggle competition rerun - use full configuration\n",
    "    print(f\"ðŸ† Competition rerun detected\")\n",
    "    SCORE = False\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "    # Full timeouts for competition\n",
    "    SAMPLING_TIMEOUT = 5 * 3600    # 5 hours\n",
    "    REFINEMENT_TIMEOUT = 5 * 3600  # 5 hours\n",
    "elif not IS_KAGGLE:\n",
    "    # Runpod / local long run - use full configuration\n",
    "    print(f\"ðŸ–¥ï¸ Runpod/local environment\")\n",
    "    if os.getenv(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "        SCORE = True  # if we're generating a submission, do scoring\n",
    "    # Full timeouts for local runs\n",
    "    SAMPLING_TIMEOUT = 5 * 3600    # 5 hours\n",
    "    REFINEMENT_TIMEOUT = 5 * 3600  # 5 hours\n",
    "else:\n",
    "    # Kaggle dev/testing - use reduced attempts for faster testing\n",
    "    print(f\"ðŸ”§ Kaggle development environment - applying a shorter timeout\")\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "    # Short timeouts for dev testing\n",
    "    SAMPLING_TIMEOUT = 120      # 60 seconds\n",
    "    REFINEMENT_TIMEOUT = 120    # 60 seconds\n",
    "\n",
    "# ENABLE_REFINEMENT Mode configuration\n",
    "if ENABLE_REFINEMENT:\n",
    "    print(\"ðŸ§ª ENABLE_REFINEMENT ENABLED\")\n",
    "    print(\"   â†’ Will run: Sampling â†’ Refinement\")\n",
    "    print(f\"   â†’ Sampling: {SAMPLING_ATTEMPTS} attempts, {MAX_WORKERS} workers, {SAMPLING_TIMEOUT}s timeout\")\n",
    "    print(f\"   â†’ Refinement: {REFINEMENT_ATTEMPTS} attempts, {MAX_WORKERS} workers, {REFINEMENT_TIMEOUT}s timeout\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Standard mode (ENABLE_REFINEMENT disabled)\")\n",
    "    print(f\"   â†’ Will run: Sampling only ({SAMPLING_ATTEMPTS} attempts, {MAX_WORKERS} workers, {SAMPLING_TIMEOUT}s timeout)\")\n",
    "\n",
    "# Optional: quick summary (helps avoid accidental submits)\n",
    "print(\n",
    "    \"Mode summary â†’ \"\n",
    "    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | ENABLE_REFINEMENT={ENABLE_REFINEMENT} |\\n\"\n",
    "    f\"TEST_INFERENCE={TEST_INFERENCE} | SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | MODEL={MODEL_PATH}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b850b6de",
   "metadata": {
    "papermill": {
     "duration": 12.898677,
     "end_time": "2025-09-14T19:17:42.942988",
     "exception": false,
     "start_time": "2025-09-14T19:17:30.044311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA version (PyTorch): 12.8\n",
      "CUDA available: True\n",
      "NumPy version: 2.2.0\n",
      "GPU count: 2\n",
      "GPU name: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eadcf6c",
   "metadata": {
    "papermill": {
     "duration": 3.554569,
     "end_time": "2025-09-14T19:17:46.501106",
     "exception": false,
     "start_time": "2025-09-14T19:17:42.946537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang version: 0.5.1.post2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0916 09:56:21.688000 1010 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0916 09:56:21.688000 1010 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashInfer version: 0.2.14.post1\n"
     ]
    }
   ],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6fe90b",
   "metadata": {
    "papermill": {
     "duration": 0.723513,
     "end_time": "2025-09-14T19:17:47.228354",
     "exception": false,
     "start_time": "2025-09-14T19:17:46.504841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_KAGGLE:\n",
    "    import os, shutil, subprocess, stat\n",
    "    \n",
    "    # 1) Where to place binaries + cache\n",
    "    WRK_BIN = \"/kaggle/working/bin\"\n",
    "    TRITON_CACHE = \"/kaggle/working/.triton\"\n",
    "    os.makedirs(WRK_BIN, exist_ok=True)\n",
    "    os.makedirs(TRITON_CACHE, exist_ok=True)\n",
    "    \n",
    "    # 2) Preferred source for ptxas/cuobjdump/nvdisasm\n",
    "    SYSTEM_CUDA_BIN = \"/usr/local/cuda/bin\"\n",
    "    FALLBACK_VENDORED = \"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin\"  # if you have it\n",
    "    \n",
    "    def copy_tool(name: str):\n",
    "        for src_dir in (SYSTEM_CUDA_BIN, FALLBACK_VENDORED):\n",
    "            src = os.path.join(src_dir, name)\n",
    "            if os.path.exists(src):\n",
    "                dst = os.path.join(WRK_BIN, name)\n",
    "                shutil.copy2(src, dst)\n",
    "                # ensure executable bit\n",
    "                os.chmod(dst, os.stat(dst).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "                return dst\n",
    "        raise FileNotFoundError(f\"Could not find {name} in {SYSTEM_CUDA_BIN} or {FALLBACK_VENDORED}\")\n",
    "    \n",
    "    ptxas_path = copy_tool(\"ptxas\")\n",
    "    try:\n",
    "        cuobjdump_path = copy_tool(\"cuobjdump\")\n",
    "    except FileNotFoundError:\n",
    "        cuobjdump_path = None  # optional\n",
    "    try:\n",
    "        nvdisasm_path = copy_tool(\"nvdisasm\")\n",
    "    except FileNotFoundError:\n",
    "        nvdisasm_path = None  # optional\n",
    "    \n",
    "    # 3) Environment for Triton/JIT\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n",
    "    os.environ[\"PATH\"] = f\"{WRK_BIN}:{os.environ.get('PATH','')}\"\n",
    "    os.environ[\"TRITON_CACHE_DIR\"] = TRITON_CACHE\n",
    "    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "    os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda\"\n",
    "    \n",
    "    # Helpful fallbacks if you still hit capture issues:\n",
    "    # os.environ[\"SGLANG_DISABLE_CUDA_GRAPH\"] = \"1\"      # skip CUDA graphs (degrades perf but avoids capture)\n",
    "    # os.environ[\"TRITON_CODEGEN_FATBIN\"] = \"0\"          # can reduce Triton fatbin steps on some setups\n",
    "    \n",
    "    # 4) Smoke test: ensure ptxas runs from the new location\n",
    "    print(\"ptxas ->\", subprocess.check_output([ptxas_path, \"--version\"]).decode().strip())\n",
    "    \n",
    "    # Now it's safe to import heavy libs that trigger Triton\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53b2fff",
   "metadata": {
    "papermill": {
     "duration": 180.6259,
     "end_time": "2025-09-14T19:20:47.865965",
     "exception": false,
     "start_time": "2025-09-14T19:17:47.240065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cleared.\n",
      "ðŸ”§ Using model from Trelis/Soar-qwen-14b-FP8-Dynamic\n",
      "LOG file path: /workspace/arc-agi-2025/llm_python/submissions/sglang_server.log\n",
      "Started sglang server PID=1170 | logging to /workspace/arc-agi-2025/llm_python/submissions/sglang_server.log\n",
      "Command: /workspace/arc-agi-2025/.venv/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path Trelis/Soar-qwen-14b-FP8-Dynamic --dp 2 --kv-cache-dtype fp8_e4m3 --enable-metrics\n",
      "sglang is READY on port 8080.\n",
      "âœ… Model loaded: Trelis/Soar-qwen-14b-FP8-Dynamic\n",
      "Call stop_server() or full_cleanup() to shut it down gracefully.\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "  # Background server launcher for Kaggle with SGLang\n",
    "  import os, sys, time, subprocess, json, socket, requests\n",
    "\n",
    "  # ---------- 1) Check for existing server and cleanup ----------\n",
    "  PORT = 8080\n",
    "  HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "\n",
    "  # Check if server already running\n",
    "  try:\n",
    "      r = requests.get(HEALTH_URL, timeout=3)\n",
    "      if r.status_code == 200:\n",
    "          print(f\"Server already running on port {PORT}. Stopping it first...\")\n",
    "          # Kill existing sglang processes\n",
    "          subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "          time.sleep(3)  # Wait for cleanup\n",
    "  except:\n",
    "      pass  # No server running\n",
    "\n",
    "  # Clear CUDA memory before starting\n",
    "  try:\n",
    "      import torch\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.synchronize()\n",
    "          print(\"CUDA memory cleared.\")\n",
    "      num_gpus = torch.cuda.device_count()\n",
    "  except Exception:\n",
    "      num_gpus = 0\n",
    "      \n",
    "  model_path_to_use = str(MODEL_PATH)\n",
    "  print(f\"ðŸ”§ Using model from {model_path_to_use}\")\n",
    "\n",
    "  LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "  print(f\"LOG file path: {LOG}\")\n",
    "\n",
    "  SERVER_CMD = [\n",
    "      sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "      \"--host\", \"0.0.0.0\",\n",
    "      \"--port\", str(PORT),\n",
    "      \"--model-path\", model_path_to_use,\n",
    "      \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "      \"--kv-cache-dtype\", \"fp8_e4m3\",\n",
    "      \"--enable-metrics\",\n",
    "      \"--grammar-backend\", \"none\"\n",
    "  ]\n",
    "\n",
    "  # ---------- 2) Launch in background ----------\n",
    "  log_f = open(LOG, \"w\")\n",
    "  env = os.environ.copy()\n",
    "  proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=SUBMIT_DIR)\n",
    "  print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "  print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "\n",
    "  # ---------- 3) Wait for readiness ----------\n",
    "  def wait_ready(url, timeout_s=600):\n",
    "      t0 = time.time()\n",
    "      while time.time() - t0 < timeout_s:\n",
    "          try:\n",
    "              r = requests.get(url, timeout=3)\n",
    "              if r.status_code == 200:\n",
    "                  return True\n",
    "          except Exception:\n",
    "              pass\n",
    "          time.sleep(2)\n",
    "      return False\n",
    "\n",
    "  ready = wait_ready(HEALTH_URL)\n",
    "  log_f.flush()\n",
    "\n",
    "  if ready:\n",
    "      print(f\"sglang is READY on port {PORT}.\")\n",
    "      # Get the model name\n",
    "      try:\n",
    "          response = requests.get(HEALTH_URL)\n",
    "          if response.status_code == 200:\n",
    "              models = response.json()['data']\n",
    "              if models:\n",
    "                  model_name = models[0]['id']\n",
    "                  print(f\"âœ… Model loaded: {model_name}\")\n",
    "              else:\n",
    "                  print(\"âŒ No models found on server\")\n",
    "                  model_name = str(MODEL_PATH)\n",
    "          else:\n",
    "              print(f\"âŒ Server health check failed: {response.status_code}\")\n",
    "              model_name = str(MODEL_PATH)\n",
    "      except Exception as e:\n",
    "          print(f\"âš ï¸ Could not get model name: {e}\")\n",
    "          model_name = str(MODEL_PATH)\n",
    "  else:\n",
    "      print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "      log_f.close()\n",
    "      !tail -n 60 {LOG}\n",
    "      model_name = str(MODEL_PATH)\n",
    "\n",
    "  # ---------- 4) Cleanup functions ----------\n",
    "  def stop_server(p=proc):\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "      print(\"Server stopped.\")\n",
    "\n",
    "  def full_cleanup(p=proc):\n",
    "      # Stop server\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "\n",
    "      # Also kill any lingering sglang processes\n",
    "      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "      print(\"Server stopped and CUDA memory cleared.\")\n",
    "\n",
    "  print(\"Call stop_server() or full_cleanup() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f39f6dc",
   "metadata": {
    "papermill": {
     "duration": 10.111292,
     "end_time": "2025-09-14T19:26:58.025629",
     "exception": false,
     "start_time": "2025-09-14T19:26:47.914337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response received:\n",
      "```python\n",
      "def transform(grid):\n",
      "\n",
      "    def find\n",
      "\n",
      "â± Elapsed time: 35.30 seconds\n",
      "ðŸ”¢ Estimated tokens: 11.0\n",
      "âš¡ Output tokens/sec: 0.31\n"
     ]
    }
   ],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"âœ… Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\nâ± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"ðŸ”¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"âš¡ Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e45eb3",
   "metadata": {
    "papermill": {
     "duration": 525.478862,
     "end_time": "2025-09-14T19:35:43.513043",
     "exception": false,
     "start_time": "2025-09-14T19:26:58.034181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/arc-agi-2025\n",
      "Sampling Inference â†’ dev | attempts=128 | workers=128 | subset=evaluation | timeout=18000s\n",
      "Running command: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset evaluation --max_workers 128 --max_attempts 128 --model Trelis/Soar-qwen-14b-FP8-Dynamic --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think --parquet-output-dir /workspace/arc-agi-2025/llm_python/datasets/inference\n",
      "ðŸ“ Logging output to: /workspace/arc-agi-2025/llm_python/submissions/sampling.log\n",
      "â³ Running sampling phase (output being written to log file, 18000s timeout)...\n"
     ]
    }
   ],
   "source": [
    "if not IS_KAGGLE:\n",
    "    %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Use SAMPLING_ATTEMPTS and MAX_WORKERS for initial inference\n",
    "attempts = SAMPLING_ATTEMPTS\n",
    "workers = MAX_WORKERS\n",
    "\n",
    "# SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# can use this instead if testing evaluation during a pre-run\n",
    "SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"Sampling Inference â†’ {'competition' if IS_RERUN else 'dev'} | attempts={attempts} | workers={workers} | subset={SUBSET} | timeout={SAMPLING_TIMEOUT}s\")\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "    \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--max_workers\", str(workers),\n",
    "    \"--max_attempts\", str(attempts),\n",
    "    \"--model\", model_name,\n",
    "    \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "    \"--unsafe-executor\",\n",
    "    \"--max-tokens\", \"2000\",\n",
    "    \"--qwen-no-think\"\n",
    "]\n",
    "\n",
    "\n",
    "# Add parquet output directory if set\n",
    "if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "  cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "#   cmd_args.extend([\"--rex-stats\"])\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Handle output redirection properly\n",
    "if IS_RERUN or not IS_KAGGLE:\n",
    "    # For quiet mode, redirect to file using subprocess\n",
    "    import subprocess\n",
    "    log_file_path = f\"{SUBMIT_DIR}/sampling.log\"\n",
    "    print(f\"ðŸ“ Logging output to: {log_file_path}\")\n",
    "    \n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Wait for completion with timeout\n",
    "        print(f\"â³ Running sampling phase (output being written to log file, {SAMPLING_TIMEOUT}s timeout)...\")\n",
    "        try:\n",
    "            return_code = process.wait(timeout=SAMPLING_TIMEOUT)\n",
    "            if return_code == 0:\n",
    "                print(f\"âœ… Sampling phase completed successfully. Check {log_file_path} for details.\")\n",
    "            else:\n",
    "                print(f\"âŒ Sampling phase failed with return code {return_code}\")\n",
    "                print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "                # Show last few lines of log\n",
    "                !tail -n 20 {log_file_path}\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"â° Sampling phase timeout reached ({SAMPLING_TIMEOUT}s) - terminating process\")\n",
    "            process.terminate()\n",
    "            try:\n",
    "                # Give it 30 seconds to cleanup gracefully\n",
    "                process.wait(timeout=30)\n",
    "                print(\"âœ… Process terminated gracefully - parquet files should be saved\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"âš ï¸ Process didn't terminate gracefully, forcing kill\")\n",
    "                process.kill()\n",
    "                process.wait()\n",
    "else:\n",
    "    # For interactive mode, show output directly\n",
    "    cmd = \" \".join(cmd_args)\n",
    "    print(f\"Running: {cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02e48e",
   "metadata": {
    "papermill": {
     "duration": 190.200335,
     "end_time": "2025-09-14T19:38:53.770008",
     "exception": false,
     "start_time": "2025-09-14T19:35:43.569673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENABLE_REFINEMENT:\n",
    "    print(\"ðŸ”„ Checking if server restart is needed...\")\n",
    "    \n",
    "    # Since we're using the same model for both initial and refinement inference,\n",
    "    # we don't need to restart the server\n",
    "    print(\"âœ… Using same model for refinement - no server restart needed\")\n",
    "    print(f\"ðŸŽ¯ Continuing with existing model: {MODEL_PATH}\")\n",
    "    \n",
    "    # Just verify the server is still running and get the model name\n",
    "    if START_SERVER:\n",
    "        try:\n",
    "            HEALTH_URL = \"http://127.0.0.1:8080/v1/models\"\n",
    "            response = requests.get(HEALTH_URL, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json()['data']\n",
    "                if models:\n",
    "                    model_name = models[0]['id']\n",
    "                    print(f\"âœ… Server is running with model: {model_name}\")\n",
    "                else:\n",
    "                    print(\"âŒ No models found on server\")\n",
    "            else:\n",
    "                print(f\"âŒ Server health check failed: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not verify server status: {e}\")\n",
    "            print(\"âŒ Server may not be running properly\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ START_SERVER is False - assuming server is managed externally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0e37a",
   "metadata": {
    "papermill": {
     "duration": 556.082824,
     "end_time": "2025-09-14T19:48:09.870010",
     "exception": false,
     "start_time": "2025-09-14T19:38:53.787186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Second Inference Run - ENABLE_REFINEMENT\n",
    "# Only runs when ENABLE_REFINEMENT=true\n",
    "\n",
    "import glob\n",
    "\n",
    "if ENABLE_REFINEMENT:\n",
    "    print(\"ðŸ”„ Running refinement inference (ENABLE_REFINEMENT mode)\")\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        %cd /workspace/arc-agi-2025\n",
    "\n",
    "    # Use REFINEMENT_ATTEMPTS and MAX_WORKERS for second inference\n",
    "    attempts = REFINEMENT_ATTEMPTS\n",
    "    workers = MAX_WORKERS\n",
    "\n",
    "    SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "    print(f\"Refinement Phase â†’ {'competition' if IS_RERUN else 'dev'} | attempts={attempts} | workers={workers} | subset={SUBSET} | timeout={REFINEMENT_TIMEOUT}s\")\n",
    "\n",
    "    # ðŸ”‘ Find the latest parquet file in inference_dir\n",
    "    parquet_files = glob.glob(os.path.join(inference_dir, \"*.parquet\"))\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {inference_dir}\")\n",
    "    latest_parquet = max(parquet_files, key=os.path.getctime)\n",
    "    print(f\"ðŸ“‚ Using latest parquet file for refinement: {latest_parquet}\")\n",
    "\n",
    "    # Build the command\n",
    "    cmd_args = [\n",
    "        \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--max_workers\", str(workers),\n",
    "        \"--max_attempts\", str(attempts),\n",
    "        \"--model\", model_name,\n",
    "        \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "        \"--unsafe-executor\",\n",
    "        \"--max-tokens\", \"2000\",\n",
    "        \"--qwen-no-think\",\n",
    "        \"--refinement-ds\", latest_parquet,   # ðŸ‘ˆ add parquet path here\n",
    "        \"--include-outputs\"\n",
    "    ]\n",
    "\n",
    "    # Add parquet output directory if set\n",
    "    if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "      cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "\n",
    "    print(f\"Running refinement inference: {' '.join(cmd_args)}\")\n",
    "\n",
    "    # Handle output redirection properly\n",
    "    if IS_RERUN or not IS_KAGGLE:\n",
    "        # For quiet mode, redirect to file using subprocess\n",
    "        import subprocess\n",
    "        log_file_path = f\"{SUBMIT_DIR}/refinement.log\"\n",
    "        print(f\"ðŸ“ Logging refinement phase output to: {log_file_path}\")\n",
    "        \n",
    "        with open(log_file_path, \"w\") as log_file:\n",
    "            process = subprocess.Popen(\n",
    "                cmd_args,\n",
    "                stdout=log_file,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "                cwd=os.getcwd()\n",
    "            )\n",
    "            \n",
    "            # Wait for completion with timeout\n",
    "            print(f\"â³ Running refinement phase (output being written to log file, {REFINEMENT_TIMEOUT}s timeout)...\")\n",
    "            try:\n",
    "                return_code = process.wait(timeout=REFINEMENT_TIMEOUT)\n",
    "                if return_code == 0:\n",
    "                    print(f\"âœ… Refinement phase completed successfully. Check {log_file_path} for details.\")\n",
    "                else:\n",
    "                    print(f\"âŒ Refinement phase failed with return code {return_code}\")\n",
    "                    print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "                    # Show last few lines of log\n",
    "                    !tail -n 20 {log_file_path}\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(f\"â° Refinement phase timeout reached ({REFINEMENT_TIMEOUT}s) - terminating process\")\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    # Give it 30 seconds to cleanup gracefully\n",
    "                    process.wait(timeout=30)\n",
    "                    print(\"âœ… Process terminated gracefully - parquet files should be saved\")\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    print(\"âš ï¸ Process didn't terminate gracefully, forcing kill\")\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "    else:\n",
    "        # For interactive mode, show output directly\n",
    "        cmd = \" \".join(cmd_args)\n",
    "        print(f\"Running refinement inference: {cmd}\\n\")\n",
    "        !{cmd}\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ”„ Skipping refinement phase (ENABLE_REFINEMENT=false)\")\n",
    "    print(\"   â†’ Standard mode runs sampling phase only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971bad5",
   "metadata": {
    "papermill": {
     "duration": 10.205901,
     "end_time": "2025-09-14T19:48:20.105564",
     "exception": false,
     "start_time": "2025-09-14T19:48:09.899663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate submission using the two most recent parquet files\n",
    "if os.environ.get(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "    print(\"ðŸŽ¯ Generating submission from the two most recent parquet files...\")\n",
    "    \n",
    "    import subprocess\n",
    "    \n",
    "    output_dir = str(SUBMIT_DIR)\n",
    "    \n",
    "    # Command to generate submission using the two most recent parquet files\n",
    "    submission_cmd = [\n",
    "        \"uv\", \"run\", \"python\", \"-m\", \"llm_python.generate_submission\",\n",
    "        \"--parquet-path\", inference_dir,\n",
    "        \"--n-files\", \"2\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--output-dir\", output_dir,\n",
    "        \"--debug\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running submission generation: {' '.join(submission_cmd)}\")\n",
    "    print(f\"ðŸ“‚ Looking for parquet files in: {inference_dir}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            submission_cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,  # 5 minute timeout\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Submission generation completed successfully!\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Update submit_dir to point to the generated file\n",
    "            submit_dir = f\"{output_dir}/submission.json\"\n",
    "            print(f\"ðŸ“ Submission file: {submit_dir}\")\n",
    "        else:\n",
    "            print(f\"âŒ Submission generation failed with return code {result.returncode}\")\n",
    "            print(f\"STDOUT: {result.stdout}\")\n",
    "            print(f\"STDERR: {result.stderr}\")\n",
    "            # Fallback to default submission path\n",
    "            submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Submission generation timed out\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Submission generation error: {e}\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "else:\n",
    "    print(\"ðŸ“ Skipping submission generation (SUBMIT=false)\")\n",
    "    submit_dir = f\"{SUBMIT_DIR}/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7a2ee",
   "metadata": {
    "papermill": {
     "duration": 1.295028,
     "end_time": "2025-09-14T19:48:21.430229",
     "exception": false,
     "start_time": "2025-09-14T19:48:20.135201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission {submit_dir} --dataset {DATASET} --subset {SUBSET}\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5561b",
   "metadata": {
    "papermill": {
     "duration": 0.089127,
     "end_time": "2025-09-14T19:48:21.549571",
     "exception": false,
     "start_time": "2025-09-14T19:48:21.460444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final cleanup - stop server and free resources\n",
    "if START_SERVER and 'full_cleanup' in globals():\n",
    "    print(\"ðŸ§¹ Cleaning up server and resources...\")\n",
    "    full_cleanup()\n",
    "else:\n",
    "    print(\"ðŸ” No server cleanup needed (START_SERVER=False or cleanup function not available)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 13706937,
     "datasetId": 8063856,
     "isSourceIdPinned": true,
     "sourceId": 13032060,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 262135446,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "arc-agi-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1863.748367,
   "end_time": "2025-09-14T19:48:24.194420",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T19:17:20.446053",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
