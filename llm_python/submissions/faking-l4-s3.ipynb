{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-18T20:59:23.668560Z",
     "iopub.status.busy": "2025-08-18T20:59:23.668190Z",
     "iopub.status.idle": "2025-08-18T20:59:23.676855Z",
     "shell.execute_reply": "2025-08-18T20:59:23.676297Z",
     "shell.execute_reply.started": "2025-08-18T20:59:23.668542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "IS_RERUN = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# Set global timeout based on whether this is a competition rerun\n",
    "if IS_RERUN:\n",
    "  # Competition rerun - FULL timeout for actual scoring\n",
    "  timeout_seconds = 39600  # 11 hours\n",
    "  print(\"üèÜ Competition rerun detected - setting FULL timeout for scoring\")\n",
    "else:\n",
    "  # Development/testing - short timeout\n",
    "  timeout_seconds = 60  # 1 minute\n",
    "  print(\"üîß Development run - setting short timeout for testing\")\n",
    "\n",
    "os.environ['GLOBAL_TIMEOUT'] = str(timeout_seconds)\n",
    "print(f\"‚è∞ Global timeout set to {timeout_seconds}s ({timeout_seconds/3600:.1f} hours)\")\n",
    "\n",
    "START_SERVER = True\n",
    "TEST_INFERENCE = False #set false unless you want to see inference hitting the endpoint, before the task runner\n",
    "SUBMIT = True #to run the task runner\n",
    "SCORE = False # score if not a competition rerun OR if running the test set.\n",
    "\n",
    "os.environ[\"ARC_DATA_ROOT\"]  = \"/kaggle/input\"\n",
    "\n",
    "# to have the task runner generate a submission file\n",
    "os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# the directory for where the submission.json file will go\n",
    "os.environ[\"SUBMIT_DIR\"] = \"/kaggle/working\"\n",
    "\n",
    "# location of the db (current just saving here, not reading from it yet)\n",
    "os.environ[\"ARC_PROGRAMS_DB\"]=\"/kaggle/working/local.db\"\n",
    "\n",
    "# COMPUTE WEIGHTING PARAMS\n",
    "os.environ[\"STOP_AT_ALL_TRAIN_CORRECT\"]=\"7\"\n",
    "os.environ[\"STOP_IF_NO_TRAIN_CORRECT_AFTER\"]=\"50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:59:23.677512Z",
     "iopub.status.busy": "2025-08-18T20:59:23.677357Z",
     "iopub.status.idle": "2025-08-18T20:59:36.207082Z",
     "shell.execute_reply": "2025-08-18T20:59:36.206468Z",
     "shell.execute_reply.started": "2025-08-18T20:59:23.677498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:59:36.208526Z",
     "iopub.status.busy": "2025-08-18T20:59:36.208243Z",
     "iopub.status.idle": "2025-08-18T20:59:36.211251Z",
     "shell.execute_reply": "2025-08-18T20:59:36.210718Z",
     "shell.execute_reply.started": "2025-08-18T20:59:36.208510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls ../input/arc-1-fake-ttt-blended-c802-dataset/arc-1-fake-ttt-blended-c802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:59:36.212034Z",
     "iopub.status.busy": "2025-08-18T20:59:36.211803Z",
     "iopub.status.idle": "2025-08-18T20:59:40.039183Z",
     "shell.execute_reply": "2025-08-18T20:59:40.038600Z",
     "shell.execute_reply.started": "2025-08-18T20:59:36.212018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:59:40.040057Z",
     "iopub.status.busy": "2025-08-18T20:59:40.039744Z",
     "iopub.status.idle": "2025-08-18T20:59:40.458587Z",
     "shell.execute_reply": "2025-08-18T20:59:40.458045Z",
     "shell.execute_reply.started": "2025-08-18T20:59:40.040039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ensure that ptxas can access writable directories\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Copy PTXAS and other binaries\n",
    "os.makedirs(\"/kaggle/working/bin\", exist_ok=True)\n",
    "for binary in [\"ptxas\", \"cuobjdump\", \"nvdisasm\"]:\n",
    "    src = f\"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin/{binary}\"  # Fixed path\n",
    "    dst = f\"/kaggle/working/bin/{binary}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        os.chmod(dst, 0o755)\n",
    "\n",
    "# Set environment variables\n",
    "env = os.environ.copy()\n",
    "env[\"TRITON_PTXAS_PATH\"] = \"/kaggle/working/bin/ptxas\"\n",
    "env[\"PATH\"] = f\"/kaggle/working/bin:{env.get('PATH', '')}\"\n",
    "env[\"TRITON_CACHE_DIR\"] = \"/kaggle/working/.triton\"\n",
    "env[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.makedirs(\"/kaggle/working/.triton\", exist_ok=True)\n",
    "\n",
    "# Apply the environment variables to the current process\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:59:40.459425Z",
     "iopub.status.busy": "2025-08-18T20:59:40.459228Z",
     "iopub.status.idle": "2025-08-18T21:02:40.749112Z",
     "shell.execute_reply": "2025-08-18T21:02:40.748342Z",
     "shell.execute_reply.started": "2025-08-18T20:59:40.459408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if START_SERVER:\n",
    "    # Background server launcher for Kaggle with SGLang\n",
    "    import os, sys, time, subprocess, json, socket, requests\n",
    "    \n",
    "    BASE_PATH = \"/kaggle/input/arc-1-fake-ttt-blended-c802-dataset\"\n",
    "    # Find the first directory inside BASE_PATH\n",
    "    subdirs = [os.path.join(BASE_PATH, d) for d in os.listdir(BASE_PATH) if os.path.isdir(os.path.join(BASE_PATH, d))]\n",
    "    if not subdirs:\n",
    "        raise RuntimeError(f\"No model directory found in {BASE_PATH}\")\n",
    "    MODEL_PATH = subdirs[0]   # or pick max(subdirs) if multiple exist\n",
    "    PORT = 8080\n",
    "    LOG = f\"/kaggle/working/sglang_server.log\"\n",
    "    \n",
    "    # Auto-detect GPUs for sensible parallelism\n",
    "    try:\n",
    "        import torch\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "    except Exception:\n",
    "        num_gpus = 0\n",
    "    \n",
    "    SERVER_CMD = [\n",
    "        sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", str(PORT),\n",
    "        \"--model-path\", MODEL_PATH,\n",
    "        \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "        \"--kv-cache-dtype\", \"fp8_e4m3\"\n",
    "    ]\n",
    "    HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"  # sglang doesn't always expose /health\n",
    "    \n",
    "    # ---------- 2) Launch in background ----------\n",
    "    log_f = open(LOG, \"w\")\n",
    "    env = os.environ.copy()\n",
    "    proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=\"/kaggle/working\")\n",
    "    print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "    print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "    \n",
    "    # ---------- 3) Wait for readiness ----------\n",
    "    def wait_ready(url, timeout_s=180):\n",
    "        t0 = time.time()\n",
    "        while time.time() - t0 < timeout_s:\n",
    "            try:\n",
    "                r = requests.get(url, timeout=3)\n",
    "                if r.status_code == 200:\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(2)\n",
    "        return False\n",
    "    \n",
    "    ready = wait_ready(HEALTH_URL)\n",
    "    log_f.flush()\n",
    "    \n",
    "    if ready:\n",
    "        print(f\"sglang is READY on port {PORT}.\")\n",
    "        print(f\"- Tail logs: !tail -n 50 {LOG}\")\n",
    "        print(f\"- List models: !curl -s http://127.0.0.1:{PORT}/v1/models | jq .\")\n",
    "    else:\n",
    "        print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "        log_f.close()\n",
    "        !tail -n 60 {LOG}\n",
    "    \n",
    "    # Provide a tiny helper to stop it later\n",
    "    def stop_server(p=proc):\n",
    "        try:\n",
    "            p.terminate()\n",
    "            p.wait(timeout=10)\n",
    "        except Exception:\n",
    "            p.kill()\n",
    "        print(\"Server stopped.\")\n",
    "    \n",
    "    print(\"Call stop_server() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:02:40.750429Z",
     "iopub.status.busy": "2025-08-18T21:02:40.749986Z",
     "iopub.status.idle": "2025-08-18T21:04:10.766308Z",
     "shell.execute_reply": "2025-08-18T21:04:10.765726Z",
     "shell.execute_reply.started": "2025-08-18T21:02:40.750403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if START_SERVER:\n",
    "    import requests\n",
    "    import time\n",
    "    \n",
    "    def check_models():\n",
    "        url = \"http://127.0.0.1:8080/v1/models\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "    \n",
    "            print(\"‚úÖ Server is responding!\")\n",
    "            print(\"Available models:\")\n",
    "            for model in result['data']:\n",
    "                print(f\"  - {model['id']}\")\n",
    "    \n",
    "            return result['data'][0]['id'] if result['data'] else None\n",
    "    \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"‚ùå Connection failed - server may not be ready yet\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Poll every 30 seconds until we get a model\n",
    "    model_name = None\n",
    "    while not model_name:\n",
    "        model_name = check_models()\n",
    "        if not model_name:\n",
    "            print(\"‚è≥ Waiting 30 seconds before retrying...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Found model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:04:10.767330Z",
     "iopub.status.busy": "2025-08-18T21:04:10.767068Z",
     "iopub.status.idle": "2025-08-18T21:04:10.774115Z",
     "shell.execute_reply": "2025-08-18T21:04:10.773634Z",
     "shell.execute_reply.started": "2025-08-18T21:04:10.767312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"‚úÖ Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\n‚è± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"üî¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"‚ö° Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:04:10.776077Z",
     "iopub.status.busy": "2025-08-18T21:04:10.775842Z",
     "iopub.status.idle": "2025-08-18T21:04:10.790464Z",
     "shell.execute_reply": "2025-08-18T21:04:10.789964Z",
     "shell.execute_reply.started": "2025-08-18T21:04:10.776061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Your messages from before\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\": \"user\", \"content\": \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\npython\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n\\n\"}\n",
    "    ]\n",
    "    \n",
    "    # Number of identical requests to send\n",
    "    N = 32\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # define this before runninga\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 24000\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    responses = []\n",
    "    for _ in range(N):\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=1200)\n",
    "        r.raise_for_status()\n",
    "        responses.append(r.json())\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_elapsed = end_time - start_time\n",
    "    \n",
    "    # Token counting (rough estimate: 4 chars/token)\n",
    "    total_tokens = 0\n",
    "    for resp in responses:\n",
    "        output_text = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "        total_tokens += len(output_text) / 4\n",
    "    \n",
    "    tokens_per_sec = total_tokens / total_elapsed\n",
    "    avg_time_per_request = total_elapsed / N\n",
    "    \n",
    "    print(f\"‚úÖ Completed {N} requests\")\n",
    "    print(f\"‚è± Total elapsed: {total_elapsed:.2f} sec\")\n",
    "    print(f\"‚è± Avg per request: {avg_time_per_request:.2f} sec\")\n",
    "    print(f\"üî¢ Estimated total output tokens: {total_tokens:.1f}\")\n",
    "    print(f\"‚ö° Output tokens/sec: {tokens_per_sec:.2f}\")\n",
    "    \n",
    "    # Optional: print first response\n",
    "    print(\"\\nExample output:\")\n",
    "    print(responses[0][\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:04:10.791342Z",
     "iopub.status.busy": "2025-08-18T21:04:10.791165Z",
     "iopub.status.idle": "2025-08-18T21:05:33.104898Z",
     "shell.execute_reply": "2025-08-18T21:05:33.103998Z",
     "shell.execute_reply.started": "2025-08-18T21:04:10.791328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Derive attempts/workers for the two modes\n",
    "MAX_ATTEMPTS = 512 if (IS_RERUN and SUBMIT) else 8\n",
    "MAX_WORKERS  = 16\n",
    "\n",
    "SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# # can use this instead if testing evaluation during a pre-run\n",
    "# SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"Mode: {'competition' if IS_RERUN else 'dev'} | SUBMIT={SUBMIT} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "# Build the command\n",
    "cmd = (\n",
    "  \"uv run python -m llm_python.run_arc_tasks_soar \"\n",
    "  \"--dataset arc-prize-2025 \"\n",
    "  f\"--subset {SUBSET} \"\n",
    "  f\"--max_workers {MAX_WORKERS} \"\n",
    "  f\"--max_attempts {MAX_ATTEMPTS} \"\n",
    "  f\"--model \\\"{model_name}\\\" \"\n",
    "  \"--base-url http://127.0.0.1:8080/v1 \"\n",
    "  \"--unsafe-executor \"\n",
    "  \"--max-tokens 2000 \"\n",
    "  \"--qwen-no-think\"\n",
    ")\n",
    "\n",
    "# Optionally quiet the private rerun by redirecting logs to a file\n",
    "if IS_RERUN:\n",
    "    cmd += \" >> /kaggle/working/run.log 2>&1\"\n",
    "\n",
    "print(f\"Running {cmd}\\n\\n\")\n",
    "\n",
    "# Run\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:05:33.106050Z",
     "iopub.status.busy": "2025-08-18T21:05:33.105816Z",
     "iopub.status.idle": "2025-08-18T21:05:33.110838Z",
     "shell.execute_reply": "2025-08-18T21:05:33.110088Z",
     "shell.execute_reply.started": "2025-08-18T21:05:33.106027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission \"/kaggle/working/submission.json\"\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8063856,
     "isSourceIdPinned": true,
     "sourceId": 12801036,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 256290376,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 256685724,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
