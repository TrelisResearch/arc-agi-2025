{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7971a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-07T15:06:56.284092Z",
     "iopub.status.busy": "2025-09-07T15:06:56.283834Z",
     "iopub.status.idle": "2025-09-07T15:06:56.344464Z",
     "shell.execute_reply": "2025-09-07T15:06:56.343925Z"
    },
    "papermill": {
     "duration": 0.066889,
     "end_time": "2025-09-07T15:06:56.345480",
     "exception": false,
     "start_time": "2025-09-07T15:06:56.278591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# Timeout Management\n",
    "# ============================================================================\n",
    "\n",
    "import signal, subprocess, time\n",
    "from typing import Sequence, Mapping, Optional\n",
    "\n",
    "class GlobalDeadline:\n",
    "    def __init__(self, total_seconds: float):\n",
    "        self.start = time.monotonic()\n",
    "        self.total = float(total_seconds)\n",
    "    def remaining(self) -> float:\n",
    "        return max(0.0, self.total - (time.monotonic() - self.start))\n",
    "    def assert_time_left(self, need: float = 0.0):\n",
    "        r = self.remaining()\n",
    "        if r <= need:\n",
    "            raise TimeoutError(f\"Global timeout exceeded (need {need:.1f}s, have {r:.1f}s).\")\n",
    "        return r\n",
    "\n",
    "def run_with_global_timeout(\n",
    "    cmd: Sequence[str],\n",
    "    deadline: GlobalDeadline,\n",
    "    name: str = \"process\",\n",
    "    env: Optional[Mapping[str, str]] = None,\n",
    "    cwd: Optional[str] = None,\n",
    "    stream_output: bool = True,   # set False if you want to capture\n",
    "    check: bool = True,\n",
    "):\n",
    "    deadline.assert_time_left(1.0)  # ensure we have *some* time left to start\n",
    "    # Start a new session â†’ dedicated process group for clean killing\n",
    "    popen_kwargs = dict(\n",
    "        env=env, cwd=cwd, start_new_session=True,\n",
    "        stdout=None if stream_output else subprocess.PIPE,\n",
    "        stderr=None if stream_output else subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "    print(f\"â–¶ï¸  {name}: starting: {' '.join(cmd)}  (time left ~{deadline.remaining():.0f}s)\")\n",
    "    proc = subprocess.Popen(cmd, **popen_kwargs)\n",
    "\n",
    "    try:\n",
    "        # Poll in short intervals so we can respect the *global* deadline\n",
    "        while True:\n",
    "            if proc.poll() is not None:\n",
    "                break\n",
    "            rem = deadline.remaining()\n",
    "            if rem <= 0:\n",
    "                raise TimeoutError(f\"Global timeout while running {name}\")\n",
    "            # Wait up to 5s (or remaining time if smaller)\n",
    "            try:\n",
    "                proc.wait(timeout=min(5.0, rem))\n",
    "            except subprocess.TimeoutExpired:\n",
    "                # loop again and re-check remaining\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        # Kill the whole process group on any timeout/error\n",
    "        try:\n",
    "            print(f\"â›” {name}: terminating process group (pid {proc.pid}) due to: {e}\")\n",
    "            os.killpg(proc.pid, signal.SIGKILL)\n",
    "        except ProcessLookupError:\n",
    "            pass\n",
    "        finally:\n",
    "            # Ensure it's fully reaped\n",
    "            try: proc.wait(timeout=2)\n",
    "            except Exception: pass\n",
    "        raise\n",
    "    finally:\n",
    "        # Optional: warn if you're getting close to the limit\n",
    "        rem = deadline.remaining()\n",
    "        if rem < 60:\n",
    "            print(f\"âš ï¸ {name}: only {rem:.1f}s remaining in global budget.\")\n",
    "\n",
    "    if check and proc.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(proc.returncode, cmd)\n",
    "    print(f\"âœ… {name}: finished with code {proc.returncode} (time left ~{deadline.remaining():.0f}s)\")\n",
    "    if not stream_output:\n",
    "        # If you set stream_output=False above, you can collect logs like this:\n",
    "        out, err = proc.communicate()\n",
    "        return proc.returncode, out, err\n",
    "    return proc.returncode\n",
    "\n",
    "# ============================================================================\n",
    "# Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# # Model for initial inference (path for Kaggle, slug for local/RunPod)\n",
    "INITIAL_MODEL_HF = \"Trelis/Qwen3-4B_ds-arc-agi-2-partial-20-c976\"  # For local/RunPod\n",
    "INITIAL_MODEL_KAGGLE = \"arc-1-fake-ttt-blended-c802-dataset\"  # Kaggle dataset name\n",
    "\n",
    "# # Model for Refinement\n",
    "REFINEMENT_MODEL_HF = \"Trelis/Qwen3-4B_ds-arc-agi-1-refinement-finetuning-partialplus-c552\"  # For local/RunPod\n",
    "REFINEMENT_MODEL_KAGGLE = \"refinement-model\"  # Kaggle dataset name\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Test Attempts (used for first and second/refinement)\n",
    "TEST_ATTEMPTS = 4\n",
    "KAGGLE_TIMEOUT = 3600*11.5 # Allow 11.5 hours for inference, keeping 30min buffer for Kaggle's 12h limit\n",
    "\n",
    "# First inference settings\n",
    "FIRST_ATTEMPTS = 128*2      # Number of attempts for first inference\n",
    "FIRST_WORKERS = 64       # Number of workers for first inference\n",
    "\n",
    "# Second inference settings\n",
    "SECOND_ATTEMPTS = 128     # Number of attempts for second inference\n",
    "SECOND_WORKERS = 64      # Number of workers for second inference\n",
    "\n",
    "# ============================================================================\n",
    "# Other Configuration\n",
    "# ============================================================================\n",
    "\n",
    "DATASET = \"arc-prize-2025\"\n",
    "\n",
    "# ---- Config flags (single source of truth) ----\n",
    "START_SERVER = True\n",
    "TEST_INFERENCE = False          # set False unless you want a quick endpoint smoke test\n",
    "SCORE = True                   # default; overridden below, depending on flags\n",
    "\n",
    "# Refinement mode?\n",
    "ENABLE_REFINEMENT=True\n",
    "\n",
    "# Env-backed flags\n",
    "IS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "IS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# String env flag for external tools\n",
    "os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# ---- Paths ----\n",
    "if IS_KAGGLE:\n",
    "    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n",
    "    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n",
    "    SUBMIT_DIR      = Path(\"/kaggle/working\")\n",
    "    ARC_PROGRAMS_PARQUET = SUBMIT_DIR\n",
    "\n",
    "    print(\"ðŸ” Searching for models in Kaggle environment...\")\n",
    "\n",
    "    # Auto-find initial model path in Kaggle's dataset structure\n",
    "    model_dataset_path = ARC_DATA_ROOT / INITIAL_MODEL_KAGGLE\n",
    "    print(f\"   Looking for initial model dataset: {model_dataset_path}\")\n",
    "\n",
    "    if model_dataset_path.exists() and model_dataset_path.is_dir():\n",
    "        # Kaggle datasets have version folders, find the first subdirectory\n",
    "        subdirs = [d for d in model_dataset_path.iterdir() if d.is_dir()]\n",
    "        if subdirs:\n",
    "            MODEL_PATH = subdirs[0]  # Use the first (usually only) version folder\n",
    "            print(f\"   âœ… Found initial model at: {MODEL_PATH}\")\n",
    "            # List what's inside to confirm it's right\n",
    "            model_contents = list(MODEL_PATH.iterdir())[:5]  # Show first 5 items\n",
    "            print(f\"      Contents: {[f.name for f in model_contents]}\")\n",
    "        else:\n",
    "            # Fallback if no subdirectory found\n",
    "            MODEL_PATH = model_dataset_path\n",
    "            print(f\"   âš ï¸ No version folder found for initial model, using: {MODEL_PATH}\")\n",
    "    else:\n",
    "        MODEL_PATH = model_dataset_path\n",
    "        print(f\"   âŒ Initial model dataset not found at: {MODEL_PATH}\")\n",
    "        print(f\"      Available datasets: {[d.name for d in ARC_DATA_ROOT.iterdir() if d.is_dir()][:10]}\")\n",
    "\n",
    "    # Auto-find refinement base model path\n",
    "    refinement_dataset_path = ARC_DATA_ROOT / REFINEMENT_MODEL_KAGGLE\n",
    "    print(f\"   Looking for refinement base dataset: {refinement_dataset_path}\")\n",
    "\n",
    "    if refinement_dataset_path.exists() and refinement_dataset_path.is_dir():\n",
    "        subdirs = [d for d in refinement_dataset_path.iterdir() if d.is_dir()]\n",
    "        if subdirs:\n",
    "            REFINEMENT_MODEL_PATH = subdirs[0]\n",
    "            print(f\"   âœ… Found refinement base at: {REFINEMENT_MODEL_PATH}\")\n",
    "            # List what's inside to confirm it's right\n",
    "            finetune_contents = list(REFINEMENT_MODEL_PATH.iterdir())[:5]  # Show first 5 items\n",
    "            print(f\"      Contents: {[f.name for f in finetune_contents]}\")\n",
    "        else:\n",
    "            REFINEMENT_MODEL_PATH = refinement_dataset_path\n",
    "            print(f\"   âš ï¸ No version folder found for refinement base, using: {REFINEMENT_MODEL_PATH}\")\n",
    "    else:\n",
    "        REFINEMENT_MODEL_PATH = refinement_dataset_path\n",
    "        print(f\"   âŒ Refinement base dataset not found at: {REFINEMENT_MODEL_PATH}\")\n",
    "\n",
    "    print(f\"\\nðŸ“¦ Final model paths:\")\n",
    "    print(f\"   Initial model: {MODEL_PATH}\")\n",
    "    print(f\"   Refinement base: {REFINEMENT_MODEL_PATH}\")\n",
    "\n",
    "else:\n",
    "    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n",
    "    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n",
    "    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n",
    "    ARC_PROGRAMS_PARQUET = Path(\"/workspace/arc-agi-2025/llm_python/datasets/inference\")\n",
    "\n",
    "    # Use local/RunPod model paths\n",
    "    MODEL_PATH = INITIAL_MODEL_HF\n",
    "    REFINEMENT_MODEL_PATH = REFINEMENT_MODEL_HF\n",
    "\n",
    "    print(f\"ðŸ“¦ Local/RunPod model paths:\")\n",
    "    print(f\"   Initial model: {MODEL_PATH}\")\n",
    "    print(f\"   Refinement base: {REFINEMENT_MODEL_PATH}\")\n",
    "\n",
    "# Set up paths - parquet files are saved by task runner in different locations\n",
    "if IS_KAGGLE:\n",
    "    # On Kaggle, parquet files are saved directly in /kaggle/working by task runner\n",
    "    inference_dir = \"/kaggle/working\"\n",
    "else:\n",
    "    # On RunPod/local, parquet files are saved in llm_python/datasets/inference\n",
    "    inference_dir = \"llm_python/datasets/inference\"\n",
    "\n",
    "# Export envs for downstream processes\n",
    "os.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\n",
    "os.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\n",
    "os.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\n",
    "os.environ[\"ARC_PROGRAMS_PARQUET\"] = str(ARC_PROGRAMS_PARQUET)\n",
    "os.environ[\"MODEL_PATH\"] = str(MODEL_PATH)\n",
    "\n",
    "# Export config flags for subprocess use\n",
    "os.environ[\"IS_KAGGLE\"] = str(IS_KAGGLE).lower()\n",
    "os.environ[\"IS_RERUN\"] = str(IS_RERUN).lower()\n",
    "os.environ[\"DATASET\"] = DATASET\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if IS_RERUN:\n",
    "    # Kaggle competition rerun\n",
    "    timeout_seconds = KAGGLE_TIMEOUT\n",
    "    print(f\"ðŸ† Competition rerun detected â€” global timeout: {timeout_seconds}s ({timeout_seconds/3600:.1f}h)\")\n",
    "    # Initialize global timeout deadline\n",
    "    deadline = GlobalDeadline(timeout_seconds)\n",
    "    TEST_INFERENCE = False\n",
    "    SCORE = False\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "elif not IS_KAGGLE:\n",
    "    # Runpod / local long run\n",
    "    timeout_seconds = None\n",
    "    print(f\"ðŸ–¥ï¸ Runpod/local long run â€” no timeout\")\n",
    "    deadline = GlobalDeadline(None)\n",
    "    if os.getenv(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "        SCORE = True  # if we're generating a submission, do scoring\n",
    "\n",
    "else:\n",
    "    # Kaggle dev/testing\n",
    "    timeout_seconds = int(KAGGLE_TIMEOUT * TEST_ATTEMPTS / (2 * (FIRST_ATTEMPTS + SECOND_ATTEMPTS)))\n",
    "    deadline = GlobalDeadline(timeout_seconds)\n",
    "    print(f\"ðŸ”§ Development run â€” setting short {timeout_seconds}s timeout for testing\")\n",
    "    # Safer default: don't auto-submit in dev\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "    FIRST_ATTEMPTS = TEST_ATTEMPTS\n",
    "    SECOND_ATTEMPTS = TEST_ATTEMPTS\n",
    "\n",
    "# ENABLE_REFINEMENT Mode configuration\n",
    "if ENABLE_REFINEMENT:\n",
    "    print(\"ðŸ§ª ENABLE_REFINEMENT ENABLED\")\n",
    "    print(\"   â†’ Will run: First inference â†’ Second inference\")\n",
    "    print(f\"   â†’ First inference: {FIRST_ATTEMPTS} attempts, {FIRST_WORKERS} workers\")\n",
    "    print(f\"   â†’ Second inference: {SECOND_ATTEMPTS} attempts, {SECOND_WORKERS} workers\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Standard mode (ENABLE_REFINEMENT disabled)\")\n",
    "    print(f\"   â†’ Will run: First inference only ({FIRST_ATTEMPTS} attempts, {FIRST_WORKERS} workers)\")\n",
    "\n",
    "# Optional: quick summary (helps avoid accidental submits)\n",
    "print(\n",
    "    \"Mode summary â†’ \"\n",
    "    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | ENABLE_REFINEMENT={ENABLE_REFINEMENT} |\\n\"\n",
    "    f\"TEST_INFERENCE={TEST_INFERENCE} | SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | INITIAL_MODEL={MODEL_PATH} | REFINEMENT_MODEL={REFINEMENT_MODEL_PATH}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81be53d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:06:56.352931Z",
     "iopub.status.busy": "2025-09-07T15:06:56.352591Z",
     "iopub.status.idle": "2025-09-07T15:07:09.841243Z",
     "shell.execute_reply": "2025-09-07T15:07:09.840641Z"
    },
    "papermill": {
     "duration": 13.493329,
     "end_time": "2025-09-07T15:07:09.842296",
     "exception": false,
     "start_time": "2025-09-07T15:06:56.348967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA version (PyTorch): 12.8\n",
      "CUDA available: True\n",
      "NumPy version: 1.26.4\n",
      "GPU count: 4\n",
      "GPU name: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672ee13e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:07:09.850127Z",
     "iopub.status.busy": "2025-09-07T15:07:09.849638Z",
     "iopub.status.idle": "2025-09-07T15:07:13.380015Z",
     "shell.execute_reply": "2025-09-07T15:07:13.379466Z"
    },
    "papermill": {
     "duration": 3.53521,
     "end_time": "2025-09-07T15:07:13.380999",
     "exception": false,
     "start_time": "2025-09-07T15:07:09.845789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang version: 0.5.2rc2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 15:07:12.667000 19 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0907 15:07:12.667000 19 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashInfer version: 0.3.0\n"
     ]
    }
   ],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcfdde5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:07:13.388980Z",
     "iopub.status.busy": "2025-09-07T15:07:13.388675Z",
     "iopub.status.idle": "2025-09-07T15:07:13.898517Z",
     "shell.execute_reply": "2025-09-07T15:07:13.897961Z"
    },
    "papermill": {
     "duration": 0.514889,
     "end_time": "2025-09-07T15:07:13.899502",
     "exception": false,
     "start_time": "2025-09-07T15:07:13.384613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptxas -> ptxas: NVIDIA (R) Ptx optimizing assembler\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:14:54_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ],
   "source": [
    "if IS_KAGGLE:\n",
    "    import os, shutil, subprocess, stat\n",
    "    \n",
    "    # 1) Where to place binaries + cache\n",
    "    WRK_BIN = \"/kaggle/working/bin\"\n",
    "    TRITON_CACHE = \"/kaggle/working/.triton\"\n",
    "    os.makedirs(WRK_BIN, exist_ok=True)\n",
    "    os.makedirs(TRITON_CACHE, exist_ok=True)\n",
    "    \n",
    "    # 2) Preferred source for ptxas/cuobjdump/nvdisasm\n",
    "    SYSTEM_CUDA_BIN = \"/usr/local/cuda/bin\"\n",
    "    FALLBACK_VENDORED = \"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin\"  # if you have it\n",
    "    \n",
    "    def copy_tool(name: str):\n",
    "        for src_dir in (SYSTEM_CUDA_BIN, FALLBACK_VENDORED):\n",
    "            src = os.path.join(src_dir, name)\n",
    "            if os.path.exists(src):\n",
    "                dst = os.path.join(WRK_BIN, name)\n",
    "                shutil.copy2(src, dst)\n",
    "                # ensure executable bit\n",
    "                os.chmod(dst, os.stat(dst).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "                return dst\n",
    "        raise FileNotFoundError(f\"Could not find {name} in {SYSTEM_CUDA_BIN} or {FALLBACK_VENDORED}\")\n",
    "    \n",
    "    ptxas_path = copy_tool(\"ptxas\")\n",
    "    try:\n",
    "        cuobjdump_path = copy_tool(\"cuobjdump\")\n",
    "    except FileNotFoundError:\n",
    "        cuobjdump_path = None  # optional\n",
    "    try:\n",
    "        nvdisasm_path = copy_tool(\"nvdisasm\")\n",
    "    except FileNotFoundError:\n",
    "        nvdisasm_path = None  # optional\n",
    "    \n",
    "    # 3) Environment for Triton/JIT\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n",
    "    os.environ[\"PATH\"] = f\"{WRK_BIN}:{os.environ.get('PATH','')}\"\n",
    "    os.environ[\"TRITON_CACHE_DIR\"] = TRITON_CACHE\n",
    "    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "    os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda\"\n",
    "    \n",
    "    # Helpful fallbacks if you still hit capture issues:\n",
    "    # os.environ[\"SGLANG_DISABLE_CUDA_GRAPH\"] = \"1\"      # skip CUDA graphs (degrades perf but avoids capture)\n",
    "    # os.environ[\"TRITON_CODEGEN_FATBIN\"] = \"0\"          # can reduce Triton fatbin steps on some setups\n",
    "    \n",
    "    # 4) Smoke test: ensure ptxas runs from the new location\n",
    "    print(\"ptxas ->\", subprocess.check_output([ptxas_path, \"--version\"]).decode().strip())\n",
    "    \n",
    "    # Now it's safe to import heavy libs that trigger Triton\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14131a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:07:13.907573Z",
     "iopub.status.busy": "2025-09-07T15:07:13.907091Z",
     "iopub.status.idle": "2025-09-07T15:10:14.484472Z",
     "shell.execute_reply": "2025-09-07T15:10:14.483429Z"
    },
    "papermill": {
     "duration": 180.582549,
     "end_time": "2025-09-07T15:10:14.485628",
     "exception": false,
     "start_time": "2025-09-07T15:07:13.903079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cleared.\n",
      "ðŸ”§ Using model from /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976\n",
      "LOG file path: /kaggle/working/sglang_server.log\n",
      "Started sglang server PID=42 | logging to /kaggle/working/sglang_server.log\n",
      "Command: /usr/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976 --dp 4 --kv-cache-dtype fp8_e4m3 --enable-metrics\n",
      "sglang not ready after timeout. Showing last 60 log lines:\n",
      "2025-09-07 15:07:51.676778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1757257671.822929      42 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1757257671.867468      42 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0907 15:08:27.575000 42 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0907 15:08:27.575000 42 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "[2025-09-07 15:08:31] server_args=ServerArgs(model_path='/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976', tokenizer_path='/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8080, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='fp8_e4m3', mem_fraction_static=0.871, max_running_requests=None, max_queued_requests=9223372036854775807, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=10412284, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, api_key=None, served_model_name='/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=4, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)\r\n",
      "[2025-09-07 15:08:31] Using default HuggingFace chat template with detected content format: string\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1757257734.442088     113 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1757257734.442910     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1757257734.451194     113 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1757257734.451797     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0907 15:09:13.035000 112 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0907 15:09:13.035000 112 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "W0907 15:09:13.036000 113 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0907 15:09:13.036000 113 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1757257776.427679     322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1757257776.436477     322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0907 15:09:54.749000 322 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0907 15:09:54.749000 322 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "[2025-09-07 15:09:55] Launch DP0 starting at GPU #0.\r\n",
      "[2025-09-07 15:09:55] Launch DP1 starting at GPU #1.\r\n",
      "[2025-09-07 15:09:55] Launch DP2 starting at GPU #2.\r\n",
      "[2025-09-07 15:09:55] Launch DP3 starting at GPU #3.\r\n",
      "Call stop_server() or full_cleanup() to shut it down gracefully.\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "  # Background server launcher for Kaggle with SGLang\n",
    "  import os, sys, time, subprocess, json, socket, requests\n",
    "\n",
    "  # ---------- 1) Check for existing server and cleanup ----------\n",
    "  PORT = 8080\n",
    "  HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "\n",
    "  # Check if server already running\n",
    "  try:\n",
    "      r = requests.get(HEALTH_URL, timeout=3)\n",
    "      if r.status_code == 200:\n",
    "          print(f\"Server already running on port {PORT}. Stopping it first...\")\n",
    "          # Kill existing sglang processes\n",
    "          subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "          time.sleep(3)  # Wait for cleanup\n",
    "  except:\n",
    "      pass  # No server running\n",
    "\n",
    "  # Clear CUDA memory before starting\n",
    "  try:\n",
    "      import torch\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.synchronize()\n",
    "          print(\"CUDA memory cleared.\")\n",
    "      num_gpus = torch.cuda.device_count()\n",
    "  except Exception:\n",
    "      num_gpus = 0\n",
    "      \n",
    "  model_path_to_use = str(MODEL_PATH)\n",
    "  print(f\"ðŸ”§ Using model from {model_path_to_use}\")\n",
    "\n",
    "  LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "  print(f\"LOG file path: {LOG}\")\n",
    "\n",
    "  SERVER_CMD = [\n",
    "      sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "      \"--host\", \"0.0.0.0\",\n",
    "      \"--port\", str(PORT),\n",
    "      \"--model-path\", model_path_to_use,\n",
    "      \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "      \"--kv-cache-dtype\", \"fp8_e4m3\",\n",
    "      \"--enable-metrics\"\n",
    "  ]\n",
    "\n",
    "  # ---------- 2) Launch in background ----------\n",
    "  log_f = open(LOG, \"w\")\n",
    "  env = os.environ.copy()\n",
    "  proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=SUBMIT_DIR)\n",
    "  print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "  print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "\n",
    "  # ---------- 3) Wait for readiness ----------\n",
    "  def wait_ready(url, timeout_s=180):\n",
    "      t0 = time.time()\n",
    "      while time.time() - t0 < timeout_s:\n",
    "          try:\n",
    "              r = requests.get(url, timeout=3)\n",
    "              if r.status_code == 200:\n",
    "                  return True\n",
    "          except Exception:\n",
    "              pass\n",
    "          time.sleep(2)\n",
    "      return False\n",
    "\n",
    "  ready = wait_ready(HEALTH_URL)\n",
    "  log_f.flush()\n",
    "\n",
    "  if ready:\n",
    "      print(f\"sglang is READY on port {PORT}.\")\n",
    "      print(f\"- Tail logs: !tail -n 50 {LOG}\")\n",
    "      print(f\"- List models: !curl -s http://127.0.0.1:{PORT}/v1/models | jq .\")\n",
    "  else:\n",
    "      print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "      log_f.close()\n",
    "      !tail -n 60 {LOG}\n",
    "\n",
    "  # ---------- 4) Cleanup functions ----------\n",
    "  def stop_server(p=proc):\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "      print(\"Server stopped.\")\n",
    "\n",
    "  def full_cleanup(p=proc):\n",
    "      # Stop server\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "\n",
    "      # Also kill any lingering sglang processes\n",
    "      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "      print(\"Server stopped and CUDA memory cleared.\")\n",
    "\n",
    "  print(\"Call stop_server() or full_cleanup() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7cbef78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:10:14.494835Z",
     "iopub.status.busy": "2025-09-07T15:10:14.494205Z",
     "iopub.status.idle": "2025-09-07T15:13:14.520932Z",
     "shell.execute_reply": "2025-09-07T15:13:14.520042Z"
    },
    "papermill": {
     "duration": 180.03253,
     "end_time": "2025-09-07T15:13:14.522072",
     "exception": false,
     "start_time": "2025-09-07T15:10:14.489542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âœ… Server is responding!\n",
      "Available models:\n",
      "  - /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976\n",
      "\n",
      "âœ… Found model: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "    import requests\n",
    "    import time\n",
    "    \n",
    "    def check_models():\n",
    "        url = \"http://127.0.0.1:8080/v1/models\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "    \n",
    "            print(\"âœ… Server is responding!\")\n",
    "            print(\"Available models:\")\n",
    "            for model in result['data']:\n",
    "                print(f\"  - {model['id']}\")\n",
    "    \n",
    "            return result['data'][0]['id'] if result['data'] else None\n",
    "    \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"âŒ Connection failed - server may not be ready yet\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Poll every 30 seconds until we get a model\n",
    "    model_name = None\n",
    "    while not model_name:\n",
    "        model_name = check_models()\n",
    "        if not model_name:\n",
    "            print(\"â³ Waiting 30 seconds before retrying...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    print(f\"\\nâœ… Found model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b318d666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:13:14.531932Z",
     "iopub.status.busy": "2025-09-07T15:13:14.531696Z",
     "iopub.status.idle": "2025-09-07T15:13:14.538921Z",
     "shell.execute_reply": "2025-09-07T15:13:14.538150Z"
    },
    "papermill": {
     "duration": 0.013393,
     "end_time": "2025-09-07T15:13:14.539910",
     "exception": false,
     "start_time": "2025-09-07T15:13:14.526517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"âœ… Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\nâ± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"ðŸ”¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"âš¡ Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c210fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:13:14.549260Z",
     "iopub.status.busy": "2025-09-07T15:13:14.549011Z",
     "iopub.status.idle": "2025-09-07T15:17:41.605355Z",
     "shell.execute_reply": "2025-09-07T15:17:41.604612Z"
    },
    "papermill": {
     "duration": 267.062507,
     "end_time": "2025-09-07T15:17:41.606710",
     "exception": false,
     "start_time": "2025-09-07T15:13:14.544203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_KAGGLE:\n",
    "    %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Use FIRST_ATTEMPTS and FIRST_WORKERS for initial inference\n",
    "MAX_ATTEMPTS = FIRST_ATTEMPTS\n",
    "MAX_WORKERS  = FIRST_WORKERS\n",
    "\n",
    "# SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# can use this instead if testing evaluation during a pre-run\n",
    "SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"First Inference â†’ {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "    \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--max_workers\", str(MAX_WORKERS),\n",
    "    \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "    \"--model\", model_name,\n",
    "    \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "    \"--unsafe-executor\",\n",
    "    \"--max-tokens\", \"2000\",\n",
    "    \"--qwen-no-think\"\n",
    "]\n",
    "\n",
    "# Add parquet output directory if set\n",
    "if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "  cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Use global timeout if in competition rerun mode\n",
    "if IS_RERUN and deadline:\n",
    "    try:\n",
    "        run_with_global_timeout(\n",
    "            cmd_args,\n",
    "            deadline,\n",
    "            name=\"First Inference\",\n",
    "            env=os.environ.copy(),\n",
    "            cwd=os.getcwd(),\n",
    "            stream_output=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(\"âœ… First inference completed successfully!\")\n",
    "    except TimeoutError as e:\n",
    "        print(f\"â±ï¸ First inference timed out: {e}\")\n",
    "        print(\"   â†’ Continuing to next step with partial results...\")\n",
    "        # Continue execution - don't raise, just log the timeout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ First inference failed with return code {e.returncode}\")\n",
    "        print(\"   â†’ Continuing to next step...\")\n",
    "        # Continue execution - don't raise, just log the error\n",
    "else:\n",
    "    # Handle output redirection for non-rerun modes\n",
    "    if not IS_KAGGLE:\n",
    "        # For quiet mode, redirect to file using subprocess\n",
    "        import subprocess\n",
    "        log_file_path = f\"{SUBMIT_DIR}/run.log\"\n",
    "        print(f\"ðŸ“ Logging output to: {log_file_path}\")\n",
    "        \n",
    "        with open(log_file_path, \"w\") as log_file:\n",
    "            process = subprocess.Popen(\n",
    "                cmd_args,\n",
    "                stdout=log_file,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "                cwd=os.getcwd()\n",
    "            )\n",
    "            \n",
    "            # Wait for completion\n",
    "            print(\"â³ Running tasks (output being written to log file)...\")\n",
    "            return_code = process.wait()\n",
    "            \n",
    "        if return_code == 0:\n",
    "            print(f\"âœ… Task runner completed successfully. Check {log_file_path} for details.\")\n",
    "        else:\n",
    "            print(f\"âŒ Task runner failed with return code {return_code}\")\n",
    "            print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "            # Show last few lines of log\n",
    "            !tail -n 20 {log_file_path}\n",
    "    else:\n",
    "        # For interactive mode, show output directly\n",
    "        cmd = \" \".join(cmd_args)\n",
    "        print(f\"Running: {cmd}\\n\")\n",
    "        !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c31739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:17:41.629547Z",
     "iopub.status.busy": "2025-09-07T15:17:41.628941Z",
     "iopub.status.idle": "2025-09-07T15:17:41.632710Z",
     "shell.execute_reply": "2025-09-07T15:17:41.632225Z"
    },
    "papermill": {
     "duration": 0.015908,
     "end_time": "2025-09-07T15:17:41.633633",
     "exception": false,
     "start_time": "2025-09-07T15:17:41.617725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH type: <class 'pathlib.PosixPath'>, value: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-2-partial-20-c976\n"
     ]
    }
   ],
   "source": [
    "print(f\"MODEL_PATH type: {type(MODEL_PATH)}, value: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b583d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:17:41.655727Z",
     "iopub.status.busy": "2025-09-07T15:17:41.655322Z",
     "iopub.status.idle": "2025-09-07T15:22:14.973587Z",
     "shell.execute_reply": "2025-09-07T15:22:14.973023Z"
    },
    "papermill": {
     "duration": 273.3411,
     "end_time": "2025-09-07T15:22:14.985328",
     "exception": false,
     "start_time": "2025-09-07T15:17:41.644228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Restarting inference server with updated model...\n",
      "ðŸ›‘ Gracefully stopping existing server...\n",
      "âœ… Server stopped gracefully\n",
      "âœ… CUDA memory cleared\n",
      "ðŸŽ¯ Using model: /kaggle/input/refinement-model/Qwen3-4B_ds-arc-agi-1-refinement-finetuning-partialplus-c552\n",
      "   â†’ Refinement model\n",
      "ðŸš€ Starting server: /usr/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /kaggle/input/refinement-model/Qwen3-4B_ds-arc-agi-1-refinement-finetuning-partialplus-c552 --dp 4 --kv-cache-dtype fp8_e4m3 --enable-metrics\n",
      "âœ… Server started with PID=10016\n",
      "âœ… Server ready!\n",
      "ðŸŽ¯ Model: /kaggle/input/refinement-model/Qwen3-4B_ds-arc-agi-1-refinement-finetuning-partialplus-c552\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_REFINEMENT:\n",
    "  # Restart the server with the (potentially) new model\n",
    "  if START_SERVER:\n",
    "      print(\"ðŸ”„ Restarting inference server with updated model...\")\n",
    "\n",
    "      # Gracefully stop existing server if it exists\n",
    "      if 'proc' in locals() and proc.poll() is None:  # Check if process is still running\n",
    "          print(\"ðŸ›‘ Gracefully stopping existing server...\")\n",
    "          try:\n",
    "              proc.terminate()  # Send SIGTERM first\n",
    "              proc.wait(timeout=30)  # Wait up to 30 seconds for graceful shutdown\n",
    "              print(\"âœ… Server stopped gracefully\")\n",
    "          except subprocess.TimeoutExpired:\n",
    "              print(\"âš ï¸  Server didn't stop gracefully, force killing...\")\n",
    "              proc.kill()\n",
    "              proc.wait()\n",
    "          except Exception as e:\n",
    "              print(f\"âš ï¸  Error stopping server: {e}\")\n",
    "\n",
    "      # Wait a bit longer after graceful shutdown\n",
    "      time.sleep(5)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "              print(\"âœ… CUDA memory cleared\")\n",
    "      except Exception:\n",
    "          pass\n",
    "\n",
    "      # Get GPU count\n",
    "      try:\n",
    "          import torch\n",
    "          num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "      except:\n",
    "          num_gpus = 1\n",
    "\n",
    "      # Choose which model to use: refinement if available, otherwise original\n",
    "      model_to_use = REFINEMENT_MODEL_PATH if REFINEMENT_MODEL_PATH else MODEL_PATH\n",
    "      print(f\"ðŸŽ¯ Using model: {model_to_use}\")\n",
    "      print(f\"   â†’ {'Refinement' if REFINEMENT_MODEL_PATH else 'Original'} model\")\n",
    "\n",
    "      # Restart server with appropriate model\n",
    "      PORT = 8080\n",
    "      LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "      SERVER_CMD = [\n",
    "          sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "          \"--host\", \"0.0.0.0\",\n",
    "          \"--port\", str(PORT),\n",
    "          \"--model-path\", str(model_to_use),\n",
    "          \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "          \"--kv-cache-dtype\", \"fp8_e4m3\",\n",
    "          \"--enable-metrics\"\n",
    "      ]\n",
    "\n",
    "      print(f\"ðŸš€ Starting server: {' '.join(SERVER_CMD)}\")\n",
    "\n",
    "      log_f = open(LOG, \"a\")\n",
    "      proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT,\n",
    "                             env=os.environ.copy(), cwd=SUBMIT_DIR)\n",
    "\n",
    "      print(f\"âœ… Server started with PID={proc.pid}\")\n",
    "\n",
    "      # Wait for readiness with better error handling\n",
    "      def wait_ready(url, timeout_s=600):\n",
    "          t0 = time.time()\n",
    "          while time.time() - t0 < timeout_s:\n",
    "              try:\n",
    "                  r = requests.get(url, timeout=5)\n",
    "                  if r.status_code == 200:\n",
    "                      return True\n",
    "              except Exception:\n",
    "                  pass\n",
    "              time.sleep(3)  # Check less frequently\n",
    "          return False\n",
    "\n",
    "      HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "      if wait_ready(HEALTH_URL):\n",
    "          print(\"âœ… Server ready!\")\n",
    "\n",
    "          # Update model_name\n",
    "          try:\n",
    "              response = requests.get(HEALTH_URL)\n",
    "              if response.status_code == 200:\n",
    "                  models = response.json()['data']\n",
    "                  if models:\n",
    "                      model_name = models[0]['id']\n",
    "                      print(f\"ðŸŽ¯ Model: {model_name}\")\n",
    "          except Exception as e:\n",
    "              print(f\"âš ï¸  Could not get model name: {e}\")\n",
    "      else:\n",
    "          print(\"âŒ Server failed to start properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a226b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:22:15.008032Z",
     "iopub.status.busy": "2025-09-07T15:22:15.007506Z",
     "iopub.status.idle": "2025-09-07T15:25:51.850646Z",
     "shell.execute_reply": "2025-09-07T15:25:51.849896Z"
    },
    "papermill": {
     "duration": 216.856383,
     "end_time": "2025-09-07T15:25:51.852532",
     "exception": false,
     "start_time": "2025-09-07T15:22:14.996149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Second Inference Run - ENABLE_REFINEMENT\n",
    "# Only runs when ENABLE_REFINEMENT=true\n",
    "\n",
    "import glob\n",
    "\n",
    "if ENABLE_REFINEMENT:\n",
    "    print(\"ðŸ”„ Running SECOND inference (ENABLE_REFINEMENT mode)\")\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        %cd /workspace/arc-agi-2025\n",
    "\n",
    "    # Use SECOND_ATTEMPTS and SECOND_WORKERS for second inference\n",
    "    MAX_ATTEMPTS = SECOND_ATTEMPTS\n",
    "    MAX_WORKERS  = SECOND_WORKERS\n",
    "\n",
    "    SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "    print(f\"ENABLE_REFINEMENT Second Run â†’ {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "    # ðŸ”‘ Find the latest parquet file in inference_dir\n",
    "    parquet_files = glob.glob(os.path.join(inference_dir, \"*.parquet\"))\n",
    "    if not parquet_files:\n",
    "        print(f\"âš ï¸ No parquet files found in {inference_dir} - skipping refinement\")\n",
    "    else:\n",
    "        latest_parquet = max(parquet_files, key=os.path.getctime)\n",
    "        print(f\"ðŸ“‚ Using latest parquet file for refinement: {latest_parquet}\")\n",
    "\n",
    "        # Build the command\n",
    "        cmd_args = [\n",
    "            \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "            \"--dataset\", DATASET,\n",
    "            \"--subset\", SUBSET,\n",
    "            \"--max_workers\", str(MAX_WORKERS),\n",
    "            \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "            \"--model\", model_name,\n",
    "            \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "            \"--unsafe-executor\",\n",
    "            \"--max-tokens\", \"2000\",\n",
    "            \"--qwen-no-think\",\n",
    "            \"--refinement-ds\", latest_parquet,   # ðŸ‘ˆ add parquet path here\n",
    "            \"--include-outputs\"\n",
    "        ]\n",
    "\n",
    "        # Add parquet output directory if set\n",
    "        if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "          cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "\n",
    "        print(f\"Running ENABLE_REFINEMENT second inference: {' '.join(cmd_args)}\")\n",
    "\n",
    "        # Use global timeout if in competition rerun mode\n",
    "        if IS_RERUN and deadline:\n",
    "            try:\n",
    "                run_with_global_timeout(\n",
    "                    cmd_args,\n",
    "                    deadline,\n",
    "                    name=\"Second Inference (Refinement)\",\n",
    "                    env=os.environ.copy(),\n",
    "                    cwd=os.getcwd(),\n",
    "                    stream_output=True,\n",
    "                    check=True\n",
    "                )\n",
    "                print(\"âœ… Second inference (refinement) completed successfully!\")\n",
    "            except TimeoutError as e:\n",
    "                print(f\"â±ï¸ Second inference timed out: {e}\")\n",
    "                print(\"   â†’ Continuing to submission generation with available results...\")\n",
    "                # Continue execution - don't raise, just log the timeout\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"âŒ Second inference failed with return code {e.returncode}\")\n",
    "                print(\"   â†’ Continuing to submission generation...\")\n",
    "                # Continue execution - don't raise, just log the error\n",
    "        else:\n",
    "            # Handle output redirection for non-rerun modes\n",
    "            if not IS_KAGGLE:\n",
    "                # For quiet mode, redirect to file using subprocess\n",
    "                import subprocess\n",
    "                log_file_path = f\"{SUBMIT_DIR}/run_second.log\"\n",
    "                print(f\"ðŸ“ Logging ENABLE_REFINEMENT second run output to: {log_file_path}\")\n",
    "                \n",
    "                with open(log_file_path, \"w\") as log_file:\n",
    "                    process = subprocess.Popen(\n",
    "                        cmd_args,\n",
    "                        stdout=log_file,\n",
    "                        stderr=subprocess.STDOUT,\n",
    "                        text=True,\n",
    "                        cwd=os.getcwd()\n",
    "                    )\n",
    "                    \n",
    "                    # Wait for completion\n",
    "                    print(\"â³ Running ENABLE_REFINEMENT second inference (output being written to log file)...\")\n",
    "                    return_code = process.wait()\n",
    "                    \n",
    "                if return_code == 0:\n",
    "                    print(f\"âœ… ENABLE_REFINEMENT second inference completed successfully. Check {log_file_path} for details.\")\n",
    "                else:\n",
    "                    print(f\"âŒ ENABLE_REFINEMENT second inference failed with return code {return_code}\")\n",
    "                    print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "                    # Show last few lines of log\n",
    "                    !tail -n 20 {log_file_path}\n",
    "            else:\n",
    "                # For interactive mode, show output directly\n",
    "                cmd = \" \".join(cmd_args)\n",
    "                print(f\"Running ENABLE_REFINEMENT second inference: {cmd}\\n\")\n",
    "                !{cmd}\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ”„ Skipping second inference (ENABLE_REFINEMENT=false)\")\n",
    "    print(\"   â†’ Standard mode runs first inference only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ed4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:25:51.922507Z",
     "iopub.status.busy": "2025-09-07T15:25:51.921902Z",
     "iopub.status.idle": "2025-09-07T15:26:02.672508Z",
     "shell.execute_reply": "2025-09-07T15:26:02.671922Z"
    },
    "papermill": {
     "duration": 10.786398,
     "end_time": "2025-09-07T15:26:02.673590",
     "exception": false,
     "start_time": "2025-09-07T15:25:51.887192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate submission using the two most recent parquet files\n",
    "if os.environ.get(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "    print(\"ðŸŽ¯ Generating submission from the two most recent parquet files...\")\n",
    "    \n",
    "    import subprocess\n",
    "    \n",
    "    output_dir = str(SUBMIT_DIR)\n",
    "    \n",
    "    # Command to generate submission using the two most recent parquet files\n",
    "    submission_cmd = [\n",
    "        \"uv\", \"run\", \"python\", \"-m\", \"llm_python.generate_submission\",\n",
    "        \"--parquet-path\", inference_dir,\n",
    "        \"--n-files\", \"2\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--output-dir\", output_dir,\n",
    "        \"--debug\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running submission generation: {' '.join(submission_cmd)}\")\n",
    "    print(f\"ðŸ“‚ Looking for parquet files in: {inference_dir}\")\n",
    "    \n",
    "    # Always use the original approach - submission generation should NOT be subject to global timeout\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            submission_cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,  # 5 minute timeout (separate from global timeout)\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Submission generation completed successfully!\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Update submit_dir to point to the generated file\n",
    "            submit_dir = f\"{output_dir}/submission.json\"\n",
    "            print(f\"ðŸ“ Submission file: {submit_dir}\")\n",
    "        else:\n",
    "            print(f\"âŒ Submission generation failed with return code {result.returncode}\")\n",
    "            print(f\"STDOUT: {result.stdout}\")\n",
    "            print(f\"STDERR: {result.stderr}\")\n",
    "            # Fallback to default submission path\n",
    "            submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Submission generation timed out\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Submission generation error: {e}\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "else:\n",
    "    print(\"ðŸ“ Skipping submission generation (SUBMIT=false)\")\n",
    "    submit_dir = f\"{SUBMIT_DIR}/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f1f27c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:26:02.710084Z",
     "iopub.status.busy": "2025-09-07T15:26:02.709538Z",
     "iopub.status.idle": "2025-09-07T15:26:04.082130Z",
     "shell.execute_reply": "2025-09-07T15:26:04.081396Z"
    },
    "papermill": {
     "duration": 1.391929,
     "end_time": "2025-09-07T15:26:04.083481",
     "exception": false,
     "start_time": "2025-09-07T15:26:02.691552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validating submission file: /kaggle/working/submission.json\r\n",
      "\r\n",
      "ðŸ” VALIDATING SUBMISSION: /kaggle/working/submission.json\r\n",
      "ðŸ“Š Validation Results:\r\n",
      "  Total tasks: 120\r\n",
      "  Total predictions: 172\r\n",
      "  Empty predictions ([[0,0],[0,0]]): 1\r\n",
      "âœ… VALIDATION PASSED - No structural errors found\r\n",
      "ðŸŽ¯ Submission file is ready for competition!\r\n",
      "ðŸ“‚ Loading submission: /kaggle/working/submission.json\r\n",
      "ðŸ” Scoring against arc-prize-2025/evaluation\r\n",
      "============================================================\r\n",
      "SUBMISSION SCORING RESULTS\r\n",
      "============================================================\r\n",
      "Dataset: arc-prize-2025\r\n",
      "Subset: evaluation\r\n",
      "Reference tasks: 120\r\n",
      "Tasks scored: 120\r\n",
      "Total predictions: 344\r\n",
      "\r\n",
      "ðŸ“Š PREDICTION-LEVEL METRICS:\r\n",
      "  Pass@1 (first attempt): 1/344 (0.3%)\r\n",
      "  Pass@2 (either attempt): 1/344 (0.3%)\r\n",
      "\r\n",
      "ðŸ“Š TASK-LEVEL METRICS:\r\n",
      "  Tasks Pass@1 (all outputs correct on first attempt): 1/120 (0.8%)\r\n",
      "  Tasks Pass@2 (all outputs correct on either attempt): 1/120 (0.8%)\r\n"
     ]
    }
   ],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission {submit_dir} --dataset {DATASET} --subset {SUBSET}\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8855b956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:26:04.120374Z",
     "iopub.status.busy": "2025-09-07T15:26:04.120136Z",
     "iopub.status.idle": "2025-09-07T15:26:04.146671Z",
     "shell.execute_reply": "2025-09-07T15:26:04.146138Z"
    },
    "papermill": {
     "duration": 0.045552,
     "end_time": "2025-09-07T15:26:04.147547",
     "exception": false,
     "start_time": "2025-09-07T15:26:04.101995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up server and resources...\n",
      "Server stopped and CUDA memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup - stop server and free resources\n",
    "if START_SERVER and 'full_cleanup' in globals():\n",
    "    print(\"ðŸ§¹ Cleaning up server and resources...\")\n",
    "    full_cleanup()\n",
    "else:\n",
    "    print(\"ðŸ” No server cleanup needed (START_SERVER=False or cleanup function not available)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8063856,
     "isSourceIdPinned": false,
     "sourceId": 12964217,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211105,
     "sourceId": 12973191,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 260395312,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1160.813478,
   "end_time": "2025-09-07T15:26:06.781035",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-07T15:06:45.967557",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
