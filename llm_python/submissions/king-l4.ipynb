{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17965875",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-27T17:44:02.069825Z",
     "iopub.status.busy": "2025-08-27T17:44:02.069603Z",
     "iopub.status.idle": "2025-08-27T17:44:02.106700Z",
     "shell.execute_reply": "2025-08-27T17:44:02.106198Z"
    },
    "papermill": {
     "duration": 0.042995,
     "end_time": "2025-08-27T17:44:02.107619",
     "exception": false,
     "start_time": "2025-08-27T17:44:02.064624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for models in Kaggle environment...\n",
      "   Looking for initial model dataset: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset\n",
      "   âœ… Found initial model at: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "      Contents: ['model.safetensors.index.json', 'config.json', 'merges.txt', 'model-00001-of-00002.safetensors', '.cache']\n",
      "   Looking for fine-tune base dataset: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset\n",
      "   âœ… Found fine-tune base at: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "      Contents: ['model.safetensors.index.json', 'config.json', 'merges.txt', 'model-00001-of-00002.safetensors', '.cache']\n",
      "\n",
      "ðŸ“¦ Final model paths:\n",
      "   Initial model: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "   Fine-tune base: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "ðŸ”§ Development run â€” setting short 60s timeout for testing\n",
      "â° Global timeout set to 60s (0.0 hours)\n",
      "ðŸ§ª TTT (Test-Time Training) mode ENABLED\n",
      "   â†’ Will run: First inference â†’ Fine-tuning â†’ Second inference\n",
      "   â†’ First inference: 128 attempts, 64 workers\n",
      "   â†’ Second inference: 128 attempts, 64 workers\n",
      "Mode summary â†’ IS_KAGGLE=True | IS_RERUN=False | TTT_MODE=True |\n",
      "TEST_INFERENCE=False | SCORE=False | SUBMIT=true | INITIAL_MODEL=/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# # Model for initial inference (path for Kaggle, slug for local/RunPod)\n",
    "# INITIAL_MODEL_PATH = \"openai/gpt-oss-20b\"  # For local/RunPod\n",
    "# INITIAL_MODEL_KAGGLE = \"openaigpt-oss-20b\"  # Kaggle dataset name\n",
    "\n",
    "INITIAL_MODEL_PATH = \"Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\"  # For local/RunPod\n",
    "INITIAL_MODEL_KAGGLE = \"arc-1-fake-ttt-blended-c802-dataset\"  # Kaggle dataset name\n",
    "\n",
    "# Base model for fine-tuning (used after fine-tuning for second inference)\n",
    "FINETUNE_BASE_MODEL = \"Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\"  # For local/RunPod\n",
    "FINETUNE_BASE_KAGGLE = \"arc-1-fake-ttt-blended-c802-dataset\"  # Kaggle dataset name\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# First inference settings\n",
    "FIRST_ATTEMPTS = 128      # Number of attempts for first inference\n",
    "FIRST_WORKERS = 64       # Number of workers for first inference\n",
    "\n",
    "# Second inference settings (after fine-tuning in TTT mode)\n",
    "SECOND_ATTEMPTS = 128     # Number of attempts for second inference\n",
    "SECOND_WORKERS = 64      # Number of workers for second inference\n",
    "\n",
    "# ============================================================================\n",
    "# Other Configuration\n",
    "# ============================================================================\n",
    "\n",
    "DATASET = \"arc-prize-2025\"\n",
    "\n",
    "# ---- Config flags (single source of truth) ----\n",
    "START_SERVER = True\n",
    "TEST_INFERENCE = False          # set False unless you want a quick endpoint smoke test\n",
    "SCORE = False                   # default; overridden in branches below\n",
    "\n",
    "# TTT Mode: Controls Test-Time Training workflow\n",
    "TTT_MODE = True\n",
    "\n",
    "# Env-backed flags\n",
    "IS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "IS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# String env flag for external tools\n",
    "os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# ---- Paths ----\n",
    "if IS_KAGGLE:\n",
    "    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n",
    "    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n",
    "    SUBMIT_DIR      = Path(\"/kaggle/working\")\n",
    "    ARC_PROGRAMS_PARQUET = SUBMIT_DIR\n",
    "\n",
    "    print(\"ðŸ” Searching for models in Kaggle environment...\")\n",
    "\n",
    "    # Auto-find initial model path in Kaggle's dataset structure\n",
    "    model_dataset_path = ARC_DATA_ROOT / INITIAL_MODEL_KAGGLE\n",
    "    print(f\"   Looking for initial model dataset: {model_dataset_path}\")\n",
    "\n",
    "    if model_dataset_path.exists() and model_dataset_path.is_dir():\n",
    "        # Kaggle datasets have version folders, find the first subdirectory\n",
    "        subdirs = [d for d in model_dataset_path.iterdir() if d.is_dir()]\n",
    "        if subdirs:\n",
    "            MODEL_PATH = subdirs[0]  # Use the first (usually only) version folder\n",
    "            print(f\"   âœ… Found initial model at: {MODEL_PATH}\")\n",
    "            # List what's inside to confirm it's right\n",
    "            model_contents = list(MODEL_PATH.iterdir())[:5]  # Show first 5 items\n",
    "            print(f\"      Contents: {[f.name for f in model_contents]}\")\n",
    "        else:\n",
    "            # Fallback if no subdirectory found\n",
    "            MODEL_PATH = model_dataset_path\n",
    "            print(f\"   âš ï¸ No version folder found for initial model, using: {MODEL_PATH}\")\n",
    "    else:\n",
    "        MODEL_PATH = model_dataset_path\n",
    "        print(f\"   âŒ Initial model dataset not found at: {MODEL_PATH}\")\n",
    "        print(f\"      Available datasets: {[d.name for d in ARC_DATA_ROOT.iterdir() if d.is_dir()][:10]}\")\n",
    "\n",
    "    # Auto-find fine-tuning base model path\n",
    "    finetune_dataset_path = ARC_DATA_ROOT / FINETUNE_BASE_KAGGLE\n",
    "    print(f\"   Looking for fine-tune base dataset: {finetune_dataset_path}\")\n",
    "\n",
    "    if finetune_dataset_path.exists() and finetune_dataset_path.is_dir():\n",
    "        subdirs = [d for d in finetune_dataset_path.iterdir() if d.is_dir()]\n",
    "        if subdirs:\n",
    "            FINETUNE_BASE_PATH = subdirs[0]\n",
    "            print(f\"   âœ… Found fine-tune base at: {FINETUNE_BASE_PATH}\")\n",
    "            # List what's inside to confirm it's right\n",
    "            finetune_contents = list(FINETUNE_BASE_PATH.iterdir())[:5]  # Show first 5 items\n",
    "            print(f\"      Contents: {[f.name for f in finetune_contents]}\")\n",
    "        else:\n",
    "            FINETUNE_BASE_PATH = finetune_dataset_path\n",
    "            print(f\"   âš ï¸ No version folder found for fine-tune base, using: {FINETUNE_BASE_PATH}\")\n",
    "    else:\n",
    "        FINETUNE_BASE_PATH = finetune_dataset_path\n",
    "        print(f\"   âŒ Fine-tune base dataset not found at: {FINETUNE_BASE_PATH}\")\n",
    "\n",
    "    print(f\"\\nðŸ“¦ Final model paths:\")\n",
    "    print(f\"   Initial model: {MODEL_PATH}\")\n",
    "    print(f\"   Fine-tune base: {FINETUNE_BASE_PATH}\")\n",
    "\n",
    "else:\n",
    "    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n",
    "    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n",
    "    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n",
    "    ARC_PROGRAMS_PARQUET = Path(\"/workspace/arc-agi-2025/llm_python/datasets/inference\")\n",
    "\n",
    "    # Use local/RunPod model paths\n",
    "    MODEL_PATH = INITIAL_MODEL_PATH\n",
    "    FINETUNE_BASE_PATH = FINETUNE_BASE_MODEL\n",
    "\n",
    "    print(f\"ðŸ“¦ Local/RunPod model paths:\")\n",
    "    print(f\"   Initial model: {MODEL_PATH}\")\n",
    "    print(f\"   Fine-tune base: {FINETUNE_BASE_PATH}\")\n",
    "\n",
    "# Initialize fine-tuned model path (will be set after fine-tuning)\n",
    "FINE_TUNED_MODEL_PATH = None\n",
    "\n",
    "# Export envs for downstream processes\n",
    "os.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\n",
    "os.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\n",
    "os.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\n",
    "os.environ[\"ARC_PROGRAMS_PARQUET\"] = str(ARC_PROGRAMS_PARQUET)\n",
    "os.environ[\"MODEL_PATH\"] = str(MODEL_PATH)\n",
    "\n",
    "# Export TTT and config flags for subprocess use\n",
    "os.environ[\"TTT_MODE\"] = str(TTT_MODE).lower()\n",
    "os.environ[\"IS_KAGGLE\"] = str(IS_KAGGLE).lower()\n",
    "os.environ[\"IS_RERUN\"] = str(IS_RERUN).lower()\n",
    "os.environ[\"DATASET\"] = DATASET\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Timeouts & mode tweaks ----\n",
    "FULL_TIMEOUT = 3600*5 - 600 # ~5 hour timeout for inference\n",
    "\n",
    "if IS_RERUN:\n",
    "    # Kaggle competition rerun\n",
    "    timeout_seconds = FULL_TIMEOUT\n",
    "    print(f\"ðŸ† Competition rerun detected â€” setting FULL {timeout_seconds}s timeout for ARC task runner\")\n",
    "    TEST_INFERENCE = False\n",
    "    SCORE = False\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "elif not IS_KAGGLE:\n",
    "    # Runpod / local long run\n",
    "    timeout_seconds = FULL_TIMEOUT\n",
    "    print(f\"ðŸ–¥ï¸ Runpod/local long run â€” setting FULL {timeout_seconds}s timeout for ARC task runner\")\n",
    "    if os.getenv(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "        SCORE = True  # if we're generating a submission, do scoring\n",
    "\n",
    "else:\n",
    "    # Kaggle dev/testing\n",
    "    timeout_seconds = 60  # 1 minute\n",
    "    print(f\"ðŸ”§ Development run â€” setting short {timeout_seconds}s timeout for testing\")\n",
    "    # Safer default: don't auto-submit in dev\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# Export timeout\n",
    "os.environ[\"GLOBAL_TIMEOUT\"] = str(timeout_seconds)\n",
    "print(f\"â° Global timeout set to {timeout_seconds}s ({timeout_seconds/3600:.1f} hours)\")\n",
    "\n",
    "# TTT Mode configuration\n",
    "if TTT_MODE:\n",
    "    print(\"ðŸ§ª TTT (Test-Time Training) mode ENABLED\")\n",
    "    print(\"   â†’ Will run: First inference â†’ Fine-tuning â†’ Second inference\")\n",
    "    print(f\"   â†’ First inference: {FIRST_ATTEMPTS} attempts, {FIRST_WORKERS} workers\")\n",
    "    print(f\"   â†’ Second inference: {SECOND_ATTEMPTS} attempts, {SECOND_WORKERS} workers\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Standard mode (TTT disabled)\")\n",
    "    print(f\"   â†’ Will run: First inference only ({FIRST_ATTEMPTS} attempts, {FIRST_WORKERS} workers)\")\n",
    "\n",
    "# Optional: quick summary (helps avoid accidental submits)\n",
    "print(\n",
    "    \"Mode summary â†’ \"\n",
    "    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | TTT_MODE={TTT_MODE} |\\n\"\n",
    "    f\"TEST_INFERENCE={TEST_INFERENCE} | SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | INITIAL_MODEL={MODEL_PATH}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c98e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:44:02.115298Z",
     "iopub.status.busy": "2025-08-27T17:44:02.114933Z",
     "iopub.status.idle": "2025-08-27T17:44:14.236944Z",
     "shell.execute_reply": "2025-08-27T17:44:14.236350Z"
    },
    "papermill": {
     "duration": 12.126747,
     "end_time": "2025-08-27T17:44:14.238018",
     "exception": false,
     "start_time": "2025-08-27T17:44:02.111271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA version (PyTorch): 12.8\n",
      "CUDA available: True\n",
      "NumPy version: 1.26.4\n",
      "GPU count: 4\n",
      "GPU name: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c6778d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:44:14.246204Z",
     "iopub.status.busy": "2025-08-27T17:44:14.245879Z",
     "iopub.status.idle": "2025-08-27T17:44:17.398895Z",
     "shell.execute_reply": "2025-08-27T17:44:17.398279Z"
    },
    "papermill": {
     "duration": 3.158248,
     "end_time": "2025-08-27T17:44:17.399972",
     "exception": false,
     "start_time": "2025-08-27T17:44:14.241724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang version: 0.5.1.post2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 17:44:16.782000 19 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0827 17:44:16.782000 19 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashInfer version: 0.2.14.post1\n"
     ]
    }
   ],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575369de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:44:17.408593Z",
     "iopub.status.busy": "2025-08-27T17:44:17.408054Z",
     "iopub.status.idle": "2025-08-27T17:44:17.900166Z",
     "shell.execute_reply": "2025-08-27T17:44:17.899590Z"
    },
    "papermill": {
     "duration": 0.497273,
     "end_time": "2025-08-27T17:44:17.901140",
     "exception": false,
     "start_time": "2025-08-27T17:44:17.403867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptxas -> ptxas: NVIDIA (R) Ptx optimizing assembler\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:14:54_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ],
   "source": [
    "if IS_KAGGLE:\n",
    "    import os, shutil, subprocess, stat\n",
    "    \n",
    "    # 1) Where to place binaries + cache\n",
    "    WRK_BIN = \"/kaggle/working/bin\"\n",
    "    TRITON_CACHE = \"/kaggle/working/.triton\"\n",
    "    os.makedirs(WRK_BIN, exist_ok=True)\n",
    "    os.makedirs(TRITON_CACHE, exist_ok=True)\n",
    "    \n",
    "    # 2) Preferred source for ptxas/cuobjdump/nvdisasm\n",
    "    SYSTEM_CUDA_BIN = \"/usr/local/cuda/bin\"\n",
    "    FALLBACK_VENDORED = \"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin\"  # if you have it\n",
    "    \n",
    "    def copy_tool(name: str):\n",
    "        for src_dir in (SYSTEM_CUDA_BIN, FALLBACK_VENDORED):\n",
    "            src = os.path.join(src_dir, name)\n",
    "            if os.path.exists(src):\n",
    "                dst = os.path.join(WRK_BIN, name)\n",
    "                shutil.copy2(src, dst)\n",
    "                # ensure executable bit\n",
    "                os.chmod(dst, os.stat(dst).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "                return dst\n",
    "        raise FileNotFoundError(f\"Could not find {name} in {SYSTEM_CUDA_BIN} or {FALLBACK_VENDORED}\")\n",
    "    \n",
    "    ptxas_path = copy_tool(\"ptxas\")\n",
    "    try:\n",
    "        cuobjdump_path = copy_tool(\"cuobjdump\")\n",
    "    except FileNotFoundError:\n",
    "        cuobjdump_path = None  # optional\n",
    "    try:\n",
    "        nvdisasm_path = copy_tool(\"nvdisasm\")\n",
    "    except FileNotFoundError:\n",
    "        nvdisasm_path = None  # optional\n",
    "    \n",
    "    # 3) Environment for Triton/JIT\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n",
    "    os.environ[\"PATH\"] = f\"{WRK_BIN}:{os.environ.get('PATH','')}\"\n",
    "    os.environ[\"TRITON_CACHE_DIR\"] = TRITON_CACHE\n",
    "    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "    os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda\"\n",
    "    \n",
    "    # Helpful fallbacks if you still hit capture issues:\n",
    "    # os.environ[\"SGLANG_DISABLE_CUDA_GRAPH\"] = \"1\"      # skip CUDA graphs (degrades perf but avoids capture)\n",
    "    # os.environ[\"TRITON_CODEGEN_FATBIN\"] = \"0\"          # can reduce Triton fatbin steps on some setups\n",
    "    \n",
    "    # 4) Smoke test: ensure ptxas runs from the new location\n",
    "    print(\"ptxas ->\", subprocess.check_output([ptxas_path, \"--version\"]).decode().strip())\n",
    "    \n",
    "    # Now it's safe to import heavy libs that trigger Triton\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8b5a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:44:17.909819Z",
     "iopub.status.busy": "2025-08-27T17:44:17.909592Z",
     "iopub.status.idle": "2025-08-27T17:47:18.361360Z",
     "shell.execute_reply": "2025-08-27T17:47:18.360680Z"
    },
    "papermill": {
     "duration": 180.45735,
     "end_time": "2025-08-27T17:47:18.362411",
     "exception": false,
     "start_time": "2025-08-27T17:44:17.905061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cleared.\n",
      "ðŸ”§ Using model from /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "LOG file path: /kaggle/working/sglang_server.log\n",
      "Started sglang server PID=42 | logging to /kaggle/working/sglang_server.log\n",
      "Command: /usr/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --dp 4 --kv-cache-dtype fp8_e4m3\n",
      "sglang not ready after timeout. Showing last 60 log lines:\n",
      "2025-08-27 17:44:48.286342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316688.424776      42 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1756316688.466371      42 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0827 17:45:20.098000 42 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:45:20.098000 42 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "[2025-08-27 17:45:22] server_args=ServerArgs(model_path='/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542', tokenizer_path='/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8080, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='fp8_e4m3', mem_fraction_static=0.871, max_running_requests=None, max_queued_requests=9223372036854775807, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=926277166, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, api_key=None, served_model_name='/kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=4, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)\r\n",
      "[2025-08-27 17:45:23] Using default HuggingFace chat template with detected content format: string\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316741.822985     113 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316741.823130     114 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1756316741.831935     113 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1756316741.832156     114 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0827 17:45:58.041000 113 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:45:58.041000 113 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "W0827 17:45:58.042000 114 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:45:58.042000 114 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316776.822787     325 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1756316776.831918     325 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0827 17:46:31.786000 325 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:46:31.786000 325 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "[2025-08-27 17:46:32] Launch DP0 starting at GPU #0.\r\n",
      "[2025-08-27 17:46:32] Launch DP1 starting at GPU #1.\r\n",
      "[2025-08-27 17:46:32] Launch DP2 starting at GPU #2.\r\n",
      "[2025-08-27 17:46:32] Launch DP3 starting at GPU #3.\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316810.465324     571 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316810.473665     575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1756316810.474210     571 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316810.474928     574 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1756316810.482635     575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1756316810.484172     574 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1756316810.512667     572 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1756316810.521697     572 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0827 17:47:05.779000 575 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:47:05.779000 575 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "W0827 17:47:05.779000 572 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:47:05.779000 572 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "W0827 17:47:05.779000 574 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:47:05.779000 574 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "W0827 17:47:05.786000 571 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \r\n",
      "W0827 17:47:05.786000 571 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\r\n",
      "[2025-08-27 17:47:07 DP2] Attention backend not explicitly specified. Use flashinfer backend by default.\r\n",
      "[2025-08-27 17:47:07 DP2] Init torch distributed begin.\r\n",
      "[2025-08-27 17:47:07 DP0] Attention backend not explicitly specified. Use flashinfer backend by default.\r\n",
      "[2025-08-27 17:47:07 DP0] Init torch distributed begin.\r\n",
      "[2025-08-27 17:47:07 DP3] Attention backend not explicitly specified. Use flashinfer backend by default.\r\n",
      "[2025-08-27 17:47:07 DP3] Init torch distributed begin.\r\n",
      "[2025-08-27 17:47:07 DP1] Attention backend not explicitly specified. Use flashinfer backend by default.\r\n",
      "[2025-08-27 17:47:07 DP1] Init torch distributed begin.\r\n",
      "[W827 17:47:15.784204977 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W827 17:47:15.784223614 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W827 17:47:15.784251999 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W827 17:47:15.784277694 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "Call stop_server() or full_cleanup() to shut it down gracefully.\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "  # Background server launcher for Kaggle with SGLang\n",
    "  import os, sys, time, subprocess, json, socket, requests\n",
    "\n",
    "  # ---------- 1) Check for existing server and cleanup ----------\n",
    "  PORT = 8080\n",
    "  HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "\n",
    "  # Check if server already running\n",
    "  try:\n",
    "      r = requests.get(HEALTH_URL, timeout=3)\n",
    "      if r.status_code == 200:\n",
    "          print(f\"Server already running on port {PORT}. Stopping it first...\")\n",
    "          # Kill existing sglang processes\n",
    "          subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "          time.sleep(3)  # Wait for cleanup\n",
    "  except:\n",
    "      pass  # No server running\n",
    "\n",
    "  # Clear CUDA memory before starting\n",
    "  try:\n",
    "      import torch\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.synchronize()\n",
    "          print(\"CUDA memory cleared.\")\n",
    "      num_gpus = torch.cuda.device_count()\n",
    "  except Exception:\n",
    "      num_gpus = 0\n",
    "      \n",
    "  model_path_to_use = str(MODEL_PATH)\n",
    "  print(f\"ðŸ”§ Using model from {model_path_to_use}\")\n",
    "\n",
    "  LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "  print(f\"LOG file path: {LOG}\")\n",
    "\n",
    "  SERVER_CMD = [\n",
    "      sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "      \"--host\", \"0.0.0.0\",\n",
    "      \"--port\", str(PORT),\n",
    "      \"--model-path\", model_path_to_use,\n",
    "      \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "      \"--kv-cache-dtype\", \"fp8_e4m3\",\n",
    "      \"--enable-metrics\"\n",
    "  ]\n",
    "\n",
    "  # ---------- 2) Launch in background ----------\n",
    "  log_f = open(LOG, \"w\")\n",
    "  env = os.environ.copy()\n",
    "  proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=SUBMIT_DIR)\n",
    "  print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "  print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "\n",
    "  # ---------- 3) Wait for readiness ----------\n",
    "  def wait_ready(url, timeout_s=180):\n",
    "      t0 = time.time()\n",
    "      while time.time() - t0 < timeout_s:\n",
    "          try:\n",
    "              r = requests.get(url, timeout=3)\n",
    "              if r.status_code == 200:\n",
    "                  return True\n",
    "          except Exception:\n",
    "              pass\n",
    "          time.sleep(2)\n",
    "      return False\n",
    "\n",
    "  ready = wait_ready(HEALTH_URL)\n",
    "  log_f.flush()\n",
    "\n",
    "  if ready:\n",
    "      print(f\"sglang is READY on port {PORT}.\")\n",
    "      print(f\"- Tail logs: !tail -n 50 {LOG}\")\n",
    "      print(f\"- List models: !curl -s http://127.0.0.1:{PORT}/v1/models | jq .\")\n",
    "  else:\n",
    "      print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "      log_f.close()\n",
    "      !tail -n 60 {LOG}\n",
    "\n",
    "  # ---------- 4) Cleanup functions ----------\n",
    "  def stop_server(p=proc):\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "      print(\"Server stopped.\")\n",
    "\n",
    "  def full_cleanup(p=proc):\n",
    "      # Stop server\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "\n",
    "      # Also kill any lingering sglang processes\n",
    "      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "      print(\"Server stopped and CUDA memory cleared.\")\n",
    "\n",
    "  print(\"Call stop_server() or full_cleanup() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96782fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:47:18.371854Z",
     "iopub.status.busy": "2025-08-27T17:47:18.371298Z",
     "iopub.status.idle": "2025-08-27T17:49:18.389167Z",
     "shell.execute_reply": "2025-08-27T17:49:18.388552Z"
    },
    "papermill": {
     "duration": 120.023761,
     "end_time": "2025-08-27T17:49:18.390359",
     "exception": false,
     "start_time": "2025-08-27T17:47:18.366598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âœ… Server is responding!\n",
      "Available models:\n",
      "  - /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "\n",
      "âœ… Found model: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "    import requests\n",
    "    import time\n",
    "    \n",
    "    def check_models():\n",
    "        url = \"http://127.0.0.1:8080/v1/models\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "    \n",
    "            print(\"âœ… Server is responding!\")\n",
    "            print(\"Available models:\")\n",
    "            for model in result['data']:\n",
    "                print(f\"  - {model['id']}\")\n",
    "    \n",
    "            return result['data'][0]['id'] if result['data'] else None\n",
    "    \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"âŒ Connection failed - server may not be ready yet\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Poll every 30 seconds until we get a model\n",
    "    model_name = None\n",
    "    while not model_name:\n",
    "        model_name = check_models()\n",
    "        if not model_name:\n",
    "            print(\"â³ Waiting 30 seconds before retrying...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    print(f\"\\nâœ… Found model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c9fbfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:49:18.400963Z",
     "iopub.status.busy": "2025-08-27T17:49:18.400157Z",
     "iopub.status.idle": "2025-08-27T17:49:18.407631Z",
     "shell.execute_reply": "2025-08-27T17:49:18.406994Z"
    },
    "papermill": {
     "duration": 0.013856,
     "end_time": "2025-08-27T17:49:18.408772",
     "exception": false,
     "start_time": "2025-08-27T17:49:18.394916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"âœ… Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\nâ± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"ðŸ”¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"âš¡ Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db25b734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:49:18.419808Z",
     "iopub.status.busy": "2025-08-27T17:49:18.419471Z",
     "iopub.status.idle": "2025-08-27T17:50:50.157845Z",
     "shell.execute_reply": "2025-08-27T17:50:50.157109Z"
    },
    "papermill": {
     "duration": 91.745667,
     "end_time": "2025-08-27T17:50:50.159202",
     "exception": false,
     "start_time": "2025-08-27T17:49:18.413535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Inference â†’ dev | attempts=8 | workers=64 | subset=test\n",
      "Running command: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset test --max_workers 64 --max_attempts 8 --model /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think --parquet-output-dir /kaggle/working\n",
      "Running: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset test --max_workers 64 --max_attempts 8 --model /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think --parquet-output-dir /kaggle/working\n",
      "\n",
      "â° Global timeout set to 60s via GLOBAL_TIMEOUT environment variable\r\n",
      "âš ï¸  WARNING: Using unrestricted executor - generated code will run directly on your system!\r\n",
      "â° API timeout: 600s (network safety only, no infrastructure timeouts)\r\n",
      "ðŸ—„ï¸ Sampled programs will be logged to /kaggle/working/20250827_174923__kaggle_input_arc-1-fake-ttt-blended-c802-dataset_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2025_test.parquet\r\n",
      "ðŸ“Š vLLM metrics monitoring enabled (http://127.0.0.1:8080/metrics)\r\n",
      "Loading subset: arc-prize-2025/test\r\n",
      "ðŸ” Validating 240 tasks...\r\n",
      "âœ… Task validation complete: 240 valid tasks\r\n",
      "ðŸ“ Tasks sorted by length (shortest to longest)\r\n",
      "\r\n",
      "Running 240 tasks from arc-prize-2025/test\r\n",
      "Model: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\r\n",
      "API: All Attempts Mode (8 attempts per task)\r\n",
      "Mode: True parallelization - 1920 total attempts\r\n",
      "Parallelization: ENABLED (64 workers)\r\n",
      "Scheduling: Batched (8 tasks Ã— 8 attempts = 64 workers used)\r\n",
      "\r\n",
      "âœ… No infrastructure timeouts - requests complete naturally to avoid GPU overload\r\n",
      "\r\n",
      "Sampling Parameters: {'max_tokens': 2000, 'temperature': 1.0, 'extra_body': {'min_p': 0.05, 'chat_template_kwargs': {'enable_thinking': False}}}\r\n",
      "Executor: unrestricted (timeout: 1s) âš ï¸  UNSAFE MODE\r\n",
      "--------------------------------------------------\r\n",
      "ðŸ“Š Starting metrics monitoring thread for http://127.0.0.1:8080/metrics\r\n",
      "ðŸ“Š Metrics thread started, polling http://127.0.0.1:8080/metrics every 30s\r\n",
      "ðŸ“Š [17:49:28] LLM metrics fetch failed\r\n",
      "\r\n",
      "ðŸ“ FIRST TASK PROMPT (00576224):\r\n",
      "================================================================================\r\n",
      "SYSTEM:\r\n",
      "You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by reasoning and generating Python code.\r\n",
      "\r\n",
      "USER:\r\n",
      "You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by generating Python code.\r\n",
      "Your goal is to analyze input-output grid pairs. The outputs were produced by applying a transformation rule to the inputs. Implement the transformation rules as a Python function.\r\n",
      "You should only write the implemented the transformation in code.\r\n",
      "You must write code in triple backticks (```python and then ```). You must write a function called 'transform' which takes a single argument, the input grid as 'list[list[int]]', and returns the transformed grid (also as 'list[list[int]]').\r\n",
      "You should make sure that you implement a version of the transformation which works in general (at least for all given input-output pairs and test input pairs).\r\n",
      "The number in the input grid can be mapped to the following colors: 0:Black; 1:Blue; 2:Red; 3:Green; 4:Yellow; 5:Grey; 6:Pink; 7:Orange; 8:Purple; 9:Brown\r\n",
      "Now, solve the following ARC-AGI task:\r\n",
      "# Task to solve:\r\n",
      "## Input 1 (grid shape: 2 by 2):\r\n",
      "[[7 9] [4 3]]\r\n",
      "## Output 1 (grid shape: 6 by 6):\r\n",
      "[[7 9 7 9 7 9] [4 3 4 3 4 3] [9 7 9 7 9 7] [3 4 3 4 3 4] [7 9 7 9 7 9] [4 3 4 3 4 3]]\r\n",
      "\r\n",
      "## Input 2 (grid shape: 2 by 2):\r\n",
      "[[8 6] [6 4]]\r\n",
      "## Output 2 (grid shape: 6 by 6):\r\n",
      "[[8 6 8 6 8 6] [6 4 6 4 6 4] [6 8 6 8 6 8] [4 6 4 6 4 6] [8 6 8 6 8 6] [6 4 6 4 6 4]]\r\n",
      "\r\n",
      "## Test Input 1 (grid shape: 2 by 2):\r\n",
      "[[3 2] [7 8]]\r\n",
      "\r\n",
      "================================================================================\r\n",
      "ðŸš€ Started 1920 attempts with 64 workers\r\n",
      "â³ No completions in last 15s â€” 0/1920 done; 1920 remaining (timeout in 60s)\r\n",
      "ðŸ“Š [17:49:58] LLM metrics fetch failed\r\n",
      "â³ No completions in last 15s â€” 0/1920 done; 1920 remaining (timeout in 45s)\r\n",
      "âœ… 3c9b0459: 8 attempts | 8 valid outputs | 5 train-perfect, 1 train-partial, 2 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 25ff71a9: 8 attempts | 8 valid outputs | 6 train-perfect, 2 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 25d8a9c8: 8 attempts | 7 valid outputs, 1 execution failures | 6 train-perfect, 1 train-incorrect (best: 100.0% train)\r\n",
      "â³ No completions in last 15s â€” 48/1920 done; 1872 remaining (timeout in 30s)\r\n",
      "âœ… 0520fde7: 8 attempts | 7 valid outputs, 1 execution failures | 1 train-perfect, 1 train-partial, 5 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 2dee498d: 8 attempts | 7 valid outputs, 1 execution failures | 5 train-perfect, 2 train-partial (best: 100.0% train)\r\n",
      "âœ… 0d3d703e: 8 attempts | 7 valid outputs, 1 invalid outputs | 3 train-perfect (of which 3 trans), 3 train-partial (of which 1 trans), 1 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 00576224: 8 attempts | 6 valid outputs, 2 execution failures | 3 train-perfect, 3 train-incorrect (best: 100.0% train)\r\n",
      "ðŸ¥ Health [100 attempts]: Success 92% | Timeout 0% | ExecErr 8% | AvgTime 19.71s\r\n",
      "âœ… 3af2c5a8: 8 attempts | 7 valid outputs, 1 invalid outputs | 2 train-perfect, 1 train-partial, 4 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 3618c87e: 8 attempts | 7 valid outputs, 1 execution failures | 4 train-perfect, 1 train-partial, 2 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 27a28665: 8 attempts | 6 valid outputs, 1 execution failures, 1 invalid outputs | 1 train-perfect, 4 train-partial, 1 train-incorrect (of which 1 trans) (best: 100.0% train)\r\n",
      "âœ… 1e0a9b12: 8 attempts | 6 valid outputs, 2 execution failures | 2 train-perfect, 4 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 2de01db2: 8 attempts | 6 valid outputs, 2 execution failures | 2 train-partial, 4 train-incorrect (best: 33.3% train)\r\n",
      "âœ… 2072aba6: 8 attempts | 7 valid outputs, 1 execution failures | 2 train-perfect, 5 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 0c786b71: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 017c7c7b: 8 attempts | 8 valid outputs | 1 train-perfect, 4 train-partial (of which 1 trans), 3 train-incorrect (best: 100.0% train)\r\n",
      "ðŸ“Š [17:50:28] LLM metrics fetch failed\r\n",
      "â³ No completions in last 15s â€” 145/1920 done; 1775 remaining (timeout in 15s)\r\n",
      "â° Global timeout reached (60s). Cancelling remaining attempts...\r\n",
      "â° Timeout: 145 attempts completed, 1711 cancelled, 64 already running\r\n",
      "â° Execution stopped after 60.0s due to global timeout\r\n",
      "ðŸ›‘ 1711 attempts were cancelled due to timeout\r\n",
      "ðŸ“Š Final status: 145 successful, 0 failed, 1711 cancelled\r\n",
      "âœ… 29c11459: 8 attempts | 5 valid outputs, 1 execution failures, 2 invalid outputs | 4 train-perfect, 1 train-partial (best: 100.0% train)\r\n",
      "âœ… 3ac3eb23: 8 attempts | 8 valid outputs | 8 train-perfect (best: 100.0% train)\r\n",
      "âœ… 234bbc79: 8 attempts | 5 valid outputs, 3 execution failures | 5 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 0692e18c: 8 attempts | 6 valid outputs, 2 execution failures | 6 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 17cae0c1: 8 attempts | 4 valid outputs, 4 execution failures | 4 train-incorrect (of which 1 trans) (best: 0.0% train)\r\n",
      "âœ… 27a77e38: 8 attempts | 8 valid outputs | 1 train-partial, 7 train-incorrect (best: 33.3% train)\r\n",
      "ðŸ¥ Health [200 attempts]: Success 87% | Timeout 0% | ExecErr 13% | AvgTime 11.55s\r\n",
      "âœ… 3979b1a8: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 3aa6fb7a: 8 attempts | 7 valid outputs, 1 execution failures | 3 train-perfect, 4 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 10fcaaa3: 8 attempts | 8 valid outputs | 2 train-partial, 6 train-incorrect (of which 1 trans) (best: 50.0% train)\r\n",
      "âš ï¸ Task 32e9702f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 05269061 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3b4c2228 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15696249 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 27f8ce4f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 30f42897 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0c9aba6e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2dc579da has no valid attempts - skipping\r\n",
      "âš ï¸ Task 08ed6ac7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 23581191 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 4258a5f9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3bd292e8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 12eac192 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 332efdb3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 195ba7dc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1478ab18 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 007bbfb7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 37ce87bb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 025d127b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1190bc91 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1bfc4729 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1caeab9d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2a28add5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 321b1fc6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 252143c9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1e5d6875 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 137eaa0f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3428a4f5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0ca9ddb6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1cf80156 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 150deff5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3f7978a0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 337b420f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a2e2828 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 12422b43 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 412b6263 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 281123b4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2601afb7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3d31c5b3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2a5f8217 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3eda0437 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 351d6448 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3bd67248 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 11e1fe23 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 178fcbfb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c0d0a4b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 03560426 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0becf7df has no valid attempts - skipping\r\n",
      "âš ï¸ Task 11852cab has no valid attempts - skipping\r\n",
      "âš ï¸ Task 137f0df0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 14b8e18c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1b60fb0c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f642eb9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f876c06 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2204b7a8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22168020 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22233c11 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2281f1f4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 228f6490 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22eb0ac0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 31aa019c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 31adaf00 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3906de3d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 292dd178 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0962bcdd has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25c199f5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 310f3251 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22a4bbc2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f0c79e5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3befdf3e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 05f2a901 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 12997ef3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 39a8645d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25e02866 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 363442ee has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22425bda has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2c737e39 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0bb8deee has no valid attempts - skipping\r\n",
      "âš ï¸ Task 21f83797 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 29623171 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 230f2e48 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 19bb5feb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2f767503 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2bcee788 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 140c817e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 20fb2937 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3de23699 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 41ace6b5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2bee17df has no valid attempts - skipping\r\n",
      "âš ï¸ Task 182e5d0f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3bdb4ada has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2697da3f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2b01abd0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3f23242b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 18447a8d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2753e76c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 342dd610 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3391f8c0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 278e5215 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1efba499 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 20818e16 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a244afd has no valid attempts - skipping\r\n",
      "âš ï¸ Task 09629e4f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 29700607 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0b17323b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 41e4d17e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25d487eb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 11dc524f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 36d67576 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 18286ef8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2ccd9fef has no valid attempts - skipping\r\n",
      "âš ï¸ Task 13f06aa5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 423a55dc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 272f95fa has no valid attempts - skipping\r\n",
      "âš ï¸ Task 17829a00 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1d61978c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3345333e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 396d80d7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3d588dc9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2faf500b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1acc24af has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2685904e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1b8318e3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 4093f84a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15663ba9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 253bf280 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c786137 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 4290ef0e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 00dbd492 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3d6c6e23 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 23b5c85d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 20981f0e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 28e73c20 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0e671a1a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3e980e27 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1990f7a8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 358ba94e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 103eff5b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0b148d64 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c02dbbe has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1e81d6f9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0a2355a6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1190e5a7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 17b80ad2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1b59e163 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3194b014 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2dd70a9a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2c608aff has no valid attempts - skipping\r\n",
      "âš ï¸ Task 17b866bd has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2f0c5170 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22806e14 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 332202d5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 342ae2ed has no valid attempts - skipping\r\n",
      "âš ï¸ Task 36fdfd69 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0e206a2e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15660dd6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1da012fc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1d398264 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 414297c0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 00d62c1b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0a938d79 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2546ccf6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c56ad9f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 13713586 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 070dd51e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a6449f1 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1e32b0e9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 32597951 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0f63c0b9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22208ba4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a07d186 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 37d3e8b2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0d87d2a6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 009d5c81 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3ee1011a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1be83260 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2037f2c7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 14754a24 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 212895b5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3a301edc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 33b52de3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f85a75f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2e65ae53 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 305b1341 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 319f2597 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 18419cfa has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2c0b0aff has no valid attempts - skipping\r\n",
      "âš ï¸ Task 33067df9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 045e512c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 34cfa167 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 184a9768 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0607ce86 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 42918530 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 06df4c85 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0a1d4ef5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15113be4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 256b0a75 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 320afe60 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 09c534e7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1d0a4b61 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25094a63 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3ad05f52 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3490cc26 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2b9ef948 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 39e1d7f9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 40f6cd08 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 05a7bcf2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 264363fd has no valid attempts - skipping\r\n",
      "\r\n",
      "==================================================\r\n",
      "SUBMIT MODE SUMMARY\r\n",
      "==================================================\r\n",
      "Dataset: arc-prize-2025\r\n",
      "Subset: test\r\n",
      "Model: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\r\n",
      "Total tasks processed: 32\r\n",
      "Total time: 60.0s\r\n",
      "Successful API calls: 32/32 (100.0%)\r\n",
      "Total tokens used: 199,602\r\n",
      "Total cost: $0.043780\r\n",
      "\r\n",
      "ðŸ“Š RESPONSE METRICS:\r\n",
      "  Total responses: 209\r\n",
      "  Code extracted: 209/209 (100.0%)\r\n",
      "  Max length responses: 0/209 (0.0%)\r\n",
      "  Timeout responses: 0/209 (0.0%)\r\n",
      "  API failure responses: 0/209 (0.0%)\r\n",
      "\r\n",
      "ðŸ“Š TRAIN METRICS:\r\n",
      "  All train correct: 19/32 (59.4%)\r\n",
      "  Min 1 train correct: 23/32 (71.9%)\r\n",
      "\r\n",
      "âš ï¸  Note: Test accuracy metrics unavailable in SUBMIT mode (no test outputs)\r\n",
      "\r\n",
      "ðŸŽ¯ To generate submission file, run:\r\n",
      "   uv run python llm_python/generate_submission.py --dataset arc-prize-2025 --subset test\r\n",
      "All sampled programs saved to /kaggle/working/20250827_174923__kaggle_input_arc-1-fake-ttt-blended-c802-dataset_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2025_test.parquet\r\n"
     ]
    }
   ],
   "source": [
    "if not IS_KAGGLE:\n",
    "    %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Use FIRST_ATTEMPTS and FIRST_WORKERS for initial inference\n",
    "MAX_ATTEMPTS = FIRST_ATTEMPTS if (IS_RERUN or not IS_KAGGLE) else min(FIRST_ATTEMPTS, 8)\n",
    "MAX_WORKERS  = FIRST_WORKERS\n",
    "\n",
    "SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# # can use this instead if testing evaluation during a pre-run\n",
    "# SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"First Inference â†’ {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "    \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--max_workers\", str(MAX_WORKERS),\n",
    "    \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "    \"--model\", model_name,\n",
    "    \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "    \"--unsafe-executor\",\n",
    "    \"--max-tokens\", \"2000\",\n",
    "    \"--qwen-no-think\"\n",
    "]\n",
    "\n",
    "\n",
    "# Add parquet output directory if set\n",
    "if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "  cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Handle output redirection properly\n",
    "if IS_RERUN or not IS_KAGGLE:\n",
    "    # For quiet mode, redirect to file using subprocess\n",
    "    import subprocess\n",
    "    log_file_path = f\"{SUBMIT_DIR}/run.log\"\n",
    "    print(f\"ðŸ“ Logging output to: {log_file_path}\")\n",
    "    \n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        print(\"â³ Running tasks (output being written to log file)...\")\n",
    "        return_code = process.wait()\n",
    "        \n",
    "    if return_code == 0:\n",
    "        print(f\"âœ… Task runner completed successfully. Check {log_file_path} for details.\")\n",
    "    else:\n",
    "        print(f\"âŒ Task runner failed with return code {return_code}\")\n",
    "        print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "        # Show last few lines of log\n",
    "        !tail -n 20 {log_file_path}\n",
    "else:\n",
    "    # For interactive mode, show output directly\n",
    "    cmd = \" \".join(cmd_args)\n",
    "    print(f\"Running: {cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa7721e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:50:50.173532Z",
     "iopub.status.busy": "2025-08-27T17:50:50.173040Z",
     "iopub.status.idle": "2025-08-27T17:50:50.176772Z",
     "shell.execute_reply": "2025-08-27T17:50:50.176265Z"
    },
    "papermill": {
     "duration": 0.011614,
     "end_time": "2025-08-27T17:50:50.177671",
     "exception": false,
     "start_time": "2025-08-27T17:50:50.166057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH type: <class 'pathlib.PosixPath'>, value: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n"
     ]
    }
   ],
   "source": [
    "print(f\"MODEL_PATH type: {type(MODEL_PATH)}, value: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d53588b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:50:50.191302Z",
     "iopub.status.busy": "2025-08-27T17:50:50.191102Z",
     "iopub.status.idle": "2025-08-27T18:04:36.258157Z",
     "shell.execute_reply": "2025-08-27T18:04:36.257558Z"
    },
    "papermill": {
     "duration": 826.084966,
     "end_time": "2025-08-27T18:04:36.269001",
     "exception": false,
     "start_time": "2025-08-27T17:50:50.184035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Fine-tuning enabled - TTT mode detected, will fine-tune model on non-transductive programs\n",
      "ðŸ“¤ Hub push setting: DISABLED (Kaggle=True)\n",
      "ðŸŽ¯ Using base model for fine-tuning: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "ðŸ› ï¸ Fine-tuning configuration:\n",
      "   MODEL_SLUG: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "   FINE_TUNING_MODE: final_only\n",
      "   DATA_SOURCE: parquet\n",
      "   ARC_PROGRAMS_PARQUET: /kaggle/working\n",
      "   MODEL_SAVE_DIR: /kaggle/working\n",
      "   PUSH_TO_HUB: false\n",
      "ðŸ›‘ Stopping inference server to free GPU memory for fine-tuning...\n",
      "âœ… CUDA memory cleared\n",
      "ðŸ“ Logging to: /kaggle/working/fine_tuning_20250827_175051.log\n",
      "ðŸš€ Starting fine-tuning...\n",
      "Running command: uv run python -u -m llm_python.fine-tuning.unsloth_arc_finetuning_soar --config llm_python/fine-tuning/config.yaml\n",
      "Config file llm_python/fine-tuning/config.yaml not found! Using default values.\n",
      "ðŸ“Š Data source: parquet (/kaggle/working)\n",
      "Config loaded:\n",
      "  config_path: llm_python/fine-tuning/config.yaml\n",
      "  test_run: False\n",
      "  execution_mode: final_only\n",
      "  data_source: parquet\n",
      "  model_slug: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "  batch_size_global: 1\n",
      "  max_rows: None\n",
      "  model_save_dir: /kaggle/working\n",
      "  parquet_path: /kaggle/working\n",
      "--------------------------------------------------\n",
      "Report to: none\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2025-08-27 17:51:12.367236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756317072.390605    3828 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756317072.398261    3828 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.9: Fast Qwen3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.278 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.49s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.22s/it]\n",
      "model.max_seq_length: 32768\n",
      "Unsloth 2025.8.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n",
      "tokenizer.padding_side: right\n",
      "âœ… Utils imported and initialized successfully\n",
      "âœ… Added code cleaning and filtering functions\n",
      "ORIGINAL CODE:\n",
      "'def solve(grid):\\n  # First comment\\n\\n\\n  # Second comment after multiple empty lines\\n  rows = len(grid)\\n  cols = len(grid[0])\\n  \\n  # Another comment\\n    \\n    \\n  return grid'\n",
      "\n",
      "ORIGINAL CODE (formatted):\n",
      "def solve(grid):\n",
      "  # First comment\n",
      "\n",
      "\n",
      "  # Second comment after multiple empty lines\n",
      "  rows = len(grid)\n",
      "  cols = len(grid[0])\n",
      "  \n",
      "  # Another comment\n",
      "    \n",
      "    \n",
      "  return grid\n",
      "\n",
      "==================================================\n",
      "CLEANED CODE:\n",
      "'def solve(grid):\\n  # First comment\\n\\n  # Second comment after multiple empty lines\\n  rows = len(grid)\\n  cols = len(grid[0])\\n\\n  # Another comment\\n\\n  return grid'\n",
      "\n",
      "CLEANED CODE (formatted):\n",
      "def solve(grid):\n",
      "  # First comment\n",
      "\n",
      "  # Second comment after multiple empty lines\n",
      "  rows = len(grid)\n",
      "  cols = len(grid[0])\n",
      "\n",
      "  # Another comment\n",
      "\n",
      "  return grid\n",
      "\n",
      "==================================================\n",
      "CHANGES SUMMARY:\n",
      "Original length: 170 chars\n",
      "Cleaned length: 158 chars\n",
      "Characters removed: 12\n",
      "âœ… Using SOAR prompts from utils:\n",
      "   System prompt: 129 chars\n",
      "   Initial turn prompt: 990 chars\n",
      "ðŸ“Š Loading programs from parquet: /kaggle/working/20250827_174923__kaggle_input_arc-1-fake-ttt-blended-c802-dataset_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2025_test.parquet\n",
      "âœ… Loaded 178 rows from parquet\n",
      "ðŸ” Filtered to 168 non-transductive programs (removed 10 transductive)\n",
      "ðŸ” Selected 80 programs with â‰¥1 train correct + 71 shortest incorrect as fallback = 151 total\n",
      "ðŸ“¦ Prepared 151 program records for fine-tuning\n",
      "ðŸ“ˆ Success rates - Train: 52.98%, Test: 0.00%\n",
      "ðŸ¤– Models in dataset: /kaggle/input/arc-1-fake-ttt-blended-c802-dataset/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "\n",
      "build parquet chat fields:   0%|          | 0/151 [00:00<?, ? examples/s]\n",
      "build parquet chat fields: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:00<00:00, 1790.48 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/151 [00:00<?, ? examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:00<00:00, 2433.00 examples/s]\n",
      "      train:  min= 652  median= 971  max=1665\n",
      "ðŸ§ª Running Pre-Training Data Integrity Tests\n",
      "ðŸ“Š Testing 32 random examples from train split\n",
      "============================================================\n",
      "\n",
      "ðŸ” Testing 32 examples...\n",
      "\n",
      "[1/32] Testing 0520fde7\n",
      "  âœ… Train: 1/3 (33.3%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[2/32] Testing 3c9b0459\n",
      "  âœ… Train: 4/4 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[3/32] Testing 10fcaaa3\n",
      "  âœ… Train: 2/4 (50.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[4/32] Testing 3ac3eb23\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[5/32] Testing 29c11459\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[6/32] Testing 2de01db2\n",
      "  âœ… Train: 1/3 (33.3%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[7/32] Testing 27a28665\n",
      "  âœ… Train: 3/7 (42.9%)\n",
      "  âœ… Test:  0/3 (0.0%)\n",
      "\n",
      "[8/32] Testing 234bbc79\n",
      "  âœ… Train: 0/4 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[9/32] Testing 00576224\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[10/32] Testing 00576224\n",
      "  âœ… Train: 0/2 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[11/32] Testing 3c9b0459\n",
      "  âœ… Train: 4/4 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[12/32] Testing 25ff71a9\n",
      "  âœ… Train: 4/4 (100.0%)\n",
      "  âœ… Test:  2/2 (100.0%)\n",
      "\n",
      "[13/32] Testing 3618c87e\n",
      "  âœ… Train: 1/3 (33.3%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[14/32] Testing 3ac3eb23\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[15/32] Testing 29c11459\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[16/32] Testing 3979b1a8\n",
      "  âœ… Train: 0/2 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[17/32] Testing 3cd86f4f\n",
      "  âœ… Train: 0/3 (0.0%)\n",
      "  âœ… Test:  0/3 (0.0%)\n",
      "\n",
      "[18/32] Testing 017c7c7b\n",
      "  âœ… Train: 1/3 (33.3%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[19/32] Testing 2de01db2\n",
      "  âœ… Train: 0/3 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[20/32] Testing 29c11459\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[21/32] Testing 1e0a9b12\n",
      "  âœ… Train: 0/3 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[22/32] Testing 3aa6fb7a\n",
      "  âœ… Train: 2/2 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[23/32] Testing 25ff71a9\n",
      "  âœ… Train: 4/4 (100.0%)\n",
      "  âœ… Test:  2/2 (100.0%)\n",
      "\n",
      "[24/32] Testing 2de01db2\n",
      "  âœ… Train: 1/3 (33.3%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[25/32] Testing 0520fde7\n",
      "  âœ… Train: 0/3 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[26/32] Testing 017c7c7b\n",
      "  âœ… Train: 1/3 (33.3%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[27/32] Testing 3618c87e\n",
      "  âœ… Train: 3/3 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[28/32] Testing 34b99a2b\n",
      "  âœ… Train: 0/4 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[29/32] Testing 0d3d703e\n",
      "  âœ… Train: 1/4 (25.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[30/32] Testing 25d8a9c8\n",
      "  âœ… Train: 4/4 (100.0%)\n",
      "  âœ… Test:  1/1 (100.0%)\n",
      "\n",
      "[31/32] Testing 3618c87e\n",
      "  âœ… Train: 0/3 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "[32/32] Testing 0692e18c\n",
      "  âœ… Train: 0/3 (0.0%)\n",
      "  âœ… Test:  0/1 (0.0%)\n",
      "\n",
      "============================================================\n",
      "ðŸ“ˆ PRE-TRAINING DATA INTEGRITY RESULTS ANALYSIS\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ OVERALL PERFORMANCE:\n",
      "   Examples tested: 32\n",
      "   Code executable: 32/32 (100.0%)\n",
      "   Examples with errors: 0/32 (0.0%)\n",
      "\n",
      "ðŸ“Š TRAINING GRIDS PERFORMANCE:\n",
      "   Average success rate: 50.6%\n",
      "   Perfect examples (100%): 13/32 (40.6%)\n",
      "   Partial examples (>0% <100%): 9/32 (28.1%)\n",
      "   Failed examples (0%): 10/32 (31.2%)\n",
      "   Grid-level accuracy: 49/100 (49.0%)\n",
      "\n",
      "ðŸŽ¯ TEST GRIDS PERFORMANCE:\n",
      "   Average success rate: 37.5%\n",
      "   Perfect examples (100%): 12/32 (37.5%)\n",
      "   Partial examples (>0% <100%): 0/32 (0.0%)\n",
      "   Failed examples (0%): 20/32 (62.5%)\n",
      "   Grid-level accuracy: 14/38 (36.8%)\n",
      "\n",
      "ðŸ“‹ DETAILED BREAKDOWN BY EXAMPLE:\n",
      "------------------------------------------------------------\n",
      "[ 1] 0520fde7\n",
      "     Train: 33.3% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[ 2] 3c9b0459\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[ 3] 10fcaaa3\n",
      "     Train: 50.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[ 4] 3ac3eb23\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[ 5] 29c11459\n",
      "     Train: 100.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[ 6] 2de01db2\n",
      "     Train: 33.3% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[ 7] 27a28665\n",
      "     Train: 42.9% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[ 8] 234bbc79\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[ 9] 00576224\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[10] 00576224\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[11] 3c9b0459\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[12] 25ff71a9\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[13] 3618c87e\n",
      "     Train: 33.3% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[14] 3ac3eb23\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[15] 29c11459\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[16] 3979b1a8\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[17] 3cd86f4f\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[18] 017c7c7b\n",
      "     Train: 33.3% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[19] 2de01db2\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[20] 29c11459\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[21] 1e0a9b12\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[22] 3aa6fb7a\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[23] 25ff71a9\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[24] 2de01db2\n",
      "     Train: 33.3% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[25] 0520fde7\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[26] 017c7c7b\n",
      "     Train: 33.3% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[27] 3618c87e\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[28] 34b99a2b\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[29] 0d3d703e\n",
      "     Train: 25.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[30] 25d8a9c8\n",
      "     Train: 100.0% | Test: 100.0% | Executed: âœ… | Errors: 0\n",
      "[31] 3618c87e\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "[32] 0692e18c\n",
      "     Train:  0.0% | Test:  0.0% | Executed: âœ… | Errors: 0\n",
      "\n",
      "ðŸ” DATASET QUALITY ASSESSMENT:\n",
      "------------------------------------------------------------\n",
      "âš ï¸  MODERATE: Ground-truth code has mixed performance on training examples\n",
      "âŒ POOR: Ground-truth code has poor generalization to test examples\n",
      "âœ… EXCELLENT: All ground-truth code is executable\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ” FAILING EXAMPLES SUMMARY:\n",
      "Found 20 examples with issues: [0, 2, 4, 5, 6, 7, 9, 12, 15, 16, 17, 18, 20, 23, 24, 25, 27, 28, 30, 31]\n",
      "To examine a specific failure, run: examine_failure(data_integrity_results, index)\n",
      "Example: examine_failure(data_integrity_results, 0)\n",
      "\n",
      "âœ… Pre-training data integrity tests complete!\n",
      "ðŸ“‹ Summary stats saved in 'summary_stats' variable\n",
      "ðŸ“Š Detailed results saved in 'data_integrity_results' variable\n",
      "\n",
      "ðŸ” DETAILED EXAMINATION: Example 22\n",
      "Task ID: 3aa6fb7a\n",
      "Dataset Index: 71\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ GROUND TRUTH CODE:\n",
      "------------------------------\n",
      "def transform(grid_lst: list[list[int]]) -> list[list[int]]:\n",
      "    grid = [row[:] for row in grid_lst]\n",
      "    blue_color = 1\n",
      "    purple_color = 8\n",
      "    for i in range(len(grid)):\n",
      "        for j in range(len(grid[0])):\n",
      "            if grid[i][j] == 0:\n",
      "                neighbors = []\n",
      "                if i > 0:\n",
      "                    neighbors.append(grid[i - 1][j])\n",
      "                if i < len(grid) - 1:\n",
      "                    neighbors.append(grid[i + 1][j])\n",
      "                if j > 0:\n",
      "                    neighbors.append(grid[i][j - 1])\n",
      "                if j < len(grid[0]) - 1:\n",
      "                    neighbors.append(grid[i][j + 1])\n",
      "                if neighbors.count(purple_color) >= 2:\n",
      "                    grid[i][j] = blue_color\n",
      "    return grid\n",
      "\n",
      "ðŸ“Š EXECUTION SUMMARY:\n",
      "Code executed successfully: True\n",
      "Train success rate: 100.0%\n",
      "Test success rate: 100.0%\n",
      "Number of errors: 0\n",
      "\n",
      "ðŸ‹ï¸ TRAINING EXAMPLES:\n",
      "==================================================\n",
      "\n",
      "Train Example 1: âœ… CORRECT\n",
      "----------------------------------------\n",
      "\n",
      "Input (from dataset):\n",
      "   0  0  0  0  8  8  0\n",
      "   0  0  0  0  0  8  0\n",
      "   0  0  8  0  0  0  0\n",
      "   0  0  8  8  0  0  0\n",
      "   0  0  0  0  0  0  0\n",
      "   0  0  0  0  8  0  0\n",
      "   0  0  0  8  8  0  0\n",
      "\n",
      "Expected (from dataset):\n",
      "   0  0  0  0  8  8  0\n",
      "   0  0  0  0  1  8  0\n",
      "   0  0  8  1  0  0  0\n",
      "   0  0  8  8  0  0  0\n",
      "   0  0  0  0  0  0  0\n",
      "   0  0  0  1  8  0  0\n",
      "   0  0  0  8  8  0  0\n",
      "\n",
      "Predicted (from code):\n",
      "   0  0  0  0  8  8  0\n",
      "   0  0  0  0  1  8  0\n",
      "   0  0  8  1  0  0  0\n",
      "   0  0  8  8  0  0  0\n",
      "   0  0  0  0  0  0  0\n",
      "   0  0  0  1  8  0  0\n",
      "   0  0  0  8  8  0  0\n",
      "\n",
      "Train Example 2: âœ… CORRECT\n",
      "----------------------------------------\n",
      "\n",
      "Input (from dataset):\n",
      "   0  0  0  0  0  0  0\n",
      "   0  8  0  0  0  0  0\n",
      "   0  8  8  0  0  0  0\n",
      "   0  0  0  0  8  8  0\n",
      "   0  0  0  0  0  8  0\n",
      "   0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0\n",
      "\n",
      "Expected (from dataset):\n",
      "   0  0  0  0  0  0  0\n",
      "   0  8  1  0  0  0  0\n",
      "   0  8  8  0  0  0  0\n",
      "   0  0  0  0  8  8  0\n",
      "   0  0  0  0  1  8  0\n",
      "   0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0\n",
      "\n",
      "Predicted (from code):\n",
      "   0  0  0  0  0  0  0\n",
      "   0  8  1  0  0  0  0\n",
      "   0  8  8  0  0  0  0\n",
      "   0  0  0  0  8  8  0\n",
      "   0  0  0  0  1  8  0\n",
      "   0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0\n",
      "\n",
      "ðŸ§ª TEST EXAMPLES:\n",
      "==================================================\n",
      "\n",
      "Test Example 1: âœ… CORRECT\n",
      "----------------------------------------\n",
      "\n",
      "Input (from dataset):\n",
      "   0  0  0  0  0  8  8\n",
      "   8  8  0  0  0  0  8\n",
      "   8  0  0  0  0  0  0\n",
      "   0  0  0  8  0  0  0\n",
      "   0  0  0  8  8  0  0\n",
      "   0  8  0  0  0  0  0\n",
      "   8  8  0  0  0  0  0\n",
      "\n",
      "Expected (from dataset):\n",
      "   0  0  0  0  0  8  8\n",
      "   8  8  0  0  0  1  8\n",
      "   8  1  0  0  0  0  0\n",
      "   0  0  0  8  1  0  0\n",
      "   0  0  0  8  8  0  0\n",
      "   1  8  0  0  0  0  0\n",
      "   8  8  0  0  0  0  0\n",
      "\n",
      "Predicted (from code):\n",
      "   0  0  0  0  0  8  8\n",
      "   8  8  0  0  0  1  8\n",
      "   8  1  0  0  0  0  0\n",
      "   0  0  0  8  1  0  0\n",
      "   0  0  0  8  8  0  0\n",
      "   1  8  0  0  0  0  0\n",
      "   8  8  0  0  0  0  0\n",
      "Extract training set date and time as dataset identifiers\n",
      "Date: 20250827 (YYYYMMDD)\n",
      "Time: 175605 (HHMMSS)\n",
      "Run name will be Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs\n",
      "Response tag selected: <|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "GPUs used: 1, per_device: 1, grad_accum: 32\n",
      "\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/151 [00:00<?, ? examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/151 [00:01<00:00, 75.45 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:01<00:00, 151.67 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:01<00:00, 114.83 examples/s]\n",
      "[setup] updates/epoch=5 total_updates=10 save_steps(raw)=0.5 effective_save_interval(steps)=5 output_dir=trainer_output\n",
      "GPU = NVIDIA L4. Max memory = 22.278 GB.\n",
      "8.719 GB of memory reserved.\n",
      "Parameter 'function'=<function train_on_responses_only.<locals>._train_on_responses_only at 0x7e5d502093a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "\n",
      "Map (num_proc=48):   0%|          | 0/151 [00:00<?, ? examples/s]\n",
      "Map (num_proc=48):   3%|â–Ž         | 4/151 [00:00<00:05, 28.98 examples/s]\n",
      "Map (num_proc=48):   8%|â–Š         | 12/151 [00:00<00:02, 51.03 examples/s]\n",
      "Map (num_proc=48):  13%|â–ˆâ–Ž        | 20/151 [00:00<00:02, 58.42 examples/s]\n",
      "Map (num_proc=48):  21%|â–ˆâ–ˆ        | 31/151 [00:00<00:01, 70.43 examples/s]\n",
      "Map (num_proc=48):  26%|â–ˆâ–ˆâ–‹       | 40/151 [00:00<00:01, 65.67 examples/s]\n",
      "Map (num_proc=48):  32%|â–ˆâ–ˆâ–ˆâ–      | 49/151 [00:00<00:01, 68.44 examples/s]\n",
      "Map (num_proc=48):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/151 [00:00<00:01, 70.41 examples/s]\n",
      "Map (num_proc=48):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/151 [00:01<00:01, 71.86 examples/s]\n",
      "Map (num_proc=48):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/151 [00:01<00:01, 69.02 examples/s]\n",
      "Map (num_proc=48):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/151 [00:01<00:00, 69.15 examples/s]\n",
      "Map (num_proc=48):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/151 [00:01<00:00, 68.90 examples/s]\n",
      "Map (num_proc=48):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 109/151 [00:01<00:00, 68.70 examples/s]\n",
      "Map (num_proc=48):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/151 [00:01<00:00, 68.35 examples/s]\n",
      "Map (num_proc=48):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 130/151 [00:01<00:00, 68.16 examples/s]\n",
      "Map (num_proc=48):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/151 [00:02<00:00, 67.29 examples/s]\n",
      "Map (num_proc=48): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:02<00:00, 64.62 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 151 | Num Epochs = 2 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 32\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 32 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 264,241,152 of 4,286,709,248 (6.16% trained)\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "\n",
      " 10%|â–ˆ         | 1/10 [00:33<04:58, 33.12s/it]\n",
      "                                              \n",
      "{'loss': 0.1287, 'grad_norm': 0.4818349778652191, 'learning_rate': 0.0001, 'epoch': 0.21}\n",
      "\n",
      " 10%|â–ˆ         | 1/10 [00:33<04:58, 33.12s/it]\n",
      " 20%|â–ˆâ–ˆ        | 2/10 [00:57<03:45, 28.20s/it]\n",
      "                                              \n",
      "{'loss': 0.1528, 'grad_norm': 0.8135301470756531, 'learning_rate': 0.0001, 'epoch': 0.42}\n",
      "\n",
      " 20%|â–ˆâ–ˆ        | 2/10 [00:57<03:45, 28.20s/it]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:22<03:06, 26.59s/it]\n",
      "                                              \n",
      "{'loss': 0.1962, 'grad_norm': 0.8418629169464111, 'learning_rate': 0.0001, 'epoch': 0.64}\n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:22<03:06, 26.59s/it]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:44<02:28, 24.78s/it]\n",
      "                                              \n",
      "{'loss': 0.1839, 'grad_norm': 0.8021779656410217, 'learning_rate': 0.0001, 'epoch': 0.85}\n",
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:44<02:28, 24.78s/it]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [02:02<01:51, 22.27s/it]\n",
      "                                              \n",
      "{'loss': 0.1838, 'grad_norm': 0.7474603652954102, 'learning_rate': 9.999999999999997e-06, 'epoch': 1.0}\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [02:02<01:51, 22.27s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [02:29<01:36, 24.01s/it]\n",
      "                                              \n",
      "{'loss': 0.0938, 'grad_norm': 0.39161416888237, 'learning_rate': 0.0001, 'epoch': 1.21}\n",
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [02:29<01:36, 24.01s/it]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:54<01:12, 24.15s/it]\n",
      "                                              \n",
      "{'loss': 0.0917, 'grad_norm': 0.4002571403980255, 'learning_rate': 0.0001, 'epoch': 1.42}\n",
      "\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:54<01:12, 24.15s/it]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [03:17<00:47, 23.97s/it]\n",
      "                                              \n",
      "{'loss': 0.0738, 'grad_norm': 0.33257436752319336, 'learning_rate': 0.0001, 'epoch': 1.64}\n",
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [03:17<00:47, 23.97s/it]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [03:40<00:23, 23.66s/it]\n",
      "                                              \n",
      "{'loss': 0.0929, 'grad_norm': 0.46856579184532166, 'learning_rate': 0.0001, 'epoch': 1.85}\n",
      "\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [03:40<00:23, 23.66s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:58<00:00, 21.96s/it]\n",
      "                                               \n",
      "{'loss': 0.072, 'grad_norm': 0.4189000427722931, 'learning_rate': 9.999999999999997e-06, 'epoch': 2.0}\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:58<00:00, 21.96s/it]\n",
      "                                               \n",
      "{'train_runtime': 241.5134, 'train_samples_per_second': 1.25, 'train_steps_per_second': 0.041, 'train_loss': 0.12693835273385048, 'epoch': 2.0}\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:01<00:00, 21.96s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:01<00:00, 24.15s/it]\n",
      "241.5134 seconds used for training.\n",
      "4.03 minutes used for training.\n",
      "Peak reserved memory = 11.189 GB.\n",
      "Peak reserved memory for training = 2.47 GB.\n",
      "Peak reserved memory % of max memory = 50.224 %.\n",
      "Peak reserved memory for training % of max memory = 11.087 %.\n",
      "TrainOutput(global_step=10, training_loss=0.12693835273385048, metrics={'train_runtime': 241.5134, 'train_samples_per_second': 1.25, 'train_steps_per_second': 0.041, 'total_flos': 7255760919386112.0, 'train_loss': 0.12693835273385048})\n",
      "ðŸ”§ Final-only mode: Only processing the last checkpoint\n",
      "ðŸ“¦ Processing final checkpoint: 10\n",
      "==((====))==  Unsloth 2025.8.9: Fast Qwen3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.278 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.49s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.22s/it]\n",
      "ðŸ’¾ Saving final model to: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\n",
      "âœ… Final model saved successfully\n",
      "\n",
      "âœ… Checkpoint processing complete for final_only mode\n",
      "âœ… Fine-tuning completed successfully!\n",
      "ðŸŽ¯ Fine-tuned model saved at: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\n",
      "ðŸ”„ Set FINE_TUNED_MODEL_PATH: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\n",
      "ðŸ“„ Full logs saved to: /kaggle/working/fine_tuning_20250827_175051.log\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Integration - TTT Mode Only\n",
    "# Only runs when TTT_MODE=true\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable fine-tuning only in TTT mode\n",
    "ENABLE_FINE_TUNING = TTT_MODE\n",
    "\n",
    "if ENABLE_FINE_TUNING:\n",
    "    print(\"ðŸ”¬ Fine-tuning enabled - TTT mode detected, will fine-tune model on non-transductive programs\")\n",
    "    \n",
    "    # Hub push control: Kaggle=false, Non-Kaggle=true\n",
    "    PUSH_TO_HUB = not IS_KAGGLE\n",
    "    print(f\"ðŸ“¤ Hub push setting: {'ENABLED' if PUSH_TO_HUB else 'DISABLED'} (Kaggle={IS_KAGGLE})\")\n",
    "    \n",
    "    # Use FINETUNE_BASE_PATH for the base model\n",
    "    print(f\"ðŸŽ¯ Using base model for fine-tuning: {FINETUNE_BASE_PATH}\")\n",
    "    \n",
    "    # Set environment variables for fine-tuning\n",
    "    fine_tuning_env = {\n",
    "        'MODEL_SLUG': str(FINETUNE_BASE_PATH),  # Use the fine-tuning base model\n",
    "        'FINE_TUNING_MODE': 'final_only',     # TTT mode uses final_only\n",
    "        'DATA_SOURCE': 'parquet',             # Load from parquet files\n",
    "        'ARC_PROGRAMS_PARQUET': str(ARC_PROGRAMS_PARQUET),  # Parquet directory path\n",
    "        'MODEL_SAVE_DIR': str(MODEL_SAVE_DIR), # Where to save fine-tuned model\n",
    "        'PUSH_TO_HUB': str(PUSH_TO_HUB).lower(),  # Hub push control\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ› ï¸ Fine-tuning configuration:\")\n",
    "    for key, value in fine_tuning_env.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "        os.environ[key] = value\n",
    "    \n",
    "    # Stop the current server to free up GPU memory\n",
    "    if 'proc' in locals():\n",
    "        print(\"ðŸ›‘ Stopping inference server to free GPU memory for fine-tuning...\")\n",
    "        try:\n",
    "            proc.terminate()\n",
    "            proc.wait(timeout=10)\n",
    "        except:\n",
    "            proc.kill()\n",
    "        \n",
    "        # Clear CUDA memory\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                print(\"âœ… CUDA memory cleared\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Ensure we're in the right directory\n",
    "    original_cwd = os.getcwd()\n",
    "    if not IS_KAGGLE:\n",
    "      os.chdir(\"/workspace/arc-agi-2025\")\n",
    "    \n",
    "    # Set up logging\n",
    "    import datetime\n",
    "    log_dir = Path(os.environ.get(\"SUBMIT_DIR\", \"logs\"))\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = log_dir / f\"fine_tuning_{timestamp}.log\"\n",
    "    \n",
    "    def log_and_print(message, log_file_handle=None):\n",
    "      \"\"\"Write to both console and log file\"\"\"\n",
    "      print(message)\n",
    "      if log_file_handle:\n",
    "          log_file_handle.write(message + \"\\n\")\n",
    "          log_file_handle.flush()\n",
    "    \n",
    "    try:\n",
    "      with open(log_file, 'w') as f:\n",
    "          log_and_print(f\"ðŸ“ Logging to: {log_file}\", f)\n",
    "    \n",
    "          if not IS_KAGGLE:\n",
    "              # Step 1: Convert notebook to script\n",
    "              log_and_print(\"ðŸ”„ Converting notebook to script...\", f)\n",
    "              convert_cmd = [\n",
    "                  \"uv\", \"run\", \"python\",\n",
    "                  \"llm_python/fine-tuning/notebook_to_script.py\",\n",
    "                  \"llm_python/fine-tuning/unsloth_arc_finetuning_soar.ipynb\"\n",
    "              ]\n",
    "    \n",
    "              convert_result = subprocess.run(convert_cmd,\n",
    "                                            capture_output=True,\n",
    "                                            text=True,\n",
    "                                            timeout=60)\n",
    "    \n",
    "              # Log full output\n",
    "              f.write(\"=== CONVERSION OUTPUT ===\\n\")\n",
    "              f.write(f\"Return code: {convert_result.returncode}\\n\")\n",
    "              f.write(f\"STDOUT:\\n{convert_result.stdout}\\n\")\n",
    "              f.write(f\"STDERR:\\n{convert_result.stderr}\\n\")\n",
    "              f.write(\"========================\\n\\n\")\n",
    "              f.flush()\n",
    "    \n",
    "              if convert_result.returncode != 0:\n",
    "                  log_and_print(f\"âŒ Notebook conversion failed: {convert_result.stderr}\", f)\n",
    "                  raise Exception(\"Notebook conversion failed\")\n",
    "    \n",
    "              log_and_print(\"âœ… Notebook converted successfully\", f)\n",
    "    \n",
    "          # Step 2: Run the actual fine-tuning\n",
    "          log_and_print(\"ðŸš€ Starting fine-tuning...\", f)\n",
    "          fine_tuning_cmd = [\n",
    "                \"uv\", \"run\", \"python\", \"-u\", \"-m\", \n",
    "                \"llm_python.fine-tuning.unsloth_arc_finetuning_soar\",\n",
    "                \"--config\", \"llm_python/fine-tuning/config.yaml\"\n",
    "          ]\n",
    "    \n",
    "          log_and_print(f\"Running command: {' '.join(fine_tuning_cmd)}\", f)\n",
    "    \n",
    "          # Run with real-time output\n",
    "          process = subprocess.Popen(\n",
    "              fine_tuning_cmd,\n",
    "              stdout=subprocess.PIPE,\n",
    "              stderr=subprocess.STDOUT,\n",
    "              text=True,\n",
    "              bufsize=1\n",
    "          )\n",
    "    \n",
    "          f.write(\"=== FINE-TUNING OUTPUT ===\\n\")\n",
    "          f.flush()\n",
    "    \n",
    "          # Stream output to both console and file\n",
    "          for line in process.stdout:\n",
    "              print(line, end='')  # Show in console\n",
    "              f.write(line)  # Save to file\n",
    "              f.flush()\n",
    "    \n",
    "          # Wait for completion\n",
    "          return_code = process.wait(timeout=6400)  # 2 hour timeout\n",
    "    \n",
    "          f.write(f\"\\n=== PROCESS COMPLETED WITH CODE: {return_code} ===\\n\")\n",
    "          f.flush()\n",
    "    \n",
    "          if return_code == 0:\n",
    "              log_and_print(\"âœ… Fine-tuning completed successfully!\", f)\n",
    "    \n",
    "              # Find the fine-tuned model\n",
    "              fine_tuned_models = list(Path(MODEL_SAVE_DIR).glob(\"*-final\"))\n",
    "              if fine_tuned_models:\n",
    "                  new_model_path = fine_tuned_models[0]\n",
    "                  log_and_print(f\"ðŸŽ¯ Fine-tuned model saved at: {new_model_path}\", f)\n",
    "    \n",
    "                  # Set the fine-tuned model path (don't overwrite original MODEL_PATH)\n",
    "                  FINE_TUNED_MODEL_PATH = str(new_model_path)\n",
    "                  log_and_print(f\"ðŸ”„ Set FINE_TUNED_MODEL_PATH: {FINE_TUNED_MODEL_PATH}\", f)\n",
    "              else:\n",
    "                  log_and_print(\"âš ï¸  Fine-tuned model not found, will use original model\", f)\n",
    "          else:\n",
    "              log_and_print(f\"âŒ Fine-tuning failed with return code {return_code}\", f)\n",
    "              log_and_print(\"ðŸ”„ Will use original model...\", f)\n",
    "    \n",
    "          log_and_print(f\"ðŸ“„ Full logs saved to: {log_file}\", f)\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Fine-tuning timed out after 2 hours, will use original model\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fine-tuning error: {e}\")\n",
    "        print(\"ðŸ”„ Will use original model...\")\n",
    "    finally:\n",
    "        # Restore original directory\n",
    "        os.chdir(original_cwd)\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ”„ Fine-tuning disabled - TTT mode not enabled\")\n",
    "    if TTT_MODE:\n",
    "        print(\"   (TTT_MODE=true detected, but ENABLE_FINE_TUNING override disabled fine-tuning)\")\n",
    "    else:\n",
    "        print(f\"   (Set TTT_MODE=true to enable Test-Time Training workflow)\")\n",
    "    print(f\"   Will use pre-loaded model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d0a22d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:04:36.290831Z",
     "iopub.status.busy": "2025-08-27T18:04:36.290273Z",
     "iopub.status.idle": "2025-08-27T18:04:36.417353Z",
     "shell.execute_reply": "2025-08-27T18:04:36.416676Z"
    },
    "papermill": {
     "duration": 0.13904,
     "end_time": "2025-08-27T18:04:36.418455",
     "exception": false,
     "start_time": "2025-08-27T18:04:36.279415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250827_174923__kaggle_input_arc-1-fake-ttt-blended-c802-dataset_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2025_test.parquet\r\n",
      "bin\r\n",
      "fine_tuning_20250827_175051.log\r\n",
      "__notebook__.ipynb\r\n",
      "Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\r\n",
      "sglang_server.log\r\n",
      "trainer_output\r\n",
      "unsloth_compiled_cache\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a42243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:04:36.440312Z",
     "iopub.status.busy": "2025-08-27T18:04:36.440075Z",
     "iopub.status.idle": "2025-08-27T18:07:23.546552Z",
     "shell.execute_reply": "2025-08-27T18:07:23.545993Z"
    },
    "papermill": {
     "duration": 167.128919,
     "end_time": "2025-08-27T18:07:23.557805",
     "exception": false,
     "start_time": "2025-08-27T18:04:36.428886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Restarting inference server with updated model...\n",
      "âœ… CUDA memory cleared\n",
      "ðŸŽ¯ Using model: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\n",
      "   â†’ Fine-tuned model\n",
      "ðŸš€ Starting server: /usr/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final --dp 4 --kv-cache-dtype fp8_e4m3\n",
      "âœ… Server started with PID=4668\n",
      "âœ… Server ready!\n",
      "ðŸŽ¯ Model: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_FINE_TUNING:\n",
    "  # Restart the server with the (potentially) new model\n",
    "  if START_SERVER:\n",
    "      print(\"ðŸ”„ Restarting inference server with updated model...\")\n",
    "\n",
    "      # Gracefully stop existing server if it exists\n",
    "      if 'proc' in locals() and proc.poll() is None:  # Check if process is still running\n",
    "          print(\"ðŸ›‘ Gracefully stopping existing server...\")\n",
    "          try:\n",
    "              proc.terminate()  # Send SIGTERM first\n",
    "              proc.wait(timeout=30)  # Wait up to 30 seconds for graceful shutdown\n",
    "              print(\"âœ… Server stopped gracefully\")\n",
    "          except subprocess.TimeoutExpired:\n",
    "              print(\"âš ï¸  Server didn't stop gracefully, force killing...\")\n",
    "              proc.kill()\n",
    "              proc.wait()\n",
    "          except Exception as e:\n",
    "              print(f\"âš ï¸  Error stopping server: {e}\")\n",
    "\n",
    "      # Wait a bit longer after graceful shutdown\n",
    "      time.sleep(5)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "              print(\"âœ… CUDA memory cleared\")\n",
    "      except Exception:\n",
    "          pass\n",
    "\n",
    "      # Get GPU count\n",
    "      try:\n",
    "          import torch\n",
    "          num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "      except:\n",
    "          num_gpus = 1\n",
    "\n",
    "      # Choose which model to use: fine-tuned if available, otherwise original\n",
    "      model_to_use = FINE_TUNED_MODEL_PATH if FINE_TUNED_MODEL_PATH else MODEL_PATH\n",
    "      print(f\"ðŸŽ¯ Using model: {model_to_use}\")\n",
    "      print(f\"   â†’ {'Fine-tuned' if FINE_TUNED_MODEL_PATH else 'Original'} model\")\n",
    "\n",
    "      # Restart server with appropriate model\n",
    "      PORT = 8080\n",
    "      LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "      SERVER_CMD = [\n",
    "          sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "          \"--host\", \"0.0.0.0\",\n",
    "          \"--port\", str(PORT),\n",
    "          \"--model-path\", str(model_to_use),\n",
    "          \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "          \"--kv-cache-dtype\", \"fp8_e4m3\",\n",
    "          \"--enable-metrics\"\n",
    "      ]\n",
    "\n",
    "      print(f\"ðŸš€ Starting server: {' '.join(SERVER_CMD)}\")\n",
    "\n",
    "      log_f = open(LOG, \"a\")\n",
    "      proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT,\n",
    "                             env=os.environ.copy(), cwd=SUBMIT_DIR)\n",
    "\n",
    "      print(f\"âœ… Server started with PID={proc.pid}\")\n",
    "\n",
    "      # Wait for readiness with better error handling\n",
    "      def wait_ready(url, timeout_s=600):\n",
    "          t0 = time.time()\n",
    "          while time.time() - t0 < timeout_s:\n",
    "              try:\n",
    "                  r = requests.get(url, timeout=5)\n",
    "                  if r.status_code == 200:\n",
    "                      return True\n",
    "              except Exception:\n",
    "                  pass\n",
    "              time.sleep(3)  # Check less frequently\n",
    "          return False\n",
    "\n",
    "      HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "      if wait_ready(HEALTH_URL):\n",
    "          print(\"âœ… Server ready!\")\n",
    "\n",
    "          # Update model_name\n",
    "          try:\n",
    "              response = requests.get(HEALTH_URL)\n",
    "              if response.status_code == 200:\n",
    "                  models = response.json()['data']\n",
    "                  if models:\n",
    "                      model_name = models[0]['id']\n",
    "                      print(f\"ðŸŽ¯ Model: {model_name}\")\n",
    "          except Exception as e:\n",
    "              print(f\"âš ï¸  Could not get model name: {e}\")\n",
    "      else:\n",
    "          print(\"âŒ Server failed to start properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b27d776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:07:23.580284Z",
     "iopub.status.busy": "2025-08-27T18:07:23.579699Z",
     "iopub.status.idle": "2025-08-27T18:08:52.579318Z",
     "shell.execute_reply": "2025-08-27T18:08:52.578573Z"
    },
    "papermill": {
     "duration": 89.012313,
     "end_time": "2025-08-27T18:08:52.580660",
     "exception": false,
     "start_time": "2025-08-27T18:07:23.568347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running SECOND inference with fine-tuned model (TTT mode)\n",
      "TTT Second Run â†’ dev | attempts=8 | workers=64 | subset=test\n",
      "Running TTT second inference: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset test --max_workers 64 --max_attempts 8 --model /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think --parquet-output-dir /kaggle/working\n",
      "Running TTT second inference: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset test --max_workers 64 --max_attempts 8 --model /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think --parquet-output-dir /kaggle/working\n",
      "\n",
      "â° Global timeout set to 60s via GLOBAL_TIMEOUT environment variable\r\n",
      "âš ï¸  WARNING: Using unrestricted executor - generated code will run directly on your system!\r\n",
      "â° API timeout: 600s (network safety only, no infrastructure timeouts)\r\n",
      "ðŸ—„ï¸ Sampled programs will be logged to /kaggle/working/20250827_180727__kaggle_working_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final_arc-prize-2025_test.parquet\r\n",
      "ðŸ“Š vLLM metrics monitoring enabled (http://127.0.0.1:8080/metrics)\r\n",
      "Loading subset: arc-prize-2025/test\r\n",
      "ðŸ” Validating 240 tasks...\r\n",
      "âœ… Task validation complete: 240 valid tasks\r\n",
      "ðŸ“ Tasks sorted by length (shortest to longest)\r\n",
      "\r\n",
      "Running 240 tasks from arc-prize-2025/test\r\n",
      "Model: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\r\n",
      "API: All Attempts Mode (8 attempts per task)\r\n",
      "Mode: True parallelization - 1920 total attempts\r\n",
      "Parallelization: ENABLED (64 workers)\r\n",
      "Scheduling: Batched (8 tasks Ã— 8 attempts = 64 workers used)\r\n",
      "\r\n",
      "âœ… No infrastructure timeouts - requests complete naturally to avoid GPU overload\r\n",
      "\r\n",
      "Sampling Parameters: {'max_tokens': 2000, 'temperature': 1.0, 'extra_body': {'min_p': 0.05, 'chat_template_kwargs': {'enable_thinking': False}}}\r\n",
      "Executor: unrestricted (timeout: 1s) âš ï¸  UNSAFE MODE\r\n",
      "--------------------------------------------------\r\n",
      "ðŸ“Š Starting metrics monitoring thread for http://127.0.0.1:8080/metrics\r\n",
      "ðŸ“Š Metrics thread started, polling http://127.0.0.1:8080/metrics every 30s\r\n",
      "ðŸ“Š [18:07:31] LLM metrics fetch failed\r\n",
      "\r\n",
      "ðŸ“ FIRST TASK PROMPT (00576224):\r\n",
      "================================================================================\r\n",
      "SYSTEM:\r\n",
      "You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by reasoning and generating Python code.\r\n",
      "\r\n",
      "USER:\r\n",
      "You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by generating Python code.\r\n",
      "Your goal is to analyze input-output grid pairs. The outputs were produced by applying a transformation rule to the inputs. Implement the transformation rules as a Python function.\r\n",
      "You should only write the implemented the transformation in code.\r\n",
      "You must write code in triple backticks (```python and then ```). You must write a function called 'transform' which takes a single argument, the input grid as 'list[list[int]]', and returns the transformed grid (also as 'list[list[int]]').\r\n",
      "You should make sure that you implement a version of the transformation which works in general (at least for all given input-output pairs and test input pairs).\r\n",
      "The number in the input grid can be mapped to the following colors: 0:Black; 1:Blue; 2:Red; 3:Green; 4:Yellow; 5:Grey; 6:Pink; 7:Orange; 8:Purple; 9:Brown\r\n",
      "Now, solve the following ARC-AGI task:\r\n",
      "# Task to solve:\r\n",
      "## Input 1 (grid shape: 2 by 2):\r\n",
      "[[7 9] [4 3]]\r\n",
      "## Output 1 (grid shape: 6 by 6):\r\n",
      "[[7 9 7 9 7 9] [4 3 4 3 4 3] [9 7 9 7 9 7] [3 4 3 4 3 4] [7 9 7 9 7 9] [4 3 4 3 4 3]]\r\n",
      "\r\n",
      "## Input 2 (grid shape: 2 by 2):\r\n",
      "[[8 6] [6 4]]\r\n",
      "## Output 2 (grid shape: 6 by 6):\r\n",
      "[[8 6 8 6 8 6] [6 4 6 4 6 4] [6 8 6 8 6 8] [4 6 4 6 4 6] [8 6 8 6 8 6] [6 4 6 4 6 4]]\r\n",
      "\r\n",
      "## Test Input 1 (grid shape: 2 by 2):\r\n",
      "[[3 2] [7 8]]\r\n",
      "\r\n",
      "================================================================================\r\n",
      "ðŸš€ Started 1920 attempts with 64 workers\r\n",
      "âœ… 25ff71a9: 8 attempts | 8 valid outputs | 8 train-perfect (best: 100.0% train)\r\n",
      "âœ… 3c9b0459: 8 attempts | 7 valid outputs, 1 invalid outputs | 4 train-perfect, 3 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 0d3d703e: 8 attempts | 6 valid outputs, 2 execution failures | 5 train-partial (of which 2 trans), 1 train-incorrect (best: 25.0% train)\r\n",
      "âœ… 27a28665: 8 attempts | 7 valid outputs, 1 invalid outputs | 3 train-perfect, 2 train-partial (of which 1 trans), 2 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 0520fde7: 8 attempts | 8 valid outputs | 2 train-perfect, 4 train-partial, 2 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 25d8a9c8: 8 attempts | 8 valid outputs | 7 train-perfect, 1 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 00576224: 8 attempts | 8 valid outputs | 3 train-perfect, 5 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 2dee498d: 8 attempts | 8 valid outputs | 4 train-perfect, 4 train-partial (best: 100.0% train)\r\n",
      "ðŸ¥ Health [100 attempts]: Success 97% | Timeout 0% | ExecErr 3% | AvgTime 6.80s\r\n",
      "âœ… 2072aba6: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 2de01db2: 8 attempts | 8 valid outputs | 6 train-partial, 2 train-incorrect (best: 33.3% train)\r\n",
      "âœ… 0c786b71: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 3af2c5a8: 8 attempts | 8 valid outputs | 1 train-perfect, 5 train-partial, 2 train-incorrect (best: 100.0% train)\r\n",
      "â³ No completions in last 15s â€” 114/1920 done; 1806 remaining (timeout in 60s)\r\n",
      "âœ… 3618c87e: 8 attempts | 8 valid outputs | 6 train-perfect, 2 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 1e0a9b12: 8 attempts | 7 valid outputs, 1 execution failures | 2 train-perfect, 5 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 017c7c7b: 8 attempts | 8 valid outputs | 6 train-partial, 2 train-incorrect (best: 66.7% train)\r\n",
      "âœ… 27a77e38: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 3ac3eb23: 8 attempts | 8 valid outputs | 7 train-perfect, 1 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 29c11459: 8 attempts | 8 valid outputs | 4 train-perfect, 3 train-partial, 1 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 3979b1a8: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "ðŸ¥ Health [200 attempts]: Success 96% | Timeout 0% | ExecErr 4% | AvgTime 10.26s\r\n",
      "ðŸ“Š [18:08:01] LLM metrics fetch failed\r\n",
      "â³ No completions in last 15s â€” 200/1920 done; 1720 remaining (timeout in 45s)\r\n",
      "âœ… 0692e18c: 8 attempts | 7 valid outputs, 1 execution failures | 7 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 17cae0c1: 8 attempts | 7 valid outputs, 1 execution failures | 7 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 10fcaaa3: 8 attempts | 8 valid outputs | 4 train-partial, 4 train-incorrect (best: 50.0% train)\r\n",
      "âœ… 3cd86f4f: 8 attempts | 6 valid outputs, 2 execution failures | 6 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 3aa6fb7a: 8 attempts | 8 valid outputs | 5 train-perfect, 3 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 234bbc79: 8 attempts | 6 valid outputs, 2 execution failures | 6 train-incorrect (of which 1 trans) (best: 0.0% train)\r\n",
      "âœ… 2013d3e2: 8 attempts | 7 valid outputs, 1 invalid outputs | 4 train-perfect, 3 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 31d5ba1a: 8 attempts | 5 valid outputs, 3 execution failures | 5 train-incorrect (of which 1 trans) (best: 0.0% train)\r\n",
      "âœ… 34b99a2b: 8 attempts | 7 valid outputs, 1 execution failures | 7 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 1b2d62fb: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 239be575: 8 attempts | 8 valid outputs | 1 train-perfect, 6 train-partial, 1 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 28bf18c6: 8 attempts | 5 valid outputs, 1 execution failures, 2 invalid outputs | 3 train-perfect, 2 train-incorrect (best: 100.0% train)\r\n",
      "â³ No completions in last 15s â€” 285/1920 done; 1635 remaining (timeout in 30s)\r\n",
      "âœ… 1fad071e: 8 attempts | 7 valid outputs, 1 invalid outputs | 2 train-perfect, 3 train-partial, 2 train-incorrect (best: 100.0% train)\r\n",
      "ðŸ¥ Health [300 attempts]: Success 95% | Timeout 0% | ExecErr 5% | AvgTime 10.73s\r\n",
      "âœ… 32e9702f: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 27f8ce4f: 8 attempts | 8 valid outputs | 5 train-perfect, 1 train-partial (of which 1 trans), 2 train-incorrect (of which 1 trans) (best: 100.0% train)\r\n",
      "âœ… 2dc579da: 8 attempts | 6 valid outputs, 2 invalid outputs | 4 train-partial, 2 train-incorrect (best: 33.3% train)\r\n",
      "âœ… 15696249: 8 attempts | 7 valid outputs, 1 execution failures | 6 train-partial, 1 train-incorrect (best: 50.0% train)\r\n",
      "âœ… 05269061: 8 attempts | 8 valid outputs | 1 train-perfect, 3 train-partial, 4 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 30f42897: 8 attempts | 7 valid outputs, 1 execution failures | 7 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 3b4c2228: 8 attempts | 7 valid outputs, 1 execution failures | 2 train-partial, 5 train-incorrect (of which 1 trans) (best: 20.0% train)\r\n",
      "âœ… 332efdb3: 8 attempts | 8 valid outputs | 4 train-perfect, 4 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 08ed6ac7: 8 attempts | 8 valid outputs | 1 train-perfect, 7 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 195ba7dc: 8 attempts | 7 valid outputs, 1 execution failures | 2 train-perfect, 5 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 12eac192: 8 attempts | 8 valid outputs | 3 train-partial, 5 train-incorrect (best: 25.0% train)\r\n",
      "âœ… 4258a5f9: 8 attempts | 8 valid outputs | 7 train-perfect, 1 train-incorrect (best: 100.0% train)\r\n",
      "ðŸ“Š [18:08:31] LLM metrics fetch failed\r\n",
      "â³ No completions in last 15s â€” 375/1920 done; 1545 remaining (timeout in 15s)\r\n",
      "â° Global timeout reached (60s). Cancelling remaining attempts...\r\n",
      "â° Timeout: 375 attempts completed, 1481 cancelled, 64 already running\r\n",
      "â° Execution stopped after 60.0s due to global timeout\r\n",
      "ðŸ›‘ 1481 attempts were cancelled due to timeout\r\n",
      "ðŸ“Š Final status: 375 successful, 0 failed, 1481 cancelled\r\n",
      "âœ… 0c9aba6e: 8 attempts | 8 valid outputs | 1 train-perfect, 7 train-incorrect (of which 2 trans) (best: 100.0% train)\r\n",
      "ðŸ¥ Health [400 attempts]: Success 95% | Timeout 0% | ExecErr 5% | AvgTime 10.69s\r\n",
      "âœ… 1478ab18: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âœ… 23581191: 8 attempts | 7 valid outputs, 1 execution failures | 3 train-perfect, 1 train-partial, 3 train-incorrect (best: 100.0% train)\r\n",
      "âœ… 3bd292e8: 8 attempts | 8 valid outputs | 8 train-incorrect (best: 0.0% train)\r\n",
      "âš ï¸ Task 252143c9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1e5d6875 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 137eaa0f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3428a4f5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0ca9ddb6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1cf80156 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 150deff5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3f7978a0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 337b420f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a2e2828 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 12422b43 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 412b6263 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 281123b4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2601afb7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3d31c5b3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2a5f8217 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3eda0437 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 351d6448 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3bd67248 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 11e1fe23 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 178fcbfb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c0d0a4b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 03560426 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0becf7df has no valid attempts - skipping\r\n",
      "âš ï¸ Task 11852cab has no valid attempts - skipping\r\n",
      "âš ï¸ Task 137f0df0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 14b8e18c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1b60fb0c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f642eb9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f876c06 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2204b7a8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22168020 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22233c11 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2281f1f4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 228f6490 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22eb0ac0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 31aa019c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 31adaf00 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3906de3d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 292dd178 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0962bcdd has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25c199f5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 310f3251 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22a4bbc2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f0c79e5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3befdf3e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 05f2a901 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 12997ef3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 39a8645d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25e02866 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 363442ee has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22425bda has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2c737e39 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0bb8deee has no valid attempts - skipping\r\n",
      "âš ï¸ Task 21f83797 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 29623171 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 230f2e48 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 19bb5feb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2f767503 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2bcee788 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 140c817e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 20fb2937 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3de23699 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 41ace6b5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2bee17df has no valid attempts - skipping\r\n",
      "âš ï¸ Task 182e5d0f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3bdb4ada has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2697da3f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2b01abd0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3f23242b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 18447a8d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2753e76c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 342dd610 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3391f8c0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 278e5215 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1efba499 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 20818e16 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a244afd has no valid attempts - skipping\r\n",
      "âš ï¸ Task 09629e4f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 29700607 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0b17323b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 41e4d17e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25d487eb has no valid attempts - skipping\r\n",
      "âš ï¸ Task 11dc524f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 36d67576 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 18286ef8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2ccd9fef has no valid attempts - skipping\r\n",
      "âš ï¸ Task 13f06aa5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 423a55dc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 272f95fa has no valid attempts - skipping\r\n",
      "âš ï¸ Task 17829a00 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1d61978c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3345333e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 396d80d7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3d588dc9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2faf500b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1acc24af has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2685904e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1b8318e3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 4093f84a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15663ba9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 253bf280 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c786137 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 4290ef0e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 00dbd492 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3d6c6e23 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 23b5c85d has no valid attempts - skipping\r\n",
      "âš ï¸ Task 20981f0e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 28e73c20 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0e671a1a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3e980e27 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1990f7a8 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 358ba94e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 103eff5b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0b148d64 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c02dbbe has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1e81d6f9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0a2355a6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1190e5a7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 17b80ad2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1b59e163 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3194b014 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2dd70a9a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2c608aff has no valid attempts - skipping\r\n",
      "âš ï¸ Task 17b866bd has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2f0c5170 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22806e14 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 332202d5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 342ae2ed has no valid attempts - skipping\r\n",
      "âš ï¸ Task 36fdfd69 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0e206a2e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15660dd6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1da012fc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1d398264 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 414297c0 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 00d62c1b has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0a938d79 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2546ccf6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1c56ad9f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 13713586 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 070dd51e has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a6449f1 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1e32b0e9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 32597951 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0f63c0b9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 22208ba4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1a07d186 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 37d3e8b2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0d87d2a6 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 009d5c81 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3ee1011a has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1be83260 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2037f2c7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 14754a24 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 212895b5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3a301edc has no valid attempts - skipping\r\n",
      "âš ï¸ Task 33b52de3 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1f85a75f has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2e65ae53 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 305b1341 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 319f2597 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 18419cfa has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2c0b0aff has no valid attempts - skipping\r\n",
      "âš ï¸ Task 33067df9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 045e512c has no valid attempts - skipping\r\n",
      "âš ï¸ Task 34cfa167 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 184a9768 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0607ce86 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 42918530 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 06df4c85 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 0a1d4ef5 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 15113be4 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 256b0a75 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 320afe60 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 09c534e7 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 1d0a4b61 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 25094a63 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3ad05f52 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 3490cc26 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 2b9ef948 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 39e1d7f9 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 40f6cd08 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 05a7bcf2 has no valid attempts - skipping\r\n",
      "âš ï¸ Task 264363fd has no valid attempts - skipping\r\n",
      "\r\n",
      "==================================================\r\n",
      "SUBMIT MODE SUMMARY\r\n",
      "==================================================\r\n",
      "Dataset: arc-prize-2025\r\n",
      "Subset: test\r\n",
      "Model: /kaggle/working/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\r\n",
      "Total tasks processed: 56\r\n",
      "Total time: 60.0s\r\n",
      "Successful API calls: 56/56 (100.0%)\r\n",
      "Total tokens used: 521,015\r\n",
      "Total cost: $0.108852\r\n",
      "\r\n",
      "ðŸ“Š RESPONSE METRICS:\r\n",
      "  Total responses: 439\r\n",
      "  Code extracted: 439/439 (100.0%)\r\n",
      "  Max length responses: 0/439 (0.0%)\r\n",
      "  Timeout responses: 0/439 (0.0%)\r\n",
      "  API failure responses: 0/439 (0.0%)\r\n",
      "\r\n",
      "ðŸ“Š TRAIN METRICS:\r\n",
      "  All train correct: 28/56 (50.0%)\r\n",
      "  Min 1 train correct: 37/56 (66.1%)\r\n",
      "\r\n",
      "âš ï¸  Note: Test accuracy metrics unavailable in SUBMIT mode (no test outputs)\r\n",
      "\r\n",
      "ðŸŽ¯ To generate submission file, run:\r\n",
      "   uv run python llm_python/generate_submission.py --dataset arc-prize-2025 --subset test\r\n",
      "All sampled programs saved to /kaggle/working/20250827_180727__kaggle_working_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final_arc-prize-2025_test.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Second Inference Run - TTT Mode Only\n",
    "# Only runs when TTT_MODE=true (after fine-tuning)\n",
    "\n",
    "if TTT_MODE:\n",
    "    print(\"ðŸ”„ Running SECOND inference with fine-tuned model (TTT mode)\")\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        %cd /workspace/arc-agi-2025\n",
    "\n",
    "    # Use SECOND_ATTEMPTS and SECOND_WORKERS for second inference\n",
    "    MAX_ATTEMPTS = SECOND_ATTEMPTS if (IS_RERUN or not IS_KAGGLE) else min(SECOND_ATTEMPTS, 8)\n",
    "    MAX_WORKERS  = SECOND_WORKERS\n",
    "\n",
    "    SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "    # # can use this instead if testing evaluation during a pre-run\n",
    "    # SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "    # Common env for your runner\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "    print(f\"TTT Second Run â†’ {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "    # Build the command\n",
    "    cmd_args = [\n",
    "        \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--max_workers\", str(MAX_WORKERS),\n",
    "        \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "        \"--model\", model_name,\n",
    "        \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "        \"--unsafe-executor\",\n",
    "        \"--max-tokens\", \"2000\",\n",
    "        \"--qwen-no-think\"\n",
    "    ]\n",
    "\n",
    "    # Add parquet output directory if set\n",
    "    if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "      cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "\n",
    "    print(f\"Running TTT second inference: {' '.join(cmd_args)}\")\n",
    "\n",
    "    # Handle output redirection properly\n",
    "    if IS_RERUN or not IS_KAGGLE:\n",
    "        # For quiet mode, redirect to file using subprocess\n",
    "        import subprocess\n",
    "        log_file_path = f\"{SUBMIT_DIR}/run_ttt_second.log\"\n",
    "        print(f\"ðŸ“ Logging TTT second run output to: {log_file_path}\")\n",
    "        \n",
    "        with open(log_file_path, \"w\") as log_file:\n",
    "            process = subprocess.Popen(\n",
    "                cmd_args,\n",
    "                stdout=log_file,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "                cwd=os.getcwd()\n",
    "            )\n",
    "            \n",
    "            # Wait for completion\n",
    "            print(\"â³ Running TTT second inference (output being written to log file)...\")\n",
    "            return_code = process.wait()\n",
    "            \n",
    "        if return_code == 0:\n",
    "            print(f\"âœ… TTT second inference completed successfully. Check {log_file_path} for details.\")\n",
    "        else:\n",
    "            print(f\"âŒ TTT second inference failed with return code {return_code}\")\n",
    "            print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "            # Show last few lines of log\n",
    "            !tail -n 20 {log_file_path}\n",
    "    else:\n",
    "        # For interactive mode, show output directly\n",
    "        cmd = \" \".join(cmd_args)\n",
    "        print(f\"Running TTT second inference: {cmd}\\n\")\n",
    "        !{cmd}\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ”„ Skipping second inference (TTT_MODE=false)\")\n",
    "    print(\"   â†’ Standard mode runs first inference only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aff75d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:08:52.609630Z",
     "iopub.status.busy": "2025-08-27T18:08:52.609166Z",
     "iopub.status.idle": "2025-08-27T18:08:55.443849Z",
     "shell.execute_reply": "2025-08-27T18:08:55.443247Z"
    },
    "papermill": {
     "duration": 2.849923,
     "end_time": "2025-08-27T18:08:55.444859",
     "exception": false,
     "start_time": "2025-08-27T18:08:52.594936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Generating submission from the two most recent parquet files...\n",
      "Running submission generation: uv run python -m llm_python.generate_submission --parquet-path /kaggle/working --n-files 2 --dataset arc-prize-2025 --subset test --output-dir /kaggle/working --debug\n",
      "ðŸ“‚ Looking for parquet files in: /kaggle/working\n",
      "âœ… Submission generation completed successfully!\n",
      "ðŸ” Selected 2 most recent parquet files from /kaggle/working:\n",
      "  â€¢ 20250827_180727__kaggle_working_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final_arc-prize-2025_test.parquet (modified: 2025-08-27 18:08:52)\n",
      "  â€¢ 20250827_174923__kaggle_input_arc-1-fake-ttt-blended-c802-dataset_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2025_test.parquet (modified: 2025-08-27 17:50:49)\n",
      "âœ… Loaded 408 rows from /kaggle/working/20250827_180727__kaggle_working_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final_arc-prize-2025_test.parquet\n",
      "âœ… Loaded 178 rows from /kaggle/working/20250827_174923__kaggle_input_arc-1-fake-ttt-blended-c802-dataset_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2025_test.parquet\n",
      "ðŸ“Š Combined data: 586 total rows from 2 files\n",
      "ðŸŽ¯ Generating submission for 240 tasks from arc-prize-2025/test\n",
      "âš ï¸ No attempts for task 009d5c81, using empty fallback\n",
      "âš ï¸ No attempts for task 00d62c1b, using empty fallback\n",
      "âš ï¸ No attempts for task 00dbd492, using empty fallback\n",
      "âš ï¸ No attempts for task 03560426, using empty fallback\n",
      "âš ï¸ No attempts for task 045e512c, using empty fallback\n",
      "âš ï¸ No attempts for task 05a7bcf2, using empty fallback\n",
      "âš ï¸ No attempts for task 05f2a901, using empty fallback\n",
      "âš ï¸ No attempts for task 0607ce86, using empty fallback\n",
      "âš ï¸ No attempts for task 06df4c85, using empty fallback\n",
      "âš ï¸ No attempts for task 070dd51e, using empty fallback\n",
      "âš ï¸ No attempts for task 09629e4f, using empty fallback\n",
      "âš ï¸ No attempts for task 0962bcdd, using empty fallback\n",
      "âš ï¸ No attempts for task 09c534e7, using empty fallback\n",
      "âš ï¸ No attempts for task 0a1d4ef5, using empty fallback\n",
      "âš ï¸ No attempts for task 0a2355a6, using empty fallback\n",
      "âš ï¸ No attempts for task 0a938d79, using empty fallback\n",
      "âš ï¸ No attempts for task 0b148d64, using empty fallback\n",
      "âš ï¸ No attempts for task 0b17323b, using empty fallback\n",
      "âš ï¸ No attempts for task 0bb8deee, using empty fallback\n",
      "âš ï¸ No attempts for task 0becf7df, using empty fallback\n",
      "âš ï¸ No attempts for task 0ca9ddb6, using empty fallback\n",
      "âš ï¸ No attempts for task 0d87d2a6, using empty fallback\n",
      "âš ï¸ No attempts for task 0e206a2e, using empty fallback\n",
      "âš ï¸ No attempts for task 0e671a1a, using empty fallback\n",
      "âš ï¸ No attempts for task 0f63c0b9, using empty fallback\n",
      "âš ï¸ No attempts for task 103eff5b, using empty fallback\n",
      "âš ï¸ No attempts for task 11852cab, using empty fallback\n",
      "âš ï¸ No attempts for task 1190e5a7, using empty fallback\n",
      "âš ï¸ No attempts for task 11dc524f, using empty fallback\n",
      "âš ï¸ No attempts for task 11e1fe23, using empty fallback\n",
      "âš ï¸ No attempts for task 12422b43, using empty fallback\n",
      "âš ï¸ No attempts for task 12997ef3, using empty fallback\n",
      "âš ï¸ No attempts for task 13713586, using empty fallback\n",
      "âš ï¸ No attempts for task 137eaa0f, using empty fallback\n",
      "âš ï¸ No attempts for task 137f0df0, using empty fallback\n",
      "âš ï¸ No attempts for task 13f06aa5, using empty fallback\n",
      "âš ï¸ No attempts for task 140c817e, using empty fallback\n",
      "âš ï¸ No attempts for task 14754a24, using empty fallback\n",
      "âš ï¸ No attempts for task 14b8e18c, using empty fallback\n",
      "âš ï¸ No attempts for task 150deff5, using empty fallback\n",
      "âš ï¸ No attempts for task 15113be4, using empty fallback\n",
      "âš ï¸ No attempts for task 15660dd6, using empty fallback\n",
      "âš ï¸ No attempts for task 15663ba9, using empty fallback\n",
      "âš ï¸ No attempts for task 17829a00, using empty fallback\n",
      "âš ï¸ No attempts for task 178fcbfb, using empty fallback\n",
      "âš ï¸ No attempts for task 17b80ad2, using empty fallback\n",
      "âš ï¸ No attempts for task 17b866bd, using empty fallback\n",
      "âš ï¸ No attempts for task 18286ef8, using empty fallback\n",
      "âš ï¸ No attempts for task 182e5d0f, using empty fallback\n",
      "âš ï¸ No attempts for task 18419cfa, using empty fallback\n",
      "âš ï¸ No attempts for task 18447a8d, using empty fallback\n",
      "âš ï¸ No attempts for task 184a9768, using empty fallback\n",
      "âš ï¸ No attempts for task 1990f7a8, using empty fallback\n",
      "âš ï¸ No attempts for task 19bb5feb, using empty fallback\n",
      "âš ï¸ No attempts for task 1a07d186, using empty fallback\n",
      "âš ï¸ No attempts for task 1a244afd, using empty fallback\n",
      "âš ï¸ No attempts for task 1a2e2828, using empty fallback\n",
      "âš ï¸ No attempts for task 1a6449f1, using empty fallback\n",
      "âš ï¸ No attempts for task 1acc24af, using empty fallback\n",
      "âš ï¸ No attempts for task 1b59e163, using empty fallback\n",
      "âš ï¸ No attempts for task 1b60fb0c, using empty fallback\n",
      "âš ï¸ No attempts for task 1b8318e3, using empty fallback\n",
      "âš ï¸ No attempts for task 1be83260, using empty fallback\n",
      "âš ï¸ No attempts for task 1c02dbbe, using empty fallback\n",
      "âš ï¸ No attempts for task 1c0d0a4b, using empty fallback\n",
      "âš ï¸ No attempts for task 1c56ad9f, using empty fallback\n",
      "âš ï¸ No attempts for task 1c786137, using empty fallback\n",
      "âš ï¸ No attempts for task 1cf80156, using empty fallback\n",
      "âš ï¸ No attempts for task 1d0a4b61, using empty fallback\n",
      "âš ï¸ No attempts for task 1d398264, using empty fallback\n",
      "âš ï¸ No attempts for task 1d61978c, using empty fallback\n",
      "âš ï¸ No attempts for task 1da012fc, using empty fallback\n",
      "âš ï¸ No attempts for task 1e32b0e9, using empty fallback\n",
      "âš ï¸ No attempts for task 1e5d6875, using empty fallback\n",
      "âš ï¸ No attempts for task 1e81d6f9, using empty fallback\n",
      "âš ï¸ No attempts for task 1efba499, using empty fallback\n",
      "âš ï¸ No attempts for task 1f0c79e5, using empty fallback\n",
      "âš ï¸ No attempts for task 1f642eb9, using empty fallback\n",
      "âš ï¸ No attempts for task 1f85a75f, using empty fallback\n",
      "âš ï¸ No attempts for task 1f876c06, using empty fallback\n",
      "âš ï¸ No attempts for task 2037f2c7, using empty fallback\n",
      "âš ï¸ No attempts for task 20818e16, using empty fallback\n",
      "âš ï¸ No attempts for task 20981f0e, using empty fallback\n",
      "âš ï¸ No attempts for task 20fb2937, using empty fallback\n",
      "âš ï¸ No attempts for task 212895b5, using empty fallback\n",
      "âš ï¸ No attempts for task 21f83797, using empty fallback\n",
      "âš ï¸ No attempts for task 2204b7a8, using empty fallback\n",
      "âš ï¸ No attempts for task 22168020, using empty fallback\n",
      "âš ï¸ No attempts for task 22208ba4, using empty fallback\n",
      "âš ï¸ No attempts for task 22233c11, using empty fallback\n",
      "âš ï¸ No attempts for task 22425bda, using empty fallback\n",
      "âš ï¸ No attempts for task 22806e14, using empty fallback\n",
      "âš ï¸ No attempts for task 2281f1f4, using empty fallback\n",
      "âš ï¸ No attempts for task 228f6490, using empty fallback\n",
      "âš ï¸ No attempts for task 22a4bbc2, using empty fallback\n",
      "âš ï¸ No attempts for task 22eb0ac0, using empty fallback\n",
      "âš ï¸ No attempts for task 230f2e48, using empty fallback\n",
      "âš ï¸ No attempts for task 23b5c85d, using empty fallback\n",
      "âš ï¸ No attempts for task 25094a63, using empty fallback\n",
      "âš ï¸ No attempts for task 252143c9, using empty fallback\n",
      "âš ï¸ No attempts for task 253bf280, using empty fallback\n",
      "âš ï¸ No attempts for task 2546ccf6, using empty fallback\n",
      "âš ï¸ No attempts for task 256b0a75, using empty fallback\n",
      "âš ï¸ No attempts for task 25c199f5, using empty fallback\n",
      "âš ï¸ No attempts for task 25d487eb, using empty fallback\n",
      "âš ï¸ No attempts for task 25e02866, using empty fallback\n",
      "âš ï¸ No attempts for task 2601afb7, using empty fallback\n",
      "âš ï¸ No attempts for task 264363fd, using empty fallback\n",
      "âš ï¸ No attempts for task 2685904e, using empty fallback\n",
      "âš ï¸ No attempts for task 2697da3f, using empty fallback\n",
      "âš ï¸ No attempts for task 272f95fa, using empty fallback\n",
      "âš ï¸ No attempts for task 2753e76c, using empty fallback\n",
      "âš ï¸ No attempts for task 278e5215, using empty fallback\n",
      "âš ï¸ No attempts for task 281123b4, using empty fallback\n",
      "âš ï¸ No attempts for task 28e73c20, using empty fallback\n",
      "âš ï¸ No attempts for task 292dd178, using empty fallback\n",
      "âš ï¸ No attempts for task 29623171, using empty fallback\n",
      "âš ï¸ No attempts for task 29700607, using empty fallback\n",
      "âš ï¸ No attempts for task 2a5f8217, using empty fallback\n",
      "âš ï¸ No attempts for task 2b01abd0, using empty fallback\n",
      "âš ï¸ No attempts for task 2b9ef948, using empty fallback\n",
      "âš ï¸ No attempts for task 2bcee788, using empty fallback\n",
      "âš ï¸ No attempts for task 2bee17df, using empty fallback\n",
      "âš ï¸ No attempts for task 2c0b0aff, using empty fallback\n",
      "âš ï¸ No attempts for task 2c608aff, using empty fallback\n",
      "âš ï¸ No attempts for task 2c737e39, using empty fallback\n",
      "âš ï¸ No attempts for task 2ccd9fef, using empty fallback\n",
      "âš ï¸ No attempts for task 2dd70a9a, using empty fallback\n",
      "âš ï¸ No attempts for task 2e65ae53, using empty fallback\n",
      "âš ï¸ No attempts for task 2f0c5170, using empty fallback\n",
      "âš ï¸ No attempts for task 2f767503, using empty fallback\n",
      "âš ï¸ No attempts for task 2faf500b, using empty fallback\n",
      "âš ï¸ No attempts for task 305b1341, using empty fallback\n",
      "âš ï¸ No attempts for task 310f3251, using empty fallback\n",
      "âš ï¸ No attempts for task 3194b014, using empty fallback\n",
      "âš ï¸ No attempts for task 319f2597, using empty fallback\n",
      "âš ï¸ No attempts for task 31aa019c, using empty fallback\n",
      "âš ï¸ No attempts for task 31adaf00, using empty fallback\n",
      "âš ï¸ No attempts for task 320afe60, using empty fallback\n",
      "âš ï¸ No attempts for task 32597951, using empty fallback\n",
      "âš ï¸ No attempts for task 33067df9, using empty fallback\n",
      "âš ï¸ No attempts for task 332202d5, using empty fallback\n",
      "âš ï¸ No attempts for task 3345333e, using empty fallback\n",
      "âš ï¸ No attempts for task 337b420f, using empty fallback\n",
      "âš ï¸ No attempts for task 3391f8c0, using empty fallback\n",
      "âš ï¸ No attempts for task 33b52de3, using empty fallback\n",
      "âš ï¸ No attempts for task 3428a4f5, using empty fallback\n",
      "âš ï¸ No attempts for task 342ae2ed, using empty fallback\n",
      "âš ï¸ No attempts for task 342dd610, using empty fallback\n",
      "âš ï¸ No attempts for task 3490cc26, using empty fallback\n",
      "âš ï¸ No attempts for task 34cfa167, using empty fallback\n",
      "âš ï¸ No attempts for task 351d6448, using empty fallback\n",
      "âš ï¸ No attempts for task 358ba94e, using empty fallback\n",
      "âš ï¸ No attempts for task 363442ee, using empty fallback\n",
      "âš ï¸ No attempts for task 36d67576, using empty fallback\n",
      "âš ï¸ No attempts for task 36fdfd69, using empty fallback\n",
      "âš ï¸ No attempts for task 37d3e8b2, using empty fallback\n",
      "âš ï¸ No attempts for task 3906de3d, using empty fallback\n",
      "âš ï¸ No attempts for task 396d80d7, using empty fallback\n",
      "âš ï¸ No attempts for task 39a8645d, using empty fallback\n",
      "âš ï¸ No attempts for task 39e1d7f9, using empty fallback\n",
      "âš ï¸ No attempts for task 3a301edc, using empty fallback\n",
      "âš ï¸ No attempts for task 3ad05f52, using empty fallback\n",
      "âš ï¸ No attempts for task 3bd67248, using empty fallback\n",
      "âš ï¸ No attempts for task 3bdb4ada, using empty fallback\n",
      "âš ï¸ No attempts for task 3befdf3e, using empty fallback\n",
      "âš ï¸ No attempts for task 3d31c5b3, using empty fallback\n",
      "âš ï¸ No attempts for task 3d588dc9, using empty fallback\n",
      "âš ï¸ No attempts for task 3d6c6e23, using empty fallback\n",
      "âš ï¸ No attempts for task 3de23699, using empty fallback\n",
      "âš ï¸ No attempts for task 3e980e27, using empty fallback\n",
      "âš ï¸ No attempts for task 3eda0437, using empty fallback\n",
      "âš ï¸ No attempts for task 3ee1011a, using empty fallback\n",
      "âš ï¸ No attempts for task 3f23242b, using empty fallback\n",
      "âš ï¸ No attempts for task 3f7978a0, using empty fallback\n",
      "âš ï¸ No attempts for task 4093f84a, using empty fallback\n",
      "âš ï¸ No attempts for task 40f6cd08, using empty fallback\n",
      "âš ï¸ No attempts for task 412b6263, using empty fallback\n",
      "âš ï¸ No attempts for task 414297c0, using empty fallback\n",
      "âš ï¸ No attempts for task 41ace6b5, using empty fallback\n",
      "âš ï¸ No attempts for task 41e4d17e, using empty fallback\n",
      "âš ï¸ No attempts for task 423a55dc, using empty fallback\n",
      "âš ï¸ No attempts for task 4290ef0e, using empty fallback\n",
      "âš ï¸ No attempts for task 42918530, using empty fallback\n",
      "\n",
      "âœ… Submission files created:\n",
      "ðŸ“Š Summary:\n",
      "  Total tasks in dataset: 240\n",
      "  Tasks with predictions: 56\n",
      "  Tasks with duplicated attempts: 0\n",
      "  Tasks with empty fallback: 184\n",
      "  Official file: /kaggle/working/submission.json\n",
      "  Backup file: /kaggle/working/submission_arc-prize-2025_test_180727__20250827_180855.json\n",
      "â„¹ï¸ Note: Run separate validation if needed\n",
      "ðŸŽ¯ Submission generation complete: /kaggle/working/submission.json\n",
      "\n",
      "ðŸ“ Submission file: /kaggle/working/submission.json\n"
     ]
    }
   ],
   "source": [
    "# Generate submission using the two most recent parquet files\n",
    "if os.environ.get(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "    print(\"ðŸŽ¯ Generating submission from the two most recent parquet files...\")\n",
    "    \n",
    "    import subprocess\n",
    "    \n",
    "    # Set up paths - parquet files are saved by task runner in different locations\n",
    "    if IS_KAGGLE:\n",
    "        # On Kaggle, parquet files are saved directly in /kaggle/working by task runner\n",
    "        inference_dir = \"/kaggle/working\"\n",
    "    else:\n",
    "        # On RunPod/local, parquet files are saved in llm_python/datasets/inference\n",
    "        inference_dir = \"llm_python/datasets/inference\"\n",
    "    \n",
    "    output_dir = str(SUBMIT_DIR)\n",
    "    \n",
    "    # Command to generate submission using the two most recent parquet files\n",
    "    submission_cmd = [\n",
    "        \"uv\", \"run\", \"python\", \"-m\", \"llm_python.generate_submission\",\n",
    "        \"--parquet-path\", inference_dir,\n",
    "        \"--n-files\", \"2\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--output-dir\", output_dir,\n",
    "        \"--debug\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running submission generation: {' '.join(submission_cmd)}\")\n",
    "    print(f\"ðŸ“‚ Looking for parquet files in: {inference_dir}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            submission_cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,  # 5 minute timeout\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Submission generation completed successfully!\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Update submit_dir to point to the generated file\n",
    "            submit_dir = f\"{output_dir}/submission.json\"\n",
    "            print(f\"ðŸ“ Submission file: {submit_dir}\")\n",
    "        else:\n",
    "            print(f\"âŒ Submission generation failed with return code {result.returncode}\")\n",
    "            print(f\"STDOUT: {result.stdout}\")\n",
    "            print(f\"STDERR: {result.stderr}\")\n",
    "            # Fallback to default submission path\n",
    "            submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Submission generation timed out\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Submission generation error: {e}\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "else:\n",
    "    print(\"ðŸ“ Skipping submission generation (SUBMIT=false)\")\n",
    "    submit_dir = f\"{SUBMIT_DIR}/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d42863b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:08:55.474844Z",
     "iopub.status.busy": "2025-08-27T18:08:55.474374Z",
     "iopub.status.idle": "2025-08-27T18:08:55.478245Z",
     "shell.execute_reply": "2025-08-27T18:08:55.477736Z"
    },
    "papermill": {
     "duration": 0.019365,
     "end_time": "2025-08-27T18:08:55.479184",
     "exception": false,
     "start_time": "2025-08-27T18:08:55.459819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping local scoring (competition rerun or SCORE=False).\n"
     ]
    }
   ],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission {submit_dir} --dataset {DATASET} --subset {SUBSET}\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46edba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T18:08:55.507629Z",
     "iopub.status.busy": "2025-08-27T18:08:55.507450Z",
     "iopub.status.idle": "2025-08-27T18:08:55.534699Z",
     "shell.execute_reply": "2025-08-27T18:08:55.534211Z"
    },
    "papermill": {
     "duration": 0.042414,
     "end_time": "2025-08-27T18:08:55.535588",
     "exception": false,
     "start_time": "2025-08-27T18:08:55.493174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up server and resources...\n",
      "Server stopped and CUDA memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup - stop server and free resources\n",
    "if START_SERVER and 'full_cleanup' in globals():\n",
    "    print(\"ðŸ§¹ Cleaning up server and resources...\")\n",
    "    full_cleanup()\n",
    "else:\n",
    "    print(\"ðŸ” No server cleanup needed (START_SERVER=False or cleanup function not available)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8063856,
     "sourceId": 12863743,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 258546714,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1505.168265,
   "end_time": "2025-08-27T18:08:58.165289",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-27T17:43:52.997024",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
