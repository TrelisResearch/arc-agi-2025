{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c845d89",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.013405,
     "end_time": "2025-08-21T17:20:29.935281",
     "exception": false,
     "start_time": "2025-08-21T17:20:29.921876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Runpod/local long run â€” setting FULL 17400s timeout for ARC task runner\n",
      "â° Global timeout set to 17400s (4.8 hours)\n",
      "ðŸ§ª TTT (Test-Time Training) mode ENABLED\n",
      "   â†’ Will run: First inference â†’ Fine-tuning â†’ Second inference\n",
      "Mode summary â†’ IS_KAGGLE=False | IS_RERUN=False | TTT_MODE=True |\n",
      "TEST_INFERENCE=False | SCORE=True | SUBMIT=true | MODEL_PATH=Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = \"Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\" # Need a way to get this from local if needed.\n",
    "DATASET = \"arc-prize-2024\"\n",
    "\n",
    "# ---- Config flags (single source of truth) ----\n",
    "START_SERVER = True\n",
    "TEST_INFERENCE = False          # set False unless you want a quick endpoint smoke test\n",
    "SCORE = True                   # default; overridden in branches below\n",
    "ATTEMPTS = 1\n",
    "\n",
    "# TTT Mode: Controls Test-Time Training workflow\n",
    "TTT_MODE = True\n",
    "\n",
    "# Env-backed flags\n",
    "IS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "IS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# String env flag for external tools\n",
    "os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# ---- Paths ----\n",
    "if IS_KAGGLE:\n",
    "    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n",
    "    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n",
    "    SUBMIT_DIR      = Path(\"/kaggle/working\")\n",
    "    ARC_PROGRAMS_PARQUET = SUBMIT_DIR / \"datasets\" / \"inference\"\n",
    "    MODEL_PATH = ARC_DATA_ROOT / \"arc-1-fake-ttt-blended-c802-dataset\"\n",
    "\n",
    "    # Auto-find model path by searching two levels deep\n",
    "    model_found = None\n",
    "    for dataset_dir in ARC_DATA_ROOT.iterdir():\n",
    "      if dataset_dir.is_dir():\n",
    "          for model_dir in dataset_dir.iterdir():\n",
    "              if model_dir.is_dir() and \"Qwen\" in model_dir.name:\n",
    "                  model_found = model_dir\n",
    "                  break\n",
    "      if model_found:\n",
    "          break\n",
    "    \n",
    "    if model_found:\n",
    "      MODEL_PATH = model_found\n",
    "      print(f\"ðŸ” Auto-found Kaggle model: {MODEL_PATH}\")\n",
    "    else:\n",
    "      # Fallback to the original hardcoded path structure\n",
    "      MODEL_PATH = ARC_DATA_ROOT / \"arc-1-fake-ttt-blended-c802-dataset\"\n",
    "      print(f\"âš ï¸  Model not auto-found, using fallback: {MODEL_PATH}\")\n",
    "else:\n",
    "    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n",
    "    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n",
    "    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n",
    "    ARC_PROGRAMS_PARQUET = Path(\"/workspace/arc-agi-2025/llm_python/datasets/inference\")\n",
    "\n",
    "# Initialize fine-tuned model path (will be set after fine-tuning)\n",
    "FINE_TUNED_MODEL_PATH = None\n",
    "\n",
    "# Export envs for downstream processes\n",
    "os.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\n",
    "os.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\n",
    "os.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\n",
    "os.environ[\"ARC_PROGRAMS_PARQUET\"] = str(ARC_PROGRAMS_PARQUET)\n",
    "os.environ[\"MODEL_PATH\"] = str(MODEL_PATH)\n",
    "\n",
    "# Export TTT and config flags for subprocess use\n",
    "os.environ[\"TTT_MODE\"] = str(TTT_MODE).lower()\n",
    "os.environ[\"IS_KAGGLE\"] = str(IS_KAGGLE).lower()\n",
    "os.environ[\"IS_RERUN\"] = str(IS_RERUN).lower()\n",
    "os.environ[\"DATASET\"] = DATASET\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Timeouts & mode tweaks ----\n",
    "FULL_TIMEOUT = 3600*5 - 600 # ~5 hour timeout for inference\n",
    "\n",
    "if IS_RERUN:\n",
    "    # Kaggle competition rerun\n",
    "    timeout_seconds = FULL_TIMEOUT\n",
    "    print(f\"ðŸ† Competition rerun detected â€” setting FULL {timeout_seconds}s timeout for ARC task runner\")\n",
    "    TEST_INFERENCE = False\n",
    "    SCORE = False\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "elif not IS_KAGGLE:\n",
    "    # Runpod / local long run\n",
    "    timeout_seconds = FULL_TIMEOUT\n",
    "    print(f\"ðŸ–¥ï¸ Runpod/local long run â€” setting FULL {timeout_seconds}s timeout for ARC task runner\")\n",
    "    if os.getenv(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "        SCORE = True  # if we're generating a submission, do scoring\n",
    "\n",
    "else:\n",
    "    # Kaggle dev/testing\n",
    "    timeout_seconds = 60  # 1 minute\n",
    "    print(f\"ðŸ”§ Development run â€” setting short {timeout_seconds}s timeout for testing\")\n",
    "    # Safer default: don't auto-submit in dev\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# Export timeout\n",
    "os.environ[\"GLOBAL_TIMEOUT\"] = str(timeout_seconds)\n",
    "print(f\"â° Global timeout set to {timeout_seconds}s ({timeout_seconds/3600:.1f} hours)\")\n",
    "\n",
    "# TTT Mode configuration\n",
    "if TTT_MODE:\n",
    "    print(\"ðŸ§ª TTT (Test-Time Training) mode ENABLED\")\n",
    "    print(\"   â†’ Will run: First inference â†’ Fine-tuning â†’ Second inference\")\n",
    "else:\n",
    "    print(\"ðŸ”„ Standard mode (TTT disabled)\")\n",
    "    print(\"   â†’ Will run: First inference only\")\n",
    "\n",
    "# Optional: quick summary (helps avoid accidental submits)\n",
    "print(\n",
    "    \"Mode summary â†’ \"\n",
    "    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | TTT_MODE={TTT_MODE} |\\n\"\n",
    "    f\"TEST_INFERENCE={TEST_INFERENCE} | SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | MODEL_PATH={MODEL_PATH}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c504e677",
   "metadata": {
    "papermill": {
     "duration": 33.682835,
     "end_time": "2025-08-21T17:21:03.621026",
     "exception": false,
     "start_time": "2025-08-21T17:20:29.938191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA version (PyTorch): 12.6\n",
      "CUDA available: True\n",
      "NumPy version: 2.2.0\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c30e81c",
   "metadata": {
    "papermill": {
     "duration": 9.103683,
     "end_time": "2025-08-21T17:21:12.737447",
     "exception": false,
     "start_time": "2025-08-21T17:21:03.633764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang version: 0.4.9.post3\n",
      "FlashInfer version: 0.2.7.post1\n"
     ]
    }
   ],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75ff724",
   "metadata": {
    "papermill": {
     "duration": 1.057758,
     "end_time": "2025-08-21T17:21:13.798168",
     "exception": false,
     "start_time": "2025-08-21T17:21:12.740410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_KAGGLE:\n",
    "    import os, shutil, subprocess, stat\n",
    "    \n",
    "    # 1) Where to place binaries + cache\n",
    "    WRK_BIN = \"/kaggle/working/bin\"\n",
    "    TRITON_CACHE = \"/kaggle/working/.triton\"\n",
    "    os.makedirs(WRK_BIN, exist_ok=True)\n",
    "    os.makedirs(TRITON_CACHE, exist_ok=True)\n",
    "    \n",
    "    # 2) Preferred source for ptxas/cuobjdump/nvdisasm\n",
    "    SYSTEM_CUDA_BIN = \"/usr/local/cuda/bin\"\n",
    "    FALLBACK_VENDORED = \"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin\"  # if you have it\n",
    "    \n",
    "    def copy_tool(name: str):\n",
    "        for src_dir in (SYSTEM_CUDA_BIN, FALLBACK_VENDORED):\n",
    "            src = os.path.join(src_dir, name)\n",
    "            if os.path.exists(src):\n",
    "                dst = os.path.join(WRK_BIN, name)\n",
    "                shutil.copy2(src, dst)\n",
    "                # ensure executable bit\n",
    "                os.chmod(dst, os.stat(dst).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "                return dst\n",
    "        raise FileNotFoundError(f\"Could not find {name} in {SYSTEM_CUDA_BIN} or {FALLBACK_VENDORED}\")\n",
    "    \n",
    "    ptxas_path = copy_tool(\"ptxas\")\n",
    "    try:\n",
    "        cuobjdump_path = copy_tool(\"cuobjdump\")\n",
    "    except FileNotFoundError:\n",
    "        cuobjdump_path = None  # optional\n",
    "    try:\n",
    "        nvdisasm_path = copy_tool(\"nvdisasm\")\n",
    "    except FileNotFoundError:\n",
    "        nvdisasm_path = None  # optional\n",
    "    \n",
    "    # 3) Environment for Triton/JIT\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n",
    "    os.environ[\"PATH\"] = f\"{WRK_BIN}:{os.environ.get('PATH','')}\"\n",
    "    os.environ[\"TRITON_CACHE_DIR\"] = TRITON_CACHE\n",
    "    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "    os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda\"\n",
    "    \n",
    "    # Helpful fallbacks if you still hit capture issues:\n",
    "    # os.environ[\"SGLANG_DISABLE_CUDA_GRAPH\"] = \"1\"      # skip CUDA graphs (degrades perf but avoids capture)\n",
    "    # os.environ[\"TRITON_CODEGEN_FATBIN\"] = \"0\"          # can reduce Triton fatbin steps on some setups\n",
    "    \n",
    "    # 4) Smoke test: ensure ptxas runs from the new location\n",
    "    print(\"ptxas ->\", subprocess.check_output([ptxas_path, \"--version\"]).decode().strip())\n",
    "    \n",
    "    # Now it's safe to import heavy libs that trigger Triton\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31826141",
   "metadata": {
    "papermill": {
     "duration": 180.299598,
     "end_time": "2025-08-21T17:24:14.100784",
     "exception": false,
     "start_time": "2025-08-21T17:21:13.801186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cleared.\n",
      "ðŸ”§ Local: Using model from Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "LOG file path: /workspace/arc-agi-2025/llm_python/submissions/sglang_server.log\n",
      "Started sglang server PID=1995 | logging to /workspace/arc-agi-2025/llm_python/submissions/sglang_server.log\n",
      "Command: /workspace/arc-agi-2025/.venv/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --dp 1 --kv-cache-dtype fp8_e4m3\n",
      "sglang not ready after timeout. Showing last 60 log lines:\n",
      "[2025-08-25 11:16:53] server_args=ServerArgs(model_path='Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542', tokenizer_path='Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8080, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='fp8_e4m3', mem_fraction_static=0.911, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=1053596693, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, served_model_name='Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='', hicache_storage_backend=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, enable_triton_kernel_moe=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3)\n",
      "[2025-08-25 11:18:20] Attention backend not explicitly specified. Use fa3 backend by default.\n",
      "[2025-08-25 11:18:20] Init torch distributed begin.\n",
      "[2025-08-25 11:18:20] Init torch distributed ends. mem usage=0.00 GB\n",
      "Call stop_server() or full_cleanup() to shut it down gracefully.\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "  # Background server launcher for Kaggle with SGLang\n",
    "  import os, sys, time, subprocess, json, socket, requests\n",
    "\n",
    "  # ---------- 1) Check for existing server and cleanup ----------\n",
    "  PORT = 8080\n",
    "  HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "\n",
    "  # Check if server already running\n",
    "  try:\n",
    "      r = requests.get(HEALTH_URL, timeout=3)\n",
    "      if r.status_code == 200:\n",
    "          print(f\"Server already running on port {PORT}. Stopping it first...\")\n",
    "          # Kill existing sglang processes\n",
    "          subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "          time.sleep(3)  # Wait for cleanup\n",
    "  except:\n",
    "      pass  # No server running\n",
    "\n",
    "  # Clear CUDA memory before starting\n",
    "  try:\n",
    "      import torch\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.synchronize()\n",
    "          print(\"CUDA memory cleared.\")\n",
    "      num_gpus = torch.cuda.device_count()\n",
    "  except Exception:\n",
    "      num_gpus = 0\n",
    "\n",
    "  if IS_KAGGLE:\n",
    "      # Find the first directory inside ARC_DATA_ROOT for model path\n",
    "      model_base_path = ARC_DATA_ROOT / \"arc-1-fake-ttt-blended-c802-dataset\"\n",
    "      subdirs = [model_base_path / d for d in os.listdir(model_base_path) if (model_base_path / d).is_dir()]\n",
    "      if not subdirs:\n",
    "          raise RuntimeError(f\"No model directory found in {model_base_path}\")\n",
    "      # Update MODEL_PATH for Kaggle environment (don't overwrite env variable)\n",
    "      model_path_to_use = str(subdirs[0])   \n",
    "      print(f\"ðŸ”§ Kaggle: Using model from {model_path_to_use}\")\n",
    "  else:\n",
    "      model_path_to_use = str(MODEL_PATH)\n",
    "      print(f\"ðŸ”§ Local: Using model from {model_path_to_use}\")\n",
    "\n",
    "  LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "  print(f\"LOG file path: {LOG}\")\n",
    "\n",
    "  SERVER_CMD = [\n",
    "      sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "      \"--host\", \"0.0.0.0\",\n",
    "      \"--port\", str(PORT),\n",
    "      \"--model-path\", model_path_to_use,\n",
    "      \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "      \"--kv-cache-dtype\", \"fp8_e4m3\"\n",
    "  ]\n",
    "\n",
    "  # ---------- 2) Launch in background ----------\n",
    "  log_f = open(LOG, \"w\")\n",
    "  env = os.environ.copy()\n",
    "  proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=SUBMIT_DIR)\n",
    "  print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "  print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "\n",
    "  # ---------- 3) Wait for readiness ----------\n",
    "  def wait_ready(url, timeout_s=180):\n",
    "      t0 = time.time()\n",
    "      while time.time() - t0 < timeout_s:\n",
    "          try:\n",
    "              r = requests.get(url, timeout=3)\n",
    "              if r.status_code == 200:\n",
    "                  return True\n",
    "          except Exception:\n",
    "              pass\n",
    "          time.sleep(2)\n",
    "      return False\n",
    "\n",
    "  ready = wait_ready(HEALTH_URL)\n",
    "  log_f.flush()\n",
    "\n",
    "  if ready:\n",
    "      print(f\"sglang is READY on port {PORT}.\")\n",
    "      print(f\"- Tail logs: !tail -n 50 {LOG}\")\n",
    "      print(f\"- List models: !curl -s http://127.0.0.1:{PORT}/v1/models | jq .\")\n",
    "  else:\n",
    "      print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "      log_f.close()\n",
    "      !tail -n 60 {LOG}\n",
    "\n",
    "  # ---------- 4) Cleanup functions ----------\n",
    "  def stop_server(p=proc):\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "      print(\"Server stopped.\")\n",
    "\n",
    "  def full_cleanup(p=proc):\n",
    "      # Stop server\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "\n",
    "      # Also kill any lingering sglang processes\n",
    "      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "      print(\"Server stopped and CUDA memory cleared.\")\n",
    "\n",
    "  print(\"Call stop_server() or full_cleanup() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9dbecc1",
   "metadata": {
    "papermill": {
     "duration": 450.048903,
     "end_time": "2025-08-21T17:31:44.152856",
     "exception": false,
     "start_time": "2025-08-21T17:24:14.103953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âŒ Connection failed - server may not be ready yet\n",
      "â³ Waiting 30 seconds before retrying...\n",
      "âœ… Server is responding!\n",
      "Available models:\n",
      "  - Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "\n",
      "âœ… Found model: Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "    import requests\n",
    "    import time\n",
    "    \n",
    "    def check_models():\n",
    "        url = \"http://127.0.0.1:8080/v1/models\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "    \n",
    "            print(\"âœ… Server is responding!\")\n",
    "            print(\"Available models:\")\n",
    "            for model in result['data']:\n",
    "                print(f\"  - {model['id']}\")\n",
    "    \n",
    "            return result['data'][0]['id'] if result['data'] else None\n",
    "    \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"âŒ Connection failed - server may not be ready yet\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Poll every 30 seconds until we get a model\n",
    "    model_name = None\n",
    "    while not model_name:\n",
    "        model_name = check_models()\n",
    "        if not model_name:\n",
    "            print(\"â³ Waiting 30 seconds before retrying...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    print(f\"\\nâœ… Found model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da71571",
   "metadata": {
    "papermill": {
     "duration": 0.012329,
     "end_time": "2025-08-21T17:31:44.169083",
     "exception": false,
     "start_time": "2025-08-21T17:31:44.156754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"âœ… Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\nâ± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"ðŸ”¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"âš¡ Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90074a7",
   "metadata": {
    "papermill": {
     "duration": 134.210705,
     "end_time": "2025-08-21T17:33:58.399018",
     "exception": false,
     "start_time": "2025-08-21T17:31:44.188313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/arc-agi-2025\n",
      "Mode: dev | attempts=1 | workers=16 | subset=evaluation\n",
      "Running command: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2024 --subset evaluation --max_workers 16 --max_attempts 1 --model Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think\n",
      "ðŸ“ Logging output to: /workspace/arc-agi-2025/llm_python/submissions/run.log\n",
      "â³ Running tasks (output being written to log file)...\n"
     ]
    }
   ],
   "source": [
    "if not IS_KAGGLE:\n",
    "    %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Derive attempts/workers for the two modes\n",
    "MAX_ATTEMPTS = ATTEMPTS if (IS_RERUN or not IS_KAGGLE) else 8\n",
    "MAX_WORKERS  = 16\n",
    "\n",
    "# SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# can use this instead if testing evaluation during a pre-run\n",
    "SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"Mode: {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "    \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--max_workers\", str(MAX_WORKERS),\n",
    "    \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "    \"--model\", model_name,\n",
    "    \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "    \"--unsafe-executor\",\n",
    "    \"--max-tokens\", \"2000\",\n",
    "    \"--qwen-no-think\"\n",
    "]\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Handle output redirection properly\n",
    "if IS_RERUN or not IS_KAGGLE:\n",
    "    # For quiet mode, redirect to file using subprocess\n",
    "    import subprocess\n",
    "    log_file_path = f\"{SUBMIT_DIR}/run.log\"\n",
    "    print(f\"ðŸ“ Logging output to: {log_file_path}\")\n",
    "    \n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        print(\"â³ Running tasks (output being written to log file)...\")\n",
    "        return_code = process.wait()\n",
    "        \n",
    "    if return_code == 0:\n",
    "        print(f\"âœ… Task runner completed successfully. Check {log_file_path} for details.\")\n",
    "    else:\n",
    "        print(f\"âŒ Task runner failed with return code {return_code}\")\n",
    "        print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "        # Show last few lines of log\n",
    "        !tail -n 20 {log_file_path}\n",
    "else:\n",
    "    # For interactive mode, show output directly\n",
    "    cmd = \" \".join(cmd_args)\n",
    "    print(f\"Running: {cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987517fc-7524-44cd-9e87-9755e64b7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Fine-tuning enabled - TTT mode detected, will fine-tune model on non-transductive programs\n",
      "ðŸ“¤ Hub push setting: ENABLED (Kaggle=False)\n",
      "ðŸ› ï¸ Fine-tuning configuration:\n",
      "   MODEL_SLUG: Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n",
      "   FINE_TUNING_MODE: final_only\n",
      "   DATA_SOURCE: parquet\n",
      "   ARC_PROGRAMS_PARQUET: /workspace/arc-agi-2025/llm_python/datasets/inference\n",
      "   MODEL_SAVE_DIR: /workspace/arc-agi-2025/llm_python/fine-tuning\n",
      "   PUSH_TO_HUB: true\n",
      "ðŸ›‘ Stopping inference server to free GPU memory for fine-tuning...\n",
      "âœ… CUDA memory cleared\n",
      "ðŸ“ Logging to: /workspace/arc-agi-2025/llm_python/submissions/fine_tuning_20250825_114222.log\n",
      "ðŸ”„ Converting notebook to script...\n",
      "âœ… Notebook converted successfully\n",
      "ðŸš€ Starting fine-tuning...\n",
      "Running command: uv run python llm_python/fine-tuning/unsloth_arc_finetuning_soar.py --config llm_python/fine-tuning/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Integration - TTT Mode Only\n",
    "# Only runs when TTT_MODE=true\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable fine-tuning only in TTT mode\n",
    "ENABLE_FINE_TUNING = TTT_MODE\n",
    "\n",
    "if ENABLE_FINE_TUNING:\n",
    "    print(\"ðŸ”¬ Fine-tuning enabled - TTT mode detected, will fine-tune model on non-transductive programs\")\n",
    "    \n",
    "    # Hub push control: Kaggle=false, Non-Kaggle=true\n",
    "    PUSH_TO_HUB = not IS_KAGGLE\n",
    "    print(f\"ðŸ“¤ Hub push setting: {'ENABLED' if PUSH_TO_HUB else 'DISABLED'} (Kaggle={IS_KAGGLE})\")\n",
    "    \n",
    "    # Set environment variables for fine-tuning\n",
    "    fine_tuning_env = {\n",
    "        'MODEL_SLUG': MODEL_PATH,              # Use the current model path as base\n",
    "        'FINE_TUNING_MODE': 'final_only',     # TTT mode uses final_only\n",
    "        'DATA_SOURCE': 'parquet',             # Load from parquet files\n",
    "        'ARC_PROGRAMS_PARQUET': str(ARC_PROGRAMS_PARQUET),  # Parquet directory path\n",
    "        'MODEL_SAVE_DIR': str(MODEL_SAVE_DIR), # Where to save fine-tuned model\n",
    "        'PUSH_TO_HUB': str(PUSH_TO_HUB).lower(),  # Hub push control\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ› ï¸ Fine-tuning configuration:\")\n",
    "    for key, value in fine_tuning_env.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "        os.environ[key] = value\n",
    "    \n",
    "    # Stop the current server to free up GPU memory\n",
    "    if 'proc' in locals():\n",
    "        print(\"ðŸ›‘ Stopping inference server to free GPU memory for fine-tuning...\")\n",
    "        try:\n",
    "            proc.terminate()\n",
    "            proc.wait(timeout=10)\n",
    "        except:\n",
    "            proc.kill()\n",
    "        \n",
    "        # Clear CUDA memory\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                print(\"âœ… CUDA memory cleared\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Ensure we're in the right directory\n",
    "    original_cwd = os.getcwd()\n",
    "    if not IS_KAGGLE:\n",
    "      os.chdir(\"/workspace/arc-agi-2025\")\n",
    "    \n",
    "    # Set up logging\n",
    "    import datetime\n",
    "    log_dir = Path(os.environ.get(\"SUBMIT_DIR\", \"logs\"))\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = log_dir / f\"fine_tuning_{timestamp}.log\"\n",
    "    \n",
    "    def log_and_print(message, log_file_handle=None):\n",
    "      \"\"\"Write to both console and log file\"\"\"\n",
    "      print(message)\n",
    "      if log_file_handle:\n",
    "          log_file_handle.write(message + \"\\n\")\n",
    "          log_file_handle.flush()\n",
    "    \n",
    "    try:\n",
    "      with open(log_file, 'w') as f:\n",
    "          log_and_print(f\"ðŸ“ Logging to: {log_file}\", f)\n",
    "    \n",
    "          if not IS_KAGGLE:\n",
    "              # Step 1: Convert notebook to script\n",
    "              log_and_print(\"ðŸ”„ Converting notebook to script...\", f)\n",
    "              convert_cmd = [\n",
    "                  \"uv\", \"run\", \"python\",\n",
    "                  \"llm_python/fine-tuning/notebook_to_script.py\",\n",
    "                  \"llm_python/fine-tuning/unsloth_arc_finetuning_soar.ipynb\"\n",
    "              ]\n",
    "    \n",
    "              convert_result = subprocess.run(convert_cmd,\n",
    "                                            capture_output=True,\n",
    "                                            text=True,\n",
    "                                            timeout=60)\n",
    "    \n",
    "              # Log full output\n",
    "              f.write(\"=== CONVERSION OUTPUT ===\\n\")\n",
    "              f.write(f\"Return code: {convert_result.returncode}\\n\")\n",
    "              f.write(f\"STDOUT:\\n{convert_result.stdout}\\n\")\n",
    "              f.write(f\"STDERR:\\n{convert_result.stderr}\\n\")\n",
    "              f.write(\"========================\\n\\n\")\n",
    "              f.flush()\n",
    "    \n",
    "              if convert_result.returncode != 0:\n",
    "                  log_and_print(f\"âŒ Notebook conversion failed: {convert_result.stderr}\", f)\n",
    "                  raise Exception(\"Notebook conversion failed\")\n",
    "    \n",
    "              log_and_print(\"âœ… Notebook converted successfully\", f)\n",
    "    \n",
    "          # Step 2: Run the actual fine-tuning\n",
    "          log_and_print(\"ðŸš€ Starting fine-tuning...\", f)\n",
    "          fine_tuning_cmd = [\n",
    "              \"uv\", \"run\", \"python\",\n",
    "              \"llm_python/fine-tuning/unsloth_arc_finetuning_soar.py\",\n",
    "              \"--config\", \"llm_python/fine-tuning/config.yaml\"\n",
    "          ]\n",
    "    \n",
    "          log_and_print(f\"Running command: {' '.join(fine_tuning_cmd)}\", f)\n",
    "    \n",
    "          # Run with real-time output\n",
    "          process = subprocess.Popen(\n",
    "              fine_tuning_cmd,\n",
    "              stdout=subprocess.PIPE,\n",
    "              stderr=subprocess.STDOUT,\n",
    "              text=True,\n",
    "              bufsize=1\n",
    "          )\n",
    "    \n",
    "          f.write(\"=== FINE-TUNING OUTPUT ===\\n\")\n",
    "          f.flush()\n",
    "    \n",
    "          # Stream output to both console and file\n",
    "          for line in process.stdout:\n",
    "              print(line, end='')  # Show in console\n",
    "              f.write(line)  # Save to file\n",
    "              f.flush()\n",
    "    \n",
    "          # Wait for completion\n",
    "          return_code = process.wait(timeout=6400)  # 2 hour timeout\n",
    "    \n",
    "          f.write(f\"\\n=== PROCESS COMPLETED WITH CODE: {return_code} ===\\n\")\n",
    "          f.flush()\n",
    "    \n",
    "          if return_code == 0:\n",
    "              log_and_print(\"âœ… Fine-tuning completed successfully!\", f)\n",
    "    \n",
    "              # Find the fine-tuned model\n",
    "              fine_tuned_models = list(Path(MODEL_SAVE_DIR).glob(\"*-final\"))\n",
    "              if fine_tuned_models:\n",
    "                  new_model_path = fine_tuned_models[0]\n",
    "                  log_and_print(f\"ðŸŽ¯ Fine-tuned model saved at: {new_model_path}\", f)\n",
    "    \n",
    "                  # Set the fine-tuned model path (don't overwrite original MODEL_PATH)\n",
    "                  global FINE_TUNED_MODEL_PATH\n",
    "                  FINE_TUNED_MODEL_PATH = str(new_model_path)\n",
    "                  log_and_print(f\"ðŸ”„ Set FINE_TUNED_MODEL_PATH: {FINE_TUNED_MODEL_PATH}\", f)\n",
    "              else:\n",
    "                  log_and_print(\"âš ï¸  Fine-tuned model not found, will use original model\", f)\n",
    "          else:\n",
    "              log_and_print(f\"âŒ Fine-tuning failed with return code {return_code}\", f)\n",
    "              log_and_print(\"ðŸ”„ Will use original model...\", f)\n",
    "    \n",
    "          log_and_print(f\"ðŸ“„ Full logs saved to: {log_file}\", f)\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Fine-tuning timed out after 2 hours, will use original model\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fine-tuning error: {e}\")\n",
    "        print(\"ðŸ”„ Will use original model...\")\n",
    "    finally:\n",
    "        # Restore original directory\n",
    "        os.chdir(original_cwd)\n",
    "    \n",
    "    # Clean up environment variables\n",
    "    for key in fine_tuning_env:\n",
    "        os.environ.pop(key, None)\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ”„ Fine-tuning disabled - TTT mode not enabled\")\n",
    "    if TTT_MODE:\n",
    "        print(\"   (TTT_MODE=true detected, but ENABLE_FINE_TUNING override disabled fine-tuning)\")\n",
    "    else:\n",
    "        print(f\"   (Set TTT_MODE=true to enable Test-Time Training workflow)\")\n",
    "    print(f\"   Will use pre-loaded model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f073968d-2d9d-4856-9e92-9042c1564475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Restarting inference server with updated model...\n",
      "âœ… CUDA memory cleared\n",
      "ðŸŽ¯ Using model: /workspace/arc-agi-2025/llm_python/fine-tuning/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final\n",
      "   â†’ Fine-tuned model\n",
      "ðŸš€ Starting server: /workspace/arc-agi-2025/.venv/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /workspace/arc-agi-2025/llm_python/fine-tuning/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-parquet-programs-final --dp 1 --kv-cache-dtype fp8_e4m3\n",
      "âœ… Server started with PID=90696\n",
      "âŒ Server failed to start properly\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_FINE_TUNING:\n",
    "  # Restart the server with the (potentially) new model\n",
    "  if START_SERVER:\n",
    "      print(\"ðŸ”„ Restarting inference server with updated model...\")\n",
    "\n",
    "      # Gracefully stop existing server if it exists\n",
    "      if 'proc' in locals() and proc.poll() is None:  # Check if process is still running\n",
    "          print(\"ðŸ›‘ Gracefully stopping existing server...\")\n",
    "          try:\n",
    "              proc.terminate()  # Send SIGTERM first\n",
    "              proc.wait(timeout=30)  # Wait up to 30 seconds for graceful shutdown\n",
    "              print(\"âœ… Server stopped gracefully\")\n",
    "          except subprocess.TimeoutExpired:\n",
    "              print(\"âš ï¸  Server didn't stop gracefully, force killing...\")\n",
    "              proc.kill()\n",
    "              proc.wait()\n",
    "          except Exception as e:\n",
    "              print(f\"âš ï¸  Error stopping server: {e}\")\n",
    "\n",
    "      # Wait a bit longer after graceful shutdown\n",
    "      time.sleep(5)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "              print(\"âœ… CUDA memory cleared\")\n",
    "      except Exception:\n",
    "          pass\n",
    "\n",
    "      # Get GPU count\n",
    "      try:\n",
    "          import torch\n",
    "          num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "      except:\n",
    "          num_gpus = 1\n",
    "\n",
    "      # Choose which model to use: fine-tuned if available, otherwise original\n",
    "      model_to_use = FINE_TUNED_MODEL_PATH if FINE_TUNED_MODEL_PATH else MODEL_PATH\n",
    "      print(f\"ðŸŽ¯ Using model: {model_to_use}\")\n",
    "      print(f\"   â†’ {'Fine-tuned' if FINE_TUNED_MODEL_PATH else 'Original'} model\")\n",
    "\n",
    "      # Restart server with appropriate model\n",
    "      PORT = 8080\n",
    "      LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "      SERVER_CMD = [\n",
    "          sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "          \"--host\", \"0.0.0.0\",\n",
    "          \"--port\", str(PORT),\n",
    "          \"--model-path\", str(model_to_use),\n",
    "          \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "          \"--kv-cache-dtype\", \"fp8_e4m3\"\n",
    "      ]\n",
    "\n",
    "      print(f\"ðŸš€ Starting server: {' '.join(SERVER_CMD)}\")\n",
    "\n",
    "      log_f = open(LOG, \"a\")\n",
    "      proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT,\n",
    "                             env=os.environ.copy(), cwd=SUBMIT_DIR)\n",
    "\n",
    "      print(f\"âœ… Server started with PID={proc.pid}\")\n",
    "\n",
    "      # Wait for readiness with better error handling\n",
    "      def wait_ready(url, timeout_s=600):\n",
    "          t0 = time.time()\n",
    "          while time.time() - t0 < timeout_s:\n",
    "              try:\n",
    "                  r = requests.get(url, timeout=5)\n",
    "                  if r.status_code == 200:\n",
    "                      return True\n",
    "              except Exception:\n",
    "                  pass\n",
    "              time.sleep(3)  # Check less frequently\n",
    "          return False\n",
    "\n",
    "      HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "      if wait_ready(HEALTH_URL):\n",
    "          print(\"âœ… Server ready!\")\n",
    "\n",
    "          # Update model_name\n",
    "          try:\n",
    "              response = requests.get(HEALTH_URL)\n",
    "              if response.status_code == 200:\n",
    "                  models = response.json()['data']\n",
    "                  if models:\n",
    "                      model_name = models[0]['id']\n",
    "                      print(f\"ðŸŽ¯ Model: {model_name}\")\n",
    "          except Exception as e:\n",
    "              print(f\"âš ï¸  Could not get model name: {e}\")\n",
    "      else:\n",
    "          print(\"âŒ Server failed to start properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5125dda3-b4df-4ba4-8a6c-72d6491403e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Running SECOND inference with fine-tuned model (TTT mode)\n",
      "/workspace/arc-agi-2025\n",
      "TTT Second Run â†’ dev | attempts=1 | workers=16 | subset=evaluation\n",
      "Running TTT second inference: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2024 --subset evaluation --max_workers 16 --max_attempts 1 --model Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think\n",
      "ðŸ“ Logging TTT second run output to: /workspace/arc-agi-2025/llm_python/submissions/run_ttt_second.log\n",
      "â³ Running TTT second inference (output being written to log file)...\n",
      "âœ… TTT second inference completed successfully. Check /workspace/arc-agi-2025/llm_python/submissions/run_ttt_second.log for details.\n"
     ]
    }
   ],
   "source": [
    "# Second Inference Run - TTT Mode Only\n",
    "# Only runs when TTT_MODE=true (after fine-tuning)\n",
    "\n",
    "if TTT_MODE:\n",
    "    print(\"ðŸ”„ Running SECOND inference with fine-tuned model (TTT mode)\")\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        %cd /workspace/arc-agi-2025\n",
    "\n",
    "    # Derive attempts/workers for the two modes\n",
    "    MAX_ATTEMPTS = ATTEMPTS if (IS_RERUN or not IS_KAGGLE) else 8\n",
    "    MAX_WORKERS  = 16\n",
    "\n",
    "    # SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "    # can use this instead if testing evaluation during a pre-run\n",
    "    SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "    # Common env for your runner\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "    print(f\"TTT Second Run â†’ {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "    # Build the command\n",
    "    cmd_args = [\n",
    "        \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--max_workers\", str(MAX_WORKERS),\n",
    "        \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "        \"--model\", model_name,\n",
    "        \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "        \"--unsafe-executor\",\n",
    "        \"--max-tokens\", \"2000\",\n",
    "        \"--qwen-no-think\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Running TTT second inference: {' '.join(cmd_args)}\")\n",
    "\n",
    "    # Handle output redirection properly\n",
    "    if IS_RERUN or not IS_KAGGLE:\n",
    "        # For quiet mode, redirect to file using subprocess\n",
    "        import subprocess\n",
    "        log_file_path = f\"{SUBMIT_DIR}/run_ttt_second.log\"\n",
    "        print(f\"ðŸ“ Logging TTT second run output to: {log_file_path}\")\n",
    "        \n",
    "        with open(log_file_path, \"w\") as log_file:\n",
    "            process = subprocess.Popen(\n",
    "                cmd_args,\n",
    "                stdout=log_file,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "                cwd=os.getcwd()\n",
    "            )\n",
    "            \n",
    "            # Wait for completion\n",
    "            print(\"â³ Running TTT second inference (output being written to log file)...\")\n",
    "            return_code = process.wait()\n",
    "            \n",
    "        if return_code == 0:\n",
    "            print(f\"âœ… TTT second inference completed successfully. Check {log_file_path} for details.\")\n",
    "        else:\n",
    "            print(f\"âŒ TTT second inference failed with return code {return_code}\")\n",
    "            print(f\"ðŸ“ Check {log_file_path} for error details\")\n",
    "            # Show last few lines of log\n",
    "            !tail -n 20 {log_file_path}\n",
    "    else:\n",
    "        # For interactive mode, show output directly\n",
    "        cmd = \" \".join(cmd_args)\n",
    "        print(f\"Running TTT second inference: {cmd}\\n\")\n",
    "        !{cmd}\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ”„ Skipping second inference (TTT_MODE=false)\")\n",
    "    print(\"   â†’ Standard mode runs first inference only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "zul48ndrmlf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Generating submission from the two most recent parquet files...\n",
      "Running submission generation: uv run python llm_python/generate_submission.py --parquet-path llm_python/datasets/inference --n-files 2 --dataset arc-prize-2024 --subset evaluation --output-dir /workspace/arc-agi-2025/llm_python/submissions --debug\n",
      "ðŸ“‚ Looking for parquet files in: llm_python/datasets/inference\n",
      "âœ… Submission generation completed successfully!\n",
      "ðŸ” Selected 2 most recent parquet files from llm_python/datasets/inference:\n",
      "  â€¢ 20250825_115258_Trelis_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2024_evaluation.parquet (modified: 2025-08-25 11:55:08)\n",
      "  â€¢ 20250825_113858_Trelis_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2024_evaluation.parquet (modified: 2025-08-25 11:41:53)\n",
      "âœ… Loaded 310 rows from llm_python/datasets/inference/20250825_115258_Trelis_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2024_evaluation.parquet\n",
      "âœ… Loaded 300 rows from llm_python/datasets/inference/20250825_113858_Trelis_Qwen3-4B_ds-arc-agi-1-partial-100-c1542_arc-prize-2024_evaluation.parquet\n",
      "ðŸ“Š Combined data: 610 total rows from 2 files\n",
      "ðŸŽ¯ Generating submission for 400 tasks from arc-prize-2024/evaluation\n",
      "âš ï¸ No attempts for task 0c786b71, using empty fallback\n",
      "âš ï¸ No attempts for task 1a6449f1, using empty fallback\n",
      "âš ï¸ No attempts for task 20818e16, using empty fallback\n",
      "âš ï¸ No attempts for task 2753e76c, using empty fallback\n",
      "âš ï¸ No attempts for task 2c0b0aff, using empty fallback\n",
      "âš ï¸ No attempts for task 2f0c5170, using empty fallback\n",
      "âš ï¸ No attempts for task 414297c0, using empty fallback\n",
      "âš ï¸ No attempts for task 4852f2fa, using empty fallback\n",
      "âš ï¸ No attempts for task 5289ad53, using empty fallback\n",
      "âš ï¸ No attempts for task 60c09cac, using empty fallback\n",
      "âš ï¸ No attempts for task 67636eac, using empty fallback\n",
      "âš ï¸ No attempts for task 72207abc, using empty fallback\n",
      "âš ï¸ No attempts for task 73ccf9c2, using empty fallback\n",
      "âš ï¸ No attempts for task 7d18a6fb, using empty fallback\n",
      "âš ï¸ No attempts for task 9356391f, using empty fallback\n",
      "âš ï¸ No attempts for task 9a4bb226, using empty fallback\n",
      "âš ï¸ No attempts for task a934301b, using empty fallback\n",
      "âš ï¸ No attempts for task ac3e2b04, using empty fallback\n",
      "âš ï¸ No attempts for task b0f4d537, using empty fallback\n",
      "âš ï¸ No attempts for task b4a43f3b, using empty fallback\n",
      "âš ï¸ No attempts for task b7999b51, using empty fallback\n",
      "âš ï¸ No attempts for task c658a4bd, using empty fallback\n",
      "âš ï¸ No attempts for task ccd554ac, using empty fallback\n",
      "âš ï¸ No attempts for task cd3c21df, using empty fallback\n",
      "âš ï¸ No attempts for task ce8d95cc, using empty fallback\n",
      "âš ï¸ No attempts for task d4c90558, using empty fallback\n",
      "âš ï¸ No attempts for task d56f2372, using empty fallback\n",
      "âš ï¸ No attempts for task dd2401ed, using empty fallback\n",
      "âš ï¸ No attempts for task e66aafb8, using empty fallback\n",
      "âš ï¸ No attempts for task ea9794b1, using empty fallback\n",
      "âš ï¸ No attempts for task f5aa3634, using empty fallback\n",
      "\n",
      "âœ… Submission files created:\n",
      "ðŸ“Š Summary:\n",
      "  Total tasks in dataset: 400\n",
      "  Tasks with predictions: 369\n",
      "  Tasks with duplicated attempts: 155\n",
      "  Tasks with empty fallback: 31\n",
      "  Official file: /workspace/arc-agi-2025/llm_python/submissions/submission.json\n",
      "  Backup file: /workspace/arc-agi-2025/llm_python/submissions/submission_arc-prize-2024_evaluation_115258_Trelis_20250825_115516.json\n",
      "â„¹ï¸ Note: Run separate validation if needed\n",
      "ðŸŽ¯ Submission generation complete: /workspace/arc-agi-2025/llm_python/submissions/submission.json\n",
      "\n",
      "ðŸ“ Submission file: /workspace/arc-agi-2025/llm_python/submissions/submission.json\n"
     ]
    }
   ],
   "source": [
    "# Generate submission using the two most recent parquet files\n",
    "if os.environ.get(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "    print(\"ðŸŽ¯ Generating submission from the two most recent parquet files...\")\n",
    "    \n",
    "    import subprocess\n",
    "    \n",
    "    # Set up paths - parquet files are saved by task runner in different locations\n",
    "    if IS_KAGGLE:\n",
    "        # On Kaggle, parquet files are saved directly in /kaggle/working by task runner\n",
    "        inference_dir = \"/kaggle/working\"\n",
    "    else:\n",
    "        # On RunPod/local, parquet files are saved in llm_python/datasets/inference\n",
    "        inference_dir = \"llm_python/datasets/inference\"\n",
    "    \n",
    "    output_dir = str(SUBMIT_DIR)\n",
    "    \n",
    "    # Command to generate submission using the two most recent parquet files\n",
    "    submission_cmd = [\n",
    "        \"uv\", \"run\", \"python\", \"llm_python/generate_submission.py\",\n",
    "        \"--parquet-path\", inference_dir,\n",
    "        \"--n-files\", \"2\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--output-dir\", output_dir,\n",
    "        \"--debug\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running submission generation: {' '.join(submission_cmd)}\")\n",
    "    print(f\"ðŸ“‚ Looking for parquet files in: {inference_dir}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            submission_cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,  # 5 minute timeout\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Submission generation completed successfully!\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Update submit_dir to point to the generated file\n",
    "            submit_dir = f\"{output_dir}/submission.json\"\n",
    "            print(f\"ðŸ“ Submission file: {submit_dir}\")\n",
    "        else:\n",
    "            print(f\"âŒ Submission generation failed with return code {result.returncode}\")\n",
    "            print(f\"STDOUT: {result.stdout}\")\n",
    "            print(f\"STDERR: {result.stderr}\")\n",
    "            # Fallback to default submission path\n",
    "            submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Submission generation timed out\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Submission generation error: {e}\")\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "else:\n",
    "    print(\"ðŸ“ Skipping submission generation (SUBMIT=false)\")\n",
    "    submit_dir = f\"{SUBMIT_DIR}/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ce7cddb",
   "metadata": {
    "papermill": {
     "duration": 0.012314,
     "end_time": "2025-08-21T17:33:58.417785",
     "exception": false,
     "start_time": "2025-08-21T17:33:58.405471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validating submission file: /workspace/arc-agi-2025/llm_python/submissions/submission.json\n",
      "\n",
      "ðŸ” VALIDATING SUBMISSION: /workspace/arc-agi-2025/llm_python/submissions/submission.json\n",
      "ðŸ“Š Validation Results:\n",
      "  Total tasks: 400\n",
      "  Total predictions: 419\n",
      "  Empty predictions ([[0,0],[0,0]]): 66\n",
      "âœ… VALIDATION PASSED - No structural errors found\n",
      "ðŸŽ¯ Submission file is ready for competition!\n",
      "ðŸ“‚ Loading submission: /workspace/arc-agi-2025/llm_python/submissions/submission.json\n",
      "ðŸ” Scoring against arc-prize-2024/evaluation\n",
      "============================================================\n",
      "SUBMISSION SCORING RESULTS\n",
      "============================================================\n",
      "Dataset: arc-prize-2024\n",
      "Subset: evaluation\n",
      "Reference tasks: 400\n",
      "Tasks scored: 400\n",
      "Total predictions: 838\n",
      "\n",
      "ðŸ“Š PREDICTION-LEVEL METRICS:\n",
      "  Pass@1 (first attempt): 14/838 (1.7%)\n",
      "  Pass@2 (either attempt): 15/838 (1.8%)\n",
      "\n",
      "ðŸ“Š TASK-LEVEL METRICS:\n",
      "  Tasks Pass@1 (all outputs correct on first attempt): 12/400 (3.0%)\n",
      "  Tasks Pass@2 (all outputs correct on either attempt): 13/400 (3.2%)\n"
     ]
    }
   ],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission {submit_dir} --dataset {DATASET} --subset {SUBSET}\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mfsmmv2kfy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up server and resources...\n",
      "Server stopped and CUDA memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup - stop server and free resources\n",
    "if START_SERVER and 'full_cleanup' in globals():\n",
    "    print(\"ðŸ§¹ Cleaning up server and resources...\")\n",
    "    full_cleanup()\n",
    "else:\n",
    "    print(\"ðŸ” No server cleanup needed (START_SERVER=False or cleanup function not available)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8063856,
     "isSourceIdPinned": true,
     "sourceId": 12829611,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 256290376,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 257352573,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "arc-agi-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 838.5501,
   "end_time": "2025-08-21T17:34:01.040075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-21T17:20:02.489975",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
