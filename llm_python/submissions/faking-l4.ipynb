{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c845d89",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.013405,
     "end_time": "2025-08-21T17:20:29.935281",
     "exception": false,
     "start_time": "2025-08-21T17:20:29.921876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Runpod/local long run ‚Äî setting FULL 17400s timeout for ARC task runner\n",
      "‚è∞ Global timeout set to 17400s (4.8 hours)\n",
      "Mode summary ‚Üí IS_KAGGLE=False | IS_RERUN=False | TEST_INFERENCE=False |\n",
      "SCORE=True | SUBMIT=true | MODEL_PATH=Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = \"Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542\" # Need a way to get this from local if needed.\n",
    "DATASET = \"arc-prize-2024\"\n",
    "\n",
    "# ---- Config flags (single source of truth) ----\n",
    "START_SERVER = True\n",
    "TEST_INFERENCE = False          # set False unless you want a quick endpoint smoke test\n",
    "SCORE = True                   # default; overridden in branches below\n",
    "\n",
    "# Env-backed flags\n",
    "IS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "IS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# String env flag for external tools\n",
    "os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# ---- Paths ----\n",
    "if IS_KAGGLE:\n",
    "    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n",
    "    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n",
    "    SUBMIT_DIR      = Path(\"/kaggle/working\")\n",
    "    ARC_PROGRAMS_DB = MODEL_SAVE_DIR / \"local.db\"\n",
    "    MODEL_PATH = ARC_DATA_ROOT / \"arc-1-fake-ttt-blended-c802-dataset\"\n",
    "else:\n",
    "    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n",
    "    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n",
    "    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n",
    "    ARC_PROGRAMS_DB = SUBMIT_DIR / \"local.db\"\n",
    "\n",
    "# Export envs for downstream processes\n",
    "os.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\n",
    "os.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\n",
    "os.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\n",
    "os.environ[\"ARC_PROGRAMS_DB\"] = str(ARC_PROGRAMS_DB)\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Timeouts & mode tweaks ----\n",
    "FULL_TIMEOUT = 3600*5 - 600 # ~5 hour timeout for inference\n",
    "\n",
    "if IS_RERUN:\n",
    "    # Kaggle competition rerun\n",
    "    timeout_seconds = FULL_TIMEOUT\n",
    "    print(f\"üèÜ Competition rerun detected ‚Äî setting FULL {timeout_seconds}s timeout for ARC task runner\")\n",
    "    TEST_INFERENCE = False\n",
    "    SCORE = False\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "elif not IS_KAGGLE:\n",
    "    # Runpod / local long run\n",
    "    timeout_seconds = FULL_TIMEOUT\n",
    "    print(f\"üñ•Ô∏è Runpod/local long run ‚Äî setting FULL {timeout_seconds}s timeout for ARC task runner\")\n",
    "    if os.getenv(\"SUBMIT\", \"false\").lower() == \"true\":\n",
    "        SCORE = True  # if we're generating a submission, do scoring\n",
    "\n",
    "else:\n",
    "    # Kaggle dev/testing\n",
    "    timeout_seconds = 60  # 1 minute\n",
    "    print(f\"üîß Development run ‚Äî setting short {timeout_seconds}s timeout for testing\")\n",
    "    # Safer default: don't auto-submit in dev\n",
    "    os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# Export timeout\n",
    "os.environ[\"GLOBAL_TIMEOUT\"] = str(timeout_seconds)\n",
    "print(f\"‚è∞ Global timeout set to {timeout_seconds}s ({timeout_seconds/3600:.1f} hours)\")\n",
    "\n",
    "# Optional: quick summary (helps avoid accidental submits)\n",
    "print(\n",
    "    \"Mode summary ‚Üí \"\n",
    "    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | TEST_INFERENCE={TEST_INFERENCE} |\\n\"\n",
    "    f\"SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | MODEL_PATH={MODEL_PATH}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504e677",
   "metadata": {
    "papermill": {
     "duration": 33.682835,
     "end_time": "2025-08-21T17:21:03.621026",
     "exception": false,
     "start_time": "2025-08-21T17:20:29.938191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30e81c",
   "metadata": {
    "papermill": {
     "duration": 9.103683,
     "end_time": "2025-08-21T17:21:12.737447",
     "exception": false,
     "start_time": "2025-08-21T17:21:03.633764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ff724",
   "metadata": {
    "papermill": {
     "duration": 1.057758,
     "end_time": "2025-08-21T17:21:13.798168",
     "exception": false,
     "start_time": "2025-08-21T17:21:12.740410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_KAGGLE:\n",
    "    # ensure that ptxas can access writable directories\n",
    "    import shutil\n",
    "    import os\n",
    "    import sys\n",
    "    import subprocess\n",
    "    \n",
    "    # Copy PTXAS and other binaries\n",
    "    os.makedirs(\"/kaggle/working/bin\", exist_ok=True)\n",
    "    for binary in [\"ptxas\", \"cuobjdump\", \"nvdisasm\"]:\n",
    "        src = f\"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin/{binary}\"  # Fixed path\n",
    "        dst = f\"/kaggle/working/bin/{binary}\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            os.chmod(dst, 0o755)\n",
    "    \n",
    "    # Set environment variables\n",
    "    env = os.environ.copy()\n",
    "    env[\"TRITON_PTXAS_PATH\"] = \"/kaggle/working/bin/ptxas\"\n",
    "    env[\"PATH\"] = f\"/kaggle/working/bin:{env.get('PATH', '')}\"\n",
    "    env[\"TRITON_CACHE_DIR\"] = \"/kaggle/working/.triton\"\n",
    "    env[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.makedirs(\"/kaggle/working/.triton\", exist_ok=True)\n",
    "    \n",
    "    # Apply the environment variables to the current process\n",
    "    os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31826141",
   "metadata": {
    "papermill": {
     "duration": 180.299598,
     "end_time": "2025-08-21T17:24:14.100784",
     "exception": false,
     "start_time": "2025-08-21T17:21:13.801186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if START_SERVER:\n",
    "  # Background server launcher for Kaggle with SGLang\n",
    "  import os, sys, time, subprocess, json, socket, requests\n",
    "\n",
    "  # ---------- 1) Check for existing server and cleanup ----------\n",
    "  PORT = 8080\n",
    "  HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "\n",
    "  # Check if server already running\n",
    "  try:\n",
    "      r = requests.get(HEALTH_URL, timeout=3)\n",
    "      if r.status_code == 200:\n",
    "          print(f\"Server already running on port {PORT}. Stopping it first...\")\n",
    "          # Kill existing sglang processes\n",
    "          subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "          time.sleep(3)  # Wait for cleanup\n",
    "  except:\n",
    "      pass  # No server running\n",
    "\n",
    "  # Clear CUDA memory before starting\n",
    "  try:\n",
    "      import torch\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.synchronize()\n",
    "          print(\"CUDA memory cleared.\")\n",
    "      num_gpus = torch.cuda.device_count()\n",
    "  except Exception:\n",
    "      num_gpus = 0\n",
    "\n",
    "  if IS_KAGGLE:\n",
    "      # Find the first directory inside ARC_DATA_ROOT for model path\n",
    "      model_base_path = ARC_DATA_ROOT / \"arc-1-fake-ttt-blended-c802-dataset\"\n",
    "      subdirs = [model_base_path / d for d in os.listdir(model_base_path) if (model_base_path / d).is_dir()]\n",
    "      if not subdirs:\n",
    "          raise RuntimeError(f\"No model directory found in {model_base_path}\")\n",
    "      MODEL_PATH = str(subdirs[0])   # or pick max(subdirs) if multiple exist\n",
    "\n",
    "  LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "  print(f\"LOG file path: {LOG}\")\n",
    "\n",
    "  SERVER_CMD = [\n",
    "      sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "      \"--host\", \"0.0.0.0\",\n",
    "      \"--port\", str(PORT),\n",
    "      \"--model-path\", MODEL_PATH,\n",
    "      \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "      \"--kv-cache-dtype\", \"fp8_e4m3\"\n",
    "  ]\n",
    "\n",
    "  # ---------- 2) Launch in background ----------\n",
    "  log_f = open(LOG, \"w\")\n",
    "  env = os.environ.copy()\n",
    "  proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=SUBMIT_DIR)\n",
    "  print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "  print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "\n",
    "  # ---------- 3) Wait for readiness ----------\n",
    "  def wait_ready(url, timeout_s=180):\n",
    "      t0 = time.time()\n",
    "      while time.time() - t0 < timeout_s:\n",
    "          try:\n",
    "              r = requests.get(url, timeout=3)\n",
    "              if r.status_code == 200:\n",
    "                  return True\n",
    "          except Exception:\n",
    "              pass\n",
    "          time.sleep(2)\n",
    "      return False\n",
    "\n",
    "  ready = wait_ready(HEALTH_URL)\n",
    "  log_f.flush()\n",
    "\n",
    "  if ready:\n",
    "      print(f\"sglang is READY on port {PORT}.\")\n",
    "      print(f\"- Tail logs: !tail -n 50 {LOG}\")\n",
    "      print(f\"- List models: !curl -s http://127.0.0.1:{PORT}/v1/models | jq .\")\n",
    "  else:\n",
    "      print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "      log_f.close()\n",
    "      !tail -n 60 {LOG}\n",
    "\n",
    "  # ---------- 4) Cleanup functions ----------\n",
    "  def stop_server(p=proc):\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "      print(\"Server stopped.\")\n",
    "\n",
    "  def full_cleanup(p=proc):\n",
    "      # Stop server\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "\n",
    "      # Also kill any lingering sglang processes\n",
    "      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "      print(\"Server stopped and CUDA memory cleared.\")\n",
    "\n",
    "  print(\"Call stop_server() or full_cleanup() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbecc1",
   "metadata": {
    "papermill": {
     "duration": 450.048903,
     "end_time": "2025-08-21T17:31:44.152856",
     "exception": false,
     "start_time": "2025-08-21T17:24:14.103953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if START_SERVER:\n",
    "    import requests\n",
    "    import time\n",
    "    \n",
    "    def check_models():\n",
    "        url = \"http://127.0.0.1:8080/v1/models\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "    \n",
    "            print(\"‚úÖ Server is responding!\")\n",
    "            print(\"Available models:\")\n",
    "            for model in result['data']:\n",
    "                print(f\"  - {model['id']}\")\n",
    "    \n",
    "            return result['data'][0]['id'] if result['data'] else None\n",
    "    \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"‚ùå Connection failed - server may not be ready yet\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Poll every 30 seconds until we get a model\n",
    "    model_name = None\n",
    "    while not model_name:\n",
    "        model_name = check_models()\n",
    "        if not model_name:\n",
    "            print(\"‚è≥ Waiting 30 seconds before retrying...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Found model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da71571",
   "metadata": {
    "papermill": {
     "duration": 0.012329,
     "end_time": "2025-08-21T17:31:44.169083",
     "exception": false,
     "start_time": "2025-08-21T17:31:44.156754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    url = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10\\u00d710 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"‚úÖ Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\n‚è± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"üî¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"‚ö° Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90074a7",
   "metadata": {
    "papermill": {
     "duration": 134.210705,
     "end_time": "2025-08-21T17:33:58.399018",
     "exception": false,
     "start_time": "2025-08-21T17:31:44.188313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_KAGGLE:\n",
    "    %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Derive attempts/workers for the two modes\n",
    "MAX_ATTEMPTS = 1 if (IS_RERUN or not IS_KAGGLE) else 8\n",
    "MAX_WORKERS  = 16\n",
    "\n",
    "# SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# can use this instead if testing evaluation during a pre-run\n",
    "SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"Mode: {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "    \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--max_workers\", str(MAX_WORKERS),\n",
    "    \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "    \"--model\", model_name,\n",
    "    \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "    \"--unsafe-executor\",\n",
    "    \"--max-tokens\", \"2000\",\n",
    "    \"--qwen-no-think\"\n",
    "]\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Handle output redirection properly\n",
    "if IS_RERUN or not IS_KAGGLE:\n",
    "    # For quiet mode, redirect to file using subprocess\n",
    "    import subprocess\n",
    "    log_file_path = f\"{SUBMIT_DIR}/run.log\"\n",
    "    print(f\"üìù Logging output to: {log_file_path}\")\n",
    "    \n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        print(\"‚è≥ Running tasks (output being written to log file)...\")\n",
    "        return_code = process.wait()\n",
    "        \n",
    "    if return_code == 0:\n",
    "        print(f\"‚úÖ Task runner completed successfully. Check {log_file_path} for details.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Task runner failed with return code {return_code}\")\n",
    "        print(f\"üìù Check {log_file_path} for error details\")\n",
    "        # Show last few lines of log\n",
    "        !tail -n 20 {log_file_path}\n",
    "else:\n",
    "    # For interactive mode, show output directly\n",
    "    cmd = \" \".join(cmd_args)\n",
    "    print(f\"Running: {cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987517fc-7524-44cd-9e87-9755e64b7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Integration (Optional)\n",
    "# Set ENABLE_FINE_TUNING=false to disable fine-tuning on non-transductive programs before running tasks\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "ENABLE_FINE_TUNING = os.getenv(\"ENABLE_FINE_TUNING\", \"true\").lower() == \"true\"\n",
    "\n",
    "if ENABLE_FINE_TUNING:\n",
    "    print(\"üî¨ Fine-tuning enabled - will fine-tune model on non-transductive programs\")\n",
    "    \n",
    "    # Set environment variables for fine-tuning\n",
    "    fine_tuning_env = {\n",
    "        'MODEL_SLUG': MODEL_PATH,              # Use the current model path as base\n",
    "        'FINE_TUNING_MODE': 'final_only',     # Only save final merged model\n",
    "        'DATA_SOURCE': 'database',            # Load from database\n",
    "        'ARC_PROGRAMS_DB': str(ARC_PROGRAMS_DB),  # Database path\n",
    "        'MODEL_SAVE_DIR': str(MODEL_SAVE_DIR), # Where to save fine-tuned model\n",
    "    }\n",
    "    \n",
    "    print(\"üõ†Ô∏è Fine-tuning configuration:\")\n",
    "    for key, value in fine_tuning_env.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "        os.environ[key] = value\n",
    "    \n",
    "    # Stop the current server to free up GPU memory\n",
    "    if 'proc' in locals():\n",
    "        print(\"üõë Stopping inference server to free GPU memory for fine-tuning...\")\n",
    "        try:\n",
    "            proc.terminate()\n",
    "            proc.wait(timeout=10)\n",
    "        except:\n",
    "            proc.kill()\n",
    "        \n",
    "        # Clear CUDA memory\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                print(\"‚úÖ CUDA memory cleared\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Ensure we're in the right directory\n",
    "    original_cwd = os.getcwd()\n",
    "    if not IS_KAGGLE:\n",
    "      os.chdir(\"/workspace/arc-agi-2025\")\n",
    "    \n",
    "    # Set up logging\n",
    "    import datetime\n",
    "    log_dir = Path(os.environ.get(\"SUBMIT_DIR\", \"logs\"))\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = log_dir / f\"fine_tuning_{timestamp}.log\"\n",
    "    \n",
    "    def log_and_print(message, log_file_handle=None):\n",
    "      \"\"\"Write to both console and log file\"\"\"\n",
    "      print(message)\n",
    "      if log_file_handle:\n",
    "          log_file_handle.write(message + \"\\n\")\n",
    "          log_file_handle.flush()\n",
    "    \n",
    "    try:\n",
    "      with open(log_file, 'w') as f:\n",
    "          log_and_print(f\"üìù Logging to: {log_file}\", f)\n",
    "    \n",
    "          if not IS_KAGGLE:\n",
    "              # Step 1: Convert notebook to script\n",
    "              log_and_print(\"üîÑ Converting notebook to script...\", f)\n",
    "              convert_cmd = [\n",
    "                  \"uv\", \"run\", \"python\",\n",
    "                  \"llm_python/fine-tuning/notebook_to_script.py\",\n",
    "                  \"llm_python/fine-tuning/unsloth_arc_finetuning_soar.ipynb\"\n",
    "              ]\n",
    "    \n",
    "              convert_result = subprocess.run(convert_cmd,\n",
    "                                            capture_output=True,\n",
    "                                            text=True,\n",
    "                                            timeout=60)\n",
    "    \n",
    "              # Log full output\n",
    "              f.write(\"=== CONVERSION OUTPUT ===\\n\")\n",
    "              f.write(f\"Return code: {convert_result.returncode}\\n\")\n",
    "              f.write(f\"STDOUT:\\n{convert_result.stdout}\\n\")\n",
    "              f.write(f\"STDERR:\\n{convert_result.stderr}\\n\")\n",
    "              f.write(\"========================\\n\\n\")\n",
    "              f.flush()\n",
    "    \n",
    "              if convert_result.returncode != 0:\n",
    "                  log_and_print(f\"‚ùå Notebook conversion failed: {convert_result.stderr}\", f)\n",
    "                  raise Exception(\"Notebook conversion failed\")\n",
    "    \n",
    "              log_and_print(\"‚úÖ Notebook converted successfully\", f)\n",
    "    \n",
    "          # Step 2: Run the actual fine-tuning\n",
    "          log_and_print(\"üöÄ Starting fine-tuning...\", f)\n",
    "          fine_tuning_cmd = [\n",
    "              \"uv\", \"run\", \"python\",\n",
    "              \"llm_python/fine-tuning/unsloth_arc_finetuning_soar.py\",\n",
    "              \"--config\", \"llm_python/fine-tuning/config.yaml\"\n",
    "          ]\n",
    "    \n",
    "          log_and_print(f\"Running command: {' '.join(fine_tuning_cmd)}\", f)\n",
    "    \n",
    "          # Run with real-time output\n",
    "          process = subprocess.Popen(\n",
    "              fine_tuning_cmd,\n",
    "              stdout=subprocess.PIPE,\n",
    "              stderr=subprocess.STDOUT,\n",
    "              text=True,\n",
    "              bufsize=1\n",
    "          )\n",
    "    \n",
    "          f.write(\"=== FINE-TUNING OUTPUT ===\\n\")\n",
    "          f.flush()\n",
    "    \n",
    "          # Stream output to both console and file\n",
    "          for line in process.stdout:\n",
    "              print(line, end='')  # Show in console\n",
    "              f.write(line)  # Save to file\n",
    "              f.flush()\n",
    "    \n",
    "          # Wait for completion\n",
    "          return_code = process.wait(timeout=6400)  # 2 hour timeout\n",
    "    \n",
    "          f.write(f\"\\n=== PROCESS COMPLETED WITH CODE: {return_code} ===\\n\")\n",
    "          f.flush()\n",
    "    \n",
    "          if return_code == 0:\n",
    "              log_and_print(\"‚úÖ Fine-tuning completed successfully!\", f)\n",
    "    \n",
    "              # Find the fine-tuned model\n",
    "              fine_tuned_models = list(Path(MODEL_SAVE_DIR).glob(\"*-final\"))\n",
    "              if fine_tuned_models:\n",
    "                  new_model_path = fine_tuned_models[0]\n",
    "                  log_and_print(f\"üéØ Fine-tuned model saved at: {new_model_path}\", f)\n",
    "    \n",
    "                  # Update MODEL_PATH to use the fine-tuned model\n",
    "                  MODEL_PATH = str(new_model_path)\n",
    "                  log_and_print(f\"üîÑ Updated MODEL_PATH to use fine-tuned model: {MODEL_PATH}\", f)\n",
    "              else:\n",
    "                  log_and_print(\"‚ö†Ô∏è  Fine-tuned model not found, continuing with original model\", f)\n",
    "          else:\n",
    "              log_and_print(f\"‚ùå Fine-tuning failed with return code {return_code}\", f)\n",
    "              log_and_print(\"üîÑ Continuing with original model...\", f)\n",
    "    \n",
    "          log_and_print(f\"üìÑ Full logs saved to: {log_file}\", f)\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è±Ô∏è Fine-tuning timed out after 2 hours, continuing with original model\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fine-tuning error: {e}\")\n",
    "        print(\"üîÑ Continuing with original model...\")\n",
    "    finally:\n",
    "        # Restore original directory\n",
    "        os.chdir(original_cwd)\n",
    "    \n",
    "    # Clean up environment variables\n",
    "    for key in fine_tuning_env:\n",
    "        os.environ.pop(key, None)\n",
    "    \n",
    "else:\n",
    "    print(\"üî¨ Fine-tuning disabled (set ENABLE_FINE_TUNING=true to enable)\")\n",
    "    print(f\"   Will use pre-loaded model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f073968d-2d9d-4856-9e92-9042c1564475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Restarting inference server with updated model...\n",
      "CUDA memory cleared.\n",
      "Restarting with command: /workspace/arc-agi-2025/.venv/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /workspace/arc-agi-2025/llm_python/fine-tuning/Qwen3-4B_ds-arc-agi-1-partial-100-c1542_ds-database-programs-final --dp 1 --kv-cache-dtype fp8_e4m3\n",
      "üîÑ Restarted server with PID=365662\n",
      "‚ùå Failed to restart server\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_FINE_TUNING:\n",
    "    # Restart the server with the (potentially) new model\n",
    "    if START_SERVER:\n",
    "        print(\"üîÑ Restarting inference server with updated model...\")\n",
    "        \n",
    "        # Clear any existing processes\n",
    "        subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Clear CUDA memory before starting\n",
    "        try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "              print(\"CUDA memory cleared.\")\n",
    "          num_gpus = torch.cuda.device_count()\n",
    "        except Exception:\n",
    "          num_gpus = 0\n",
    "        \n",
    "        # Get variables from earlier cells\n",
    "        PORT = 8080\n",
    "        try:\n",
    "            import torch\n",
    "            num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "        except:\n",
    "            num_gpus = 1\n",
    "        \n",
    "        # Restart server with new model\n",
    "        LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "        SERVER_CMD = [\n",
    "            sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "            \"--host\", \"0.0.0.0\",\n",
    "            \"--port\", str(PORT),\n",
    "            \"--model-path\", MODEL_PATH,\n",
    "            \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "            \"--kv-cache-dtype\", \"fp8_e4m3\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Restarting with command: {' '.join(SERVER_CMD)}\")\n",
    "        \n",
    "        log_f = open(LOG, \"a\")  # Append to existing log\n",
    "        proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, \n",
    "                               env=os.environ.copy(), cwd=SUBMIT_DIR)\n",
    "        \n",
    "        print(f\"üîÑ Restarted server with PID={proc.pid}\")\n",
    "        \n",
    "        # Wait for readiness\n",
    "        def wait_ready(url, timeout_s=180):\n",
    "            t0 = time.time()\n",
    "            while time.time() - t0 < timeout_s:\n",
    "                try:\n",
    "                    r = requests.get(url, timeout=3)\n",
    "                    if r.status_code == 200:\n",
    "                        return True\n",
    "                except Exception:\n",
    "                    pass\n",
    "                time.sleep(2)\n",
    "            return False\n",
    "        \n",
    "        HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "        if wait_ready(HEALTH_URL):\n",
    "            print(\"‚úÖ Server restarted successfully with fine-tuned model!\")\n",
    "            \n",
    "            # Update model_name\n",
    "            try:\n",
    "                response = requests.get(HEALTH_URL)\n",
    "                if response.status_code == 200:\n",
    "                    models = response.json()['data']\n",
    "                    if models:\n",
    "                        model_name = models[0]['id']\n",
    "                        print(f\"üéØ Updated model name: {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not update model name: {e}\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to restart server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5125dda3-b4df-4ba4-8a6c-72d6491403e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/arc-agi-2025\n",
      "Mode: dev | attempts=1 | workers=16 | subset=evaluation\n",
      "Running command: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2024 --subset evaluation --max_workers 16 --max_attempts 1 --model Trelis/Qwen3-4B_ds-arc-agi-1-partial-100-c1542 --base-url http://127.0.0.1:8080/v1 --unsafe-executor --max-tokens 2000 --qwen-no-think\n",
      "üìù Logging output to: /workspace/arc-agi-2025/llm_python/submissions/run.log\n",
      "‚è≥ Running tasks (output being written to log file)...\n",
      "‚úÖ Task runner completed successfully. Check /workspace/arc-agi-2025/llm_python/submissions/run.log for details.\n"
     ]
    }
   ],
   "source": [
    "if not IS_KAGGLE:\n",
    "    %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Derive attempts/workers for the two modes\n",
    "MAX_ATTEMPTS = 8 if (IS_RERUN or not IS_KAGGLE) else 8\n",
    "MAX_WORKERS  = 16\n",
    "\n",
    "# SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# can use this instead if testing evaluation during a pre-run\n",
    "SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "# Common env for your runner\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "print(f\"Mode: {'competition' if IS_RERUN else 'dev'} | attempts={MAX_ATTEMPTS} | workers={MAX_WORKERS} | subset={SUBSET}\")\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "    \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--max_workers\", str(MAX_WORKERS),\n",
    "    \"--max_attempts\", str(MAX_ATTEMPTS),\n",
    "    \"--model\", model_name,\n",
    "    \"--base-url\", \"http://127.0.0.1:8080/v1\",\n",
    "    \"--unsafe-executor\",\n",
    "    \"--max-tokens\", \"2000\",\n",
    "    \"--qwen-no-think\"\n",
    "]\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Handle output redirection properly\n",
    "if IS_RERUN or not IS_KAGGLE:\n",
    "    # For quiet mode, redirect to file using subprocess\n",
    "    import subprocess\n",
    "    log_file_path = f\"{SUBMIT_DIR}/run.log\"\n",
    "    print(f\"üìù Logging output to: {log_file_path}\")\n",
    "    \n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Wait for completion\n",
    "        print(\"‚è≥ Running tasks (output being written to log file)...\")\n",
    "        return_code = process.wait()\n",
    "        \n",
    "    if return_code == 0:\n",
    "        print(f\"‚úÖ Task runner completed successfully. Check {log_file_path} for details.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Task runner failed with return code {return_code}\")\n",
    "        print(f\"üìù Check {log_file_path} for error details\")\n",
    "        # Show last few lines of log\n",
    "        !tail -n 20 {log_file_path}\n",
    "else:\n",
    "    # For interactive mode, show output directly\n",
    "    cmd = \" \".join(cmd_args)\n",
    "    print(f\"Running: {cmd}\\n\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce7cddb",
   "metadata": {
    "papermill": {
     "duration": 0.012314,
     "end_time": "2025-08-21T17:33:58.417785",
     "exception": false,
     "start_time": "2025-08-21T17:33:58.405471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Submission file not found: /kaggle/working/submission.json\n"
     ]
    }
   ],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission \"/kaggle/working/submission.json\"\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b25953-8192-4c45-b705-7799a6fcabf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfsmmv2kfy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup - stop server and free resources\n",
    "if START_SERVER and 'full_cleanup' in globals():\n",
    "    print(\"üßπ Cleaning up server and resources...\")\n",
    "    full_cleanup()\n",
    "else:\n",
    "    print(\"üîç No server cleanup needed (START_SERVER=False or cleanup function not available)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8063856,
     "isSourceIdPinned": true,
     "sourceId": 12829611,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 256290376,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 257352573,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "arc-agi-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 838.5501,
   "end_time": "2025-08-21T17:34:01.040075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-21T17:20:02.489975",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
