{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c913ef65",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.037384,
     "end_time": "2025-09-14T19:17:30.040804",
     "exception": false,
     "start_time": "2025-09-14T19:17:30.003420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Local/RunPod model path:\n",
      "   Model: Trelis/Soar-qwen-14b-FP8-Dynamic\n",
      "üñ•Ô∏è Production mode (RunPod/local environment)\n",
      "üß™ ENABLE_REFINEMENT ENABLED\n",
      "   ‚Üí Will run: Sampling ‚Üí Refinement\n",
      "   ‚Üí Sampling: 256 attempts, 256 workers, 18000s timeout\n",
      "   ‚Üí Refinement: 256 attempts, 256 workers, 18000s timeout\n",
      "üöÄ Will start local server (no custom endpoint specified)\n",
      "Mode summary ‚Üí IS_KAGGLE=False | IS_RERUN=False | DEV_RUN=False | ENABLE_REFINEMENT=True |\n",
      "TEST_INFERENCE=True | SCORE=True | SUBMIT=false | MODEL=Trelis/Soar-qwen-14b-FP8-Dynamic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "## Notes for OpenRouter\n",
    "# You can run with OpenRouter but must first - it's easiest to set these in Runpod on startup (before kernel startup):\n",
    "# export CUSTOM_ENDPOINT=\"https://openrouter.ai\"\n",
    "# export DEV_RUN=\"true\"\n",
    "# export OPENAI_API_KEY=\"key-here\"\n",
    "\n",
    "# ============================================================================\n",
    "# Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Single model for both initial inference and refinement\n",
    "MODEL_HF = \"Trelis/Soar-qwen-14b-FP8-Dynamic\"  # For local/RunPod\n",
    "MODEL_KAGGLE = \"arc-1-fake-ttt-blended-c802-dataset\"  # Kaggle dataset name\n",
    "\n",
    "# ============================================================================\n",
    "# Inference Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Sampling attempts (initial inference)\n",
    "SAMPLING_ATTEMPTS = 256     # Number of attempts for sampling phase\n",
    "# Refinement attempts (second inference)\n",
    "REFINEMENT_ATTEMPTS = 256    # Number of attempts for refinement phase\n",
    "\n",
    "# Global workers setting\n",
    "MAX_WORKERS = 256            # Number of workers for all inference phases\n",
    "\n",
    "# ============================================================================\n",
    "# Other Configuration\n",
    "# ============================================================================\n",
    "\n",
    "DATASET = \"arc-prize-2025\"\n",
    "\n",
    "# ---- Config flags (single source of truth) ----\n",
    "# Check for custom endpoint first - if set, disable server startup and use provided endpoint\n",
    "CUSTOM_ENDPOINT = os.getenv(\"CUSTOM_ENDPOINT\")\n",
    "START_SERVER = not bool(CUSTOM_ENDPOINT)  # Don't start server if custom endpoint provided\n",
    "\n",
    "TEST_INFERENCE = True\n",
    "SCORE = True                   # default; overridden below, depending on flags\n",
    "\n",
    "# DEV_RUN can be explicitly set via environment variable, or auto-detected for Kaggle non-rerun\n",
    "DEV_RUN = os.getenv(\"DEV_RUN\", \"\").lower() == \"true\"\n",
    "\n",
    "PRODUCTION_TIMEOUT = 5 * 3600\n",
    "DEV_TIMEOUT = 120\n",
    "\n",
    "# Refinement mode?\n",
    "ENABLE_REFINEMENT=True\n",
    "\n",
    "# Env-backed flags\n",
    "IS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "IS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n",
    "\n",
    "# Auto-detect dev mode for Kaggle non-rerun environments (unless explicitly overridden)\n",
    "if IS_KAGGLE and not IS_RERUN and not os.getenv(\"DEV_RUN\"):\n",
    "    DEV_RUN = True\n",
    "    print(\"üîß Auto-detected Kaggle development environment (non-rerun)\")\n",
    "\n",
    "# # String env flag for external tools\n",
    "# os.environ[\"SUBMIT\"] = \"true\"\n",
    "\n",
    "# Ensure client key is set\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"EMPTY\")\n",
    "\n",
    "# ---- Paths ----\n",
    "if IS_KAGGLE:\n",
    "    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n",
    "    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n",
    "    SUBMIT_DIR      = Path(\"/kaggle/working\")\n",
    "    ARC_PROGRAMS_PARQUET = SUBMIT_DIR\n",
    "\n",
    "    print(\"üîç Searching for models in Kaggle environment...\")\n",
    "\n",
    "    # Auto-find model path in Kaggle's dataset structure\n",
    "    model_dataset_path = ARC_DATA_ROOT / MODEL_KAGGLE\n",
    "    print(f\"   Looking for model dataset: {model_dataset_path}\")\n",
    "\n",
    "    if model_dataset_path.exists() and model_dataset_path.is_dir():\n",
    "        # Kaggle datasets have version folders, find the first subdirectory\n",
    "        subdirs = [d for d in model_dataset_path.iterdir() if d.is_dir()]\n",
    "        if subdirs:\n",
    "            MODEL_PATH = subdirs[0]  # Use the first (usually only) version folder\n",
    "            print(f\"   ‚úÖ Found model at: {MODEL_PATH}\")\n",
    "            # List what's inside to confirm it's right\n",
    "            model_contents = list(MODEL_PATH.iterdir())[:5]  # Show first 5 items\n",
    "            print(f\"      Contents: {[f.name for f in model_contents]}\")\n",
    "        else:\n",
    "            # Fallback if no subdirectory found\n",
    "            MODEL_PATH = model_dataset_path\n",
    "            print(f\"   ‚ö†Ô∏è No version folder found for model, using: {MODEL_PATH}\")\n",
    "    else:\n",
    "        MODEL_PATH = model_dataset_path\n",
    "        print(f\"   ‚ùå Model dataset not found at: {MODEL_PATH}\")\n",
    "        print(f\"      Available datasets: {[d.name for d in ARC_DATA_ROOT.iterdir() if d.is_dir()][:10]}\")\n",
    "\n",
    "    print(f\"\\nüì¶ Final model path:\")\n",
    "    print(f\"   Model: {MODEL_PATH}\")\n",
    "\n",
    "else:\n",
    "    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n",
    "    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n",
    "    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n",
    "    ARC_PROGRAMS_PARQUET = Path(\"/workspace/arc-agi-2025/llm_python/datasets/inference\")\n",
    "\n",
    "    # Use local/RunPod model path\n",
    "    MODEL_PATH = MODEL_HF\n",
    "\n",
    "    print(f\"üì¶ Local/RunPod model path:\")\n",
    "    print(f\"   Model: {MODEL_PATH}\")\n",
    "\n",
    "# Set up paths - parquet files are saved by task runner in different locations\n",
    "if IS_KAGGLE:\n",
    "    # On Kaggle, parquet files are saved directly in /kaggle/working by task runner\n",
    "    inference_dir = \"/kaggle/working\"\n",
    "else:\n",
    "    # On RunPod/local, parquet files are saved in llm_python/datasets/inference\n",
    "    inference_dir = \"llm_python/datasets/inference\"\n",
    "\n",
    "# Export envs for downstream processes\n",
    "os.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\n",
    "os.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\n",
    "os.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\n",
    "os.environ[\"ARC_PROGRAMS_PARQUET\"] = str(ARC_PROGRAMS_PARQUET)\n",
    "os.environ[\"MODEL_PATH\"] = str(MODEL_PATH)\n",
    "\n",
    "# Export config flags for subprocess use\n",
    "os.environ[\"IS_KAGGLE\"] = str(IS_KAGGLE).lower()\n",
    "os.environ[\"IS_RERUN\"] = str(IS_RERUN).lower()\n",
    "os.environ[\"DATASET\"] = DATASET\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure based on environment\n",
    "if IS_RERUN:\n",
    "    # Kaggle competition rerun - use full configuration\n",
    "    print(f\"üèÜ Competition rerun detected\")\n",
    "    SCORE = False\n",
    "    os.environ[\"SUBMIT\"] = \"true\" # means we don't have test information available for task-wise scoring\n",
    "    # Full timeouts for competition\n",
    "    SAMPLING_TIMEOUT = PRODUCTION_TIMEOUT\n",
    "    REFINEMENT_TIMEOUT = PRODUCTION_TIMEOUT\n",
    "elif DEV_RUN:\n",
    "    # Development mode - use reduced timeouts for faster testing\n",
    "    print(f\"üîß Development mode enabled - applying shorter timeouts\")\n",
    "    SCORE = True  # if we're generating a submission, do scoring\n",
    "    os.environ[\"SUBMIT\"] = \"true\" # means we don't have test information available for task-wise scoring. Mimic Kaggle environment.\n",
    "    # Short timeouts for dev testing\n",
    "    SAMPLING_TIMEOUT = DEV_TIMEOUT\n",
    "    REFINEMENT_TIMEOUT = DEV_TIMEOUT\n",
    "else:\n",
    "    # Production mode (RunPod/local long run) - use full configuration\n",
    "    print(f\"üñ•Ô∏è Production mode (RunPod/local environment)\")\n",
    "    SCORE = True  # if we're generating a submission, do scoring\n",
    "    os.environ[\"SUBMIT\"] = \"false\" # means we will use task test outputs to score row by row\n",
    "    # Full timeouts for production runs\n",
    "    SAMPLING_TIMEOUT = PRODUCTION_TIMEOUT\n",
    "    REFINEMENT_TIMEOUT = PRODUCTION_TIMEOUT\n",
    "\n",
    "# ENABLE_REFINEMENT Mode configuration\n",
    "if ENABLE_REFINEMENT:\n",
    "    print(\"üß™ ENABLE_REFINEMENT ENABLED\")\n",
    "    print(\"   ‚Üí Will run: Sampling ‚Üí Refinement\")\n",
    "    print(f\"   ‚Üí Sampling: {SAMPLING_ATTEMPTS} attempts, {MAX_WORKERS} workers, {SAMPLING_TIMEOUT}s timeout\")\n",
    "    print(f\"   ‚Üí Refinement: {REFINEMENT_ATTEMPTS} attempts, {MAX_WORKERS} workers, {REFINEMENT_TIMEOUT}s timeout\")\n",
    "else:\n",
    "    print(\"üîÑ Standard mode (ENABLE_REFINEMENT disabled)\")\n",
    "    print(f\"   ‚Üí Will run: Sampling only ({SAMPLING_ATTEMPTS} attempts, {MAX_WORKERS} workers, {SAMPLING_TIMEOUT}s timeout)\")\n",
    "\n",
    "# Custom endpoint configuration\n",
    "if CUSTOM_ENDPOINT:\n",
    "    model_name = MODEL_HF\n",
    "    print(f\"üîó Using custom endpoint: {CUSTOM_ENDPOINT}\")\n",
    "    print(\"   ‚Üí Server startup disabled\")\n",
    "else:\n",
    "    print(\"üöÄ Will start local server (no custom endpoint specified)\")\n",
    "\n",
    "# Optional: quick summary (helps avoid accidental submits)\n",
    "print(\n",
    "    \"Mode summary ‚Üí \"\n",
    "    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | DEV_RUN={DEV_RUN} | ENABLE_REFINEMENT={ENABLE_REFINEMENT} |\\n\"\n",
    "    f\"TEST_INFERENCE={TEST_INFERENCE} | SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | MODEL={MODEL_PATH}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b850b6de",
   "metadata": {
    "papermill": {
     "duration": 12.898677,
     "end_time": "2025-09-14T19:17:42.942988",
     "exception": false,
     "start_time": "2025-09-14T19:17:30.044311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA version (PyTorch): 12.8\n",
      "CUDA available: True\n",
      "NumPy version: 2.2.0\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eadcf6c",
   "metadata": {
    "papermill": {
     "duration": 3.554569,
     "end_time": "2025-09-14T19:17:46.501106",
     "exception": false,
     "start_time": "2025-09-14T19:17:42.946537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang version: 0.5.1.post2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 11:18:57.957000 24756 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0917 11:18:57.957000 24756 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashInfer version: 0.2.14.post1\n"
     ]
    }
   ],
   "source": [
    "import sglang\n",
    "print(\"SGLang version:\", sglang.__version__)\n",
    "\n",
    "try:\n",
    "    import flashinfer\n",
    "    print(\"FlashInfer version:\", flashinfer.__version__)\n",
    "except ImportError:\n",
    "    print(\"FlashInfer not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6fe90b",
   "metadata": {
    "papermill": {
     "duration": 0.723513,
     "end_time": "2025-09-14T19:17:47.228354",
     "exception": false,
     "start_time": "2025-09-14T19:17:46.504841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_KAGGLE:\n",
    "    import os, shutil, subprocess, stat\n",
    "    \n",
    "    # 1) Where to place binaries + cache\n",
    "    WRK_BIN = \"/kaggle/working/bin\"\n",
    "    TRITON_CACHE = \"/kaggle/working/.triton\"\n",
    "    os.makedirs(WRK_BIN, exist_ok=True)\n",
    "    os.makedirs(TRITON_CACHE, exist_ok=True)\n",
    "    \n",
    "    # 2) Preferred source for ptxas/cuobjdump/nvdisasm\n",
    "    SYSTEM_CUDA_BIN = \"/usr/local/cuda/bin\"\n",
    "    FALLBACK_VENDORED = \"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin\"  # if you have it\n",
    "    \n",
    "    def copy_tool(name: str):\n",
    "        for src_dir in (SYSTEM_CUDA_BIN, FALLBACK_VENDORED):\n",
    "            src = os.path.join(src_dir, name)\n",
    "            if os.path.exists(src):\n",
    "                dst = os.path.join(WRK_BIN, name)\n",
    "                shutil.copy2(src, dst)\n",
    "                # ensure executable bit\n",
    "                os.chmod(dst, os.stat(dst).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "                return dst\n",
    "        raise FileNotFoundError(f\"Could not find {name} in {SYSTEM_CUDA_BIN} or {FALLBACK_VENDORED}\")\n",
    "    \n",
    "    ptxas_path = copy_tool(\"ptxas\")\n",
    "    try:\n",
    "        cuobjdump_path = copy_tool(\"cuobjdump\")\n",
    "    except FileNotFoundError:\n",
    "        cuobjdump_path = None  # optional\n",
    "    try:\n",
    "        nvdisasm_path = copy_tool(\"nvdisasm\")\n",
    "    except FileNotFoundError:\n",
    "        nvdisasm_path = None  # optional\n",
    "    \n",
    "    # 3) Environment for Triton/JIT\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n",
    "    os.environ[\"PATH\"] = f\"{WRK_BIN}:{os.environ.get('PATH','')}\"\n",
    "    os.environ[\"TRITON_CACHE_DIR\"] = TRITON_CACHE\n",
    "    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "    os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda\"\n",
    "    \n",
    "    # Helpful fallbacks if you still hit capture issues:\n",
    "    # os.environ[\"SGLANG_DISABLE_CUDA_GRAPH\"] = \"1\"      # skip CUDA graphs (degrades perf but avoids capture)\n",
    "    # os.environ[\"TRITON_CODEGEN_FATBIN\"] = \"0\"          # can reduce Triton fatbin steps on some setups\n",
    "    \n",
    "    # 4) Smoke test: ensure ptxas runs from the new location\n",
    "    print(\"ptxas ->\", subprocess.check_output([ptxas_path, \"--version\"]).decode().strip())\n",
    "    \n",
    "    # Now it's safe to import heavy libs that trigger Triton\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53b2fff",
   "metadata": {
    "papermill": {
     "duration": 180.6259,
     "end_time": "2025-09-14T19:20:47.865965",
     "exception": false,
     "start_time": "2025-09-14T19:17:47.240065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory cleared.\n",
      "üîß Using model from Trelis/Soar-qwen-14b-FP8-Dynamic\n",
      "LOG file path: /workspace/arc-agi-2025/llm_python/submissions/sglang_server.log\n",
      "Started sglang server PID=24914 | logging to /workspace/arc-agi-2025/llm_python/submissions/sglang_server.log\n",
      "Command: /workspace/arc-agi-2025/.venv/bin/python3 -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path Trelis/Soar-qwen-14b-FP8-Dynamic --dp 1 --enable-metrics --grammar-backend none --kv-cache-dtype fp8_e4m3\n",
      "sglang is READY on port 8080.\n",
      "‚úÖ Model loaded: Trelis/Soar-qwen-14b-FP8-Dynamic\n",
      "Call stop_server() or full_cleanup() to shut it down gracefully.\n"
     ]
    }
   ],
   "source": [
    "if START_SERVER:\n",
    "  # Background server launcher for Kaggle with SGLang\n",
    "  import os, sys, time, subprocess, json, socket, requests\n",
    "\n",
    "  # ---------- 1) Check for existing server and cleanup ----------\n",
    "  PORT = 8080\n",
    "  HEALTH_URL = f\"http://127.0.0.1:{PORT}/v1/models\"\n",
    "\n",
    "  # Check if server already running\n",
    "  try:\n",
    "      r = requests.get(HEALTH_URL, timeout=3)\n",
    "      if r.status_code == 200:\n",
    "          print(f\"Server already running on port {PORT}. Stopping it first...\")\n",
    "          # Kill existing sglang processes\n",
    "          subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "          time.sleep(3)  # Wait for cleanup\n",
    "  except:\n",
    "      pass  # No server running\n",
    "\n",
    "  # Clear CUDA memory before starting\n",
    "  try:\n",
    "      import torch\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.synchronize()\n",
    "          print(\"CUDA memory cleared.\")\n",
    "      num_gpus = torch.cuda.device_count()\n",
    "  except Exception:\n",
    "      num_gpus = 0\n",
    "      \n",
    "  model_path_to_use = str(MODEL_PATH)\n",
    "  print(f\"üîß Using model from {model_path_to_use}\")\n",
    "\n",
    "  LOG = f\"{SUBMIT_DIR}/sglang_server.log\"\n",
    "  print(f\"LOG file path: {LOG}\")\n",
    "\n",
    "  SERVER_CMD = [\n",
    "        sys.executable, \"-m\", \"sglang.launch_server\",\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", str(PORT),\n",
    "        \"--model-path\", model_path_to_use,\n",
    "        \"--dp\", str(max(1, min(num_gpus, 4))),\n",
    "        \"--enable-metrics\",\n",
    "        \"--grammar-backend\", \"none\",\n",
    "  ]\n",
    "    \n",
    "  # Add Qwen-specific flag\n",
    "  if 'qwen' in model_path_to_use.lower():\n",
    "      SERVER_CMD.extend([\"--kv-cache-dtype\", \"fp8_e4m3\"])\n",
    "\n",
    "  # ---------- 2) Launch in background ----------\n",
    "  log_f = open(LOG, \"w\")\n",
    "  env = os.environ.copy()\n",
    "  proc = subprocess.Popen(SERVER_CMD, stdout=log_f, stderr=subprocess.STDOUT, env=env, cwd=SUBMIT_DIR)\n",
    "  print(f\"Started sglang server PID={proc.pid} | logging to {LOG}\")\n",
    "  print(\"Command:\", \" \".join(SERVER_CMD))\n",
    "\n",
    "  # ---------- 3) Wait for readiness ----------\n",
    "  def wait_ready(url, timeout_s=600):\n",
    "      t0 = time.time()\n",
    "      while time.time() - t0 < timeout_s:\n",
    "          try:\n",
    "              r = requests.get(url, timeout=3)\n",
    "              if r.status_code == 200:\n",
    "                  return True\n",
    "          except Exception:\n",
    "              pass\n",
    "          time.sleep(2)\n",
    "      return False\n",
    "\n",
    "  ready = wait_ready(HEALTH_URL)\n",
    "  log_f.flush()\n",
    "\n",
    "  if ready:\n",
    "      print(f\"sglang is READY on port {PORT}.\")\n",
    "      # Get the model name\n",
    "      try:\n",
    "          response = requests.get(HEALTH_URL)\n",
    "          if response.status_code == 200:\n",
    "              models = response.json()['data']\n",
    "              if models:\n",
    "                  model_name = models[0]['id']\n",
    "                  print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "              else:\n",
    "                  print(\"‚ùå No models found on server\")\n",
    "                  model_name = str(MODEL_PATH)\n",
    "          else:\n",
    "              print(f\"‚ùå Server health check failed: {response.status_code}\")\n",
    "              model_name = str(MODEL_PATH)\n",
    "      except Exception as e:\n",
    "          print(f\"‚ö†Ô∏è Could not get model name: {e}\")\n",
    "          model_name = str(MODEL_PATH)\n",
    "  else:\n",
    "      print(f\"sglang not ready after timeout. Showing last 60 log lines:\")\n",
    "      log_f.close()\n",
    "      !tail -n 60 {LOG}\n",
    "      model_name = str(MODEL_PATH)\n",
    "\n",
    "  # ---------- 4) Cleanup functions ----------\n",
    "  def stop_server(p=proc):\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "      print(\"Server stopped.\")\n",
    "\n",
    "  def full_cleanup(p=proc):\n",
    "      # Stop server\n",
    "      try:\n",
    "          p.terminate()\n",
    "          p.wait(timeout=10)\n",
    "      except Exception:\n",
    "          p.kill()\n",
    "\n",
    "      # Also kill any lingering sglang processes\n",
    "      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n",
    "\n",
    "      # Clear CUDA memory\n",
    "      try:\n",
    "          import torch\n",
    "          if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              torch.cuda.synchronize()\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "      print(\"Server stopped and CUDA memory cleared.\")\n",
    "\n",
    "  print(\"Call stop_server() or full_cleanup() to shut it down gracefully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f39f6dc",
   "metadata": {
    "papermill": {
     "duration": 10.111292,
     "end_time": "2025-09-14T19:26:58.025629",
     "exception": false,
     "start_time": "2025-09-14T19:26:47.914337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "‚úÖ Response received:\n",
      "Based on the training examples, the transformation rule appears\n",
      "\n",
      "‚è± Elapsed time: 0.82 seconds\n",
      "üî¢ Estimated tokens: 15.8\n",
      "‚ö° Output tokens/sec: 19.29\n"
     ]
    }
   ],
   "source": [
    "if TEST_INFERENCE:\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    # Use custom endpoint if provided, otherwise use local server\n",
    "    base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else \"http://127.0.0.1:8080/v1\"\n",
    "    url = f\"{base_url}/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n",
    "        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10√ó10 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,  # from your polling loop\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": 1000\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=600)\n",
    "    print(response)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    # Estimate token count (4 chars/token assumption)\n",
    "    estimated_tokens = len(output_text) / 4\n",
    "    elapsed_time = end_time - start_time\n",
    "    tokens_per_second = estimated_tokens / elapsed_time\n",
    "    \n",
    "    print(\"‚úÖ Response received:\")\n",
    "    print(output_text)\n",
    "    print(f\"\\n‚è± Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"üî¢ Estimated tokens: {estimated_tokens:.1f}\")\n",
    "    print(f\"‚ö° Output tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e45eb3",
   "metadata": {
    "papermill": {
     "duration": 525.478862,
     "end_time": "2025-09-14T19:35:43.513043",
     "exception": false,
     "start_time": "2025-09-14T19:26:58.034181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/arc-agi-2025\n",
      "Sampling Inference ‚Üí dev | attempts=256 | workers=256 | subset=evaluation | timeout=18000s\n",
      "Running command: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset evaluation --max_workers 256 --max_attempts 256 --model Trelis/Soar-qwen-14b-FP8-Dynamic --base-url http://127.0.0.1:8080/v1 --unsafe-executor --splitter --max-tokens 2000 --qwen-no-think --parquet-output-dir /workspace/arc-agi-2025/llm_python/datasets/inference\n",
      "üìù Logging output to: /workspace/arc-agi-2025/llm_python/submissions/sampling.log\n",
      "‚è≥ Running sampling phase (output being written to log file, 18000s timeout)...\n"
     ]
    }
   ],
   "source": [
    "if not IS_KAGGLE:\n",
    "      %cd /workspace/arc-agi-2025\n",
    "\n",
    "# Use SAMPLING_ATTEMPTS and MAX_WORKERS for initial inference\n",
    "attempts = SAMPLING_ATTEMPTS\n",
    "workers = MAX_WORKERS\n",
    "\n",
    "# SUBSET = \"test\" # defaulting to test to ensure there are no loading issues.\n",
    "\n",
    "# can use this instead if testing evaluation during a pre-run\n",
    "SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "print(f\"Sampling Inference ‚Üí {'competition' if IS_RERUN else 'dev'} | attempts={attempts} | workers={workers} | subset={SUBSET} | timeout={SAMPLING_TIMEOUT}s\")\n",
    "\n",
    "# Use custom endpoint if provided, otherwise use local server\n",
    "base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else \"http://127.0.0.1:8080/v1\"\n",
    "\n",
    "# Build the command\n",
    "cmd_args = [\n",
    "  \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "  \"--dataset\", DATASET,\n",
    "  \"--subset\", SUBSET,\n",
    "  \"--max_workers\", str(workers),\n",
    "  \"--max_attempts\", str(attempts),\n",
    "  \"--model\", model_name,\n",
    "  \"--base-url\", base_url,\n",
    "  \"--unsafe-executor\",\n",
    "  \"--splitter\"\n",
    "]\n",
    "\n",
    "if 'qwen' in model_name.lower():\n",
    "    cmd_args.extend([\"--max-tokens\", \"2000\"])\n",
    "    cmd_args.extend([\"--qwen-no-think\"])\n",
    "else:\n",
    "    cmd_args.extend([\"--max-tokens\", \"64000\"])\n",
    "\n",
    "# Add parquet output directory if set\n",
    "if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "    cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd_args)}\")\n",
    "\n",
    "# Always use subprocess with timeout (works for all environments)\n",
    "import subprocess\n",
    "log_file_path = f\"{SUBMIT_DIR}/sampling.log\"\n",
    "print(f\"üìù Logging output to: {log_file_path}\")\n",
    "\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "  process = subprocess.Popen(\n",
    "      cmd_args,\n",
    "      stdout=log_file,\n",
    "      stderr=subprocess.STDOUT,\n",
    "      text=True,\n",
    "      cwd=os.getcwd()\n",
    "  )\n",
    "\n",
    "  # Wait for completion with timeout\n",
    "  print(f\"‚è≥ Running sampling phase (output being written to log file, {SAMPLING_TIMEOUT}s timeout)...\")\n",
    "  try:\n",
    "      return_code = process.wait(timeout=SAMPLING_TIMEOUT)\n",
    "      if return_code == 0:\n",
    "          print(f\"‚úÖ Sampling phase completed successfully. Check {log_file_path} for details.\")\n",
    "      else:\n",
    "          print(f\"‚ùå Sampling phase failed with return code {return_code}\")\n",
    "          print(f\"üìù Check {log_file_path} for error details\")\n",
    "          # Show last few lines of log\n",
    "          !tail -n 20 {log_file_path}\n",
    "  except subprocess.TimeoutExpired:\n",
    "      print(f\"‚è∞ Sampling phase timeout reached ({SAMPLING_TIMEOUT}s) - terminating process\")\n",
    "      process.terminate()\n",
    "      try:\n",
    "          # Give it 600 seconds to cleanup gracefully\n",
    "          process.wait(timeout=600)\n",
    "          print(\"‚úÖ Process terminated gracefully - parquet files should be saved\")\n",
    "      except subprocess.TimeoutExpired:\n",
    "          print(\"‚ö†Ô∏è Process didn't terminate gracefully, forcing kill\")\n",
    "          process.kill()\n",
    "          process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02e48e",
   "metadata": {
    "papermill": {
     "duration": 190.200335,
     "end_time": "2025-09-14T19:38:53.770008",
     "exception": false,
     "start_time": "2025-09-14T19:35:43.569673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENABLE_REFINEMENT:\n",
    "    print(\"üîÑ Checking if server restart is needed...\")\n",
    "    \n",
    "    # Since we're using the same model for both initial and refinement inference,\n",
    "    # we don't need to restart the server\n",
    "    print(\"‚úÖ Using same model for refinement - no server restart needed\")\n",
    "    print(f\"üéØ Continuing with existing model: {MODEL_PATH}\")\n",
    "    \n",
    "    # Just verify the server is still running and get the model name\n",
    "    if START_SERVER:\n",
    "        try:\n",
    "            HEALTH_URL = \"http://127.0.0.1:8080/v1/models\"\n",
    "            response = requests.get(HEALTH_URL, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json()['data']\n",
    "                if models:\n",
    "                    model_name = models[0]['id']\n",
    "                    print(f\"‚úÖ Server is running with model: {model_name}\")\n",
    "                else:\n",
    "                    print(\"‚ùå No models found on server\")\n",
    "            else:\n",
    "                print(f\"‚ùå Server health check failed: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not verify server status: {e}\")\n",
    "            print(\"‚ùå Server may not be running properly\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è START_SERVER is False - assuming server is managed externally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0e37a",
   "metadata": {
    "papermill": {
     "duration": 556.082824,
     "end_time": "2025-09-14T19:48:09.870010",
     "exception": false,
     "start_time": "2025-09-14T19:38:53.787186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Second Inference Run - ENABLE_REFINEMENT\n",
    "# Only runs when ENABLE_REFINEMENT=true\n",
    "\n",
    "import glob\n",
    "\n",
    "if ENABLE_REFINEMENT:\n",
    "    print(\"üîÑ Running refinement inference (ENABLE_REFINEMENT mode)\")\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        %cd /workspace/arc-agi-2025\n",
    "\n",
    "    # Use REFINEMENT_ATTEMPTS and MAX_WORKERS for second inference\n",
    "    attempts = REFINEMENT_ATTEMPTS\n",
    "    workers = MAX_WORKERS\n",
    "\n",
    "    SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n",
    "\n",
    "    print(f\"Refinement Phase ‚Üí {'competition' if IS_RERUN else 'dev'} | attempts={attempts} | workers={workers} | subset={SUBSET} | timeout={REFINEMENT_TIMEOUT}s\")\n",
    "\n",
    "    # üîë Find the latest parquet file in inference_dir\n",
    "    parquet_files = glob.glob(os.path.join(inference_dir, \"*.parquet\"))\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {inference_dir}\")\n",
    "    latest_parquet = max(parquet_files, key=os.path.getctime)\n",
    "    print(f\"üìÇ Using latest parquet file for refinement: {latest_parquet}\")\n",
    "\n",
    "    # Use custom endpoint if provided, otherwise use local server\n",
    "    base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else \"http://127.0.0.1:8080/v1\"\n",
    "\n",
    "    # Build the command\n",
    "    cmd_args = [\n",
    "        \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n",
    "        \"--dataset\", DATASET,\n",
    "        \"--subset\", SUBSET,\n",
    "        \"--max_workers\", str(workers),\n",
    "        \"--max_attempts\", str(attempts),\n",
    "        \"--model\", model_name,\n",
    "        \"--base-url\", base_url,\n",
    "        \"--unsafe-executor\",\n",
    "        \"--refinement-ds\", latest_parquet,   # üëà add parquet path here\n",
    "    ]\n",
    "\n",
    "    if 'qwen' in model_name.lower():\n",
    "        cmd_args.extend([\"--max-tokens\", \"2000\"])\n",
    "        cmd_args.extend([\"--qwen-no-think\"])\n",
    "    else:\n",
    "        cmd_args.extend([\"--max-tokens\", \"64000\"])\n",
    "\n",
    "    # Add parquet output directory if set\n",
    "    if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n",
    "      cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n",
    "      cmd_args.extend([\"--rex-stats\"])\n",
    "\n",
    "    print(f\"Running refinement inference: {' '.join(cmd_args)}\")\n",
    "\n",
    "    # Handle output redirection properly\n",
    "    # For quiet mode, redirect to file using subprocess\n",
    "    import subprocess\n",
    "    log_file_path = f\"{SUBMIT_DIR}/refinement.log\"\n",
    "    print(f\"üìù Logging refinement phase output to: {log_file_path}\")\n",
    "    \n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            cmd_args,\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Wait for completion with timeout\n",
    "        print(f\"‚è≥ Running refinement phase (output being written to log file, {REFINEMENT_TIMEOUT}s timeout)...\")\n",
    "        try:\n",
    "            return_code = process.wait(timeout=REFINEMENT_TIMEOUT)\n",
    "            if return_code == 0:\n",
    "                print(f\"‚úÖ Refinement phase completed successfully. Check {log_file_path} for details.\")\n",
    "            else:\n",
    "                print(f\"‚ùå Refinement phase failed with return code {return_code}\")\n",
    "                print(f\"üìù Check {log_file_path} for error details\")\n",
    "                # Show last few lines of log\n",
    "                !tail -n 20 {log_file_path}\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"‚è∞ Refinement phase timeout reached ({REFINEMENT_TIMEOUT}s) - terminating process\")\n",
    "            process.terminate()\n",
    "            try:\n",
    "                # Give it 30 seconds to cleanup gracefully\n",
    "                process.wait(timeout=30)\n",
    "                print(\"‚úÖ Process terminated gracefully - parquet files should be saved\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"‚ö†Ô∏è Process didn't terminate gracefully, forcing kill\")\n",
    "                process.kill()\n",
    "                process.wait()\n",
    "\n",
    "else:\n",
    "    print(\"üîÑ Skipping refinement phase (ENABLE_REFINEMENT=false)\")\n",
    "    print(\"   ‚Üí Standard mode runs sampling phase only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971bad5",
   "metadata": {
    "papermill": {
     "duration": 10.205901,
     "end_time": "2025-09-14T19:48:20.105564",
     "exception": false,
     "start_time": "2025-09-14T19:48:09.899663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate submission using the two most recent parquet files\n",
    "\n",
    "print(\"üéØ Generating submission from the two most recent parquet files...\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "output_dir = str(SUBMIT_DIR)\n",
    "\n",
    "# Command to generate submission using the two most recent parquet files\n",
    "submission_cmd = [\n",
    "    \"uv\", \"run\", \"python\", \"-m\", \"llm_python.generate_submission\",\n",
    "    \"--parquet-path\", inference_dir,\n",
    "    \"--n-files\", \"2\",\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--subset\", SUBSET,\n",
    "    \"--output-dir\", output_dir,\n",
    "    \"--debug\"\n",
    "]\n",
    "\n",
    "print(f\"Running submission generation: {' '.join(submission_cmd)}\")\n",
    "print(f\"üìÇ Looking for parquet files in: {inference_dir}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        submission_cmd,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=300,  # 5 minute timeout\n",
    "        cwd=os.getcwd()\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Submission generation completed successfully!\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Update submit_dir to point to the generated file\n",
    "        submit_dir = f\"{output_dir}/submission.json\"\n",
    "        print(f\"üìÅ Submission file: {submit_dir}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Submission generation failed with return code {result.returncode}\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        # Fallback to default submission path\n",
    "        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è Submission generation timed out\")\n",
    "    submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Submission generation error: {e}\")\n",
    "    submit_dir = f\"{SUBMIT_DIR}/submission.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7a2ee",
   "metadata": {
    "papermill": {
     "duration": 1.295028,
     "end_time": "2025-09-14T19:48:21.430229",
     "exception": false,
     "start_time": "2025-09-14T19:48:20.135201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only score in dev/commit runs\n",
    "if SCORE and not IS_RERUN:\n",
    "    !uv run python -m llm_python.score_submission --submission {submit_dir} --dataset {DATASET} --subset {SUBSET}\n",
    "else:\n",
    "    print(\"Skipping local scoring (competition rerun or SCORE=False).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5561b",
   "metadata": {
    "papermill": {
     "duration": 0.089127,
     "end_time": "2025-09-14T19:48:21.549571",
     "exception": false,
     "start_time": "2025-09-14T19:48:21.460444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final cleanup - stop server and free resources\n",
    "if START_SERVER and 'full_cleanup' in globals():\n",
    "    print(\"üßπ Cleaning up server and resources...\")\n",
    "    full_cleanup()\n",
    "else:\n",
    "    print(\"üîç No server cleanup needed (START_SERVER=False or cleanup function not available)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 13706937,
     "datasetId": 8063856,
     "isSourceIdPinned": true,
     "sourceId": 13032060,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 262135446,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "arc-agi-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1863.748367,
   "end_time": "2025-09-14T19:48:24.194420",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T19:17:20.446053",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
