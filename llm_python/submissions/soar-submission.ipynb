{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13032060,"sourceType":"datasetVersion","datasetId":8063856,"isSourceIdPinned":true},{"sourceId":262658968,"sourceType":"kernelVersion"},{"sourceId":510361,"sourceType":"modelInstanceVersion","modelInstanceId":404469,"modelId":422375}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1863.748367,"end_time":"2025-09-14T19:48:24.194420","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-14T19:17:20.446053","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n## Notes for OpenRouter\n# You can run with OpenRouter but must first - it's easiest to set these in Runpod on startup (before kernel startup):\n# export CUSTOM_ENDPOINT=\"https://openrouter.ai\"\n# export DEV_RUN=\"true\"\n# export OPENAI_API_KEY=\"key-here\"\n\n# ============================================================================\n# Model Configuration\n# ============================================================================\n\n# Single model for both initial inference and refinement\nMODEL_HF = \"openai/gpt-oss-20b\"  # For local/RunPod\n# MODEL_HF = \"Trelis/Soar-qwen-14b-FP8-Dynamic\"  # For local/RunPod\nMODEL_KAGGLE = \"gpt-oss-20b/transformers/default\"  # Kaggle dataset name\n# MODEL_KAGGLE = \"arc-1-fake-ttt-blended-c802-dataset\"  # Kaggle dataset name\n\n# ============================================================================\n# Inference Configuration\n# ============================================================================\n\n# Sampling attempts (initial inference)\nSAMPLING_ATTEMPTS = 4     # Number of attempts for sampling phase\n# Refinement attempts (second inference)\nREFINEMENT_ATTEMPTS = 4    # Number of attempts for refinement phase\n\n# Number of refinement phases\nR = 1  # Number of refinement phases\nREFINEMENT_ATTEMPTS_PER_PHASE = REFINEMENT_ATTEMPTS // R\n\n# Global workers setting\nMAX_WORKERS = 32            # Number of workers for all inference phases\n\n# ============================================================================\n# Other Configuration\n# ============================================================================\n\nDATASET = \"arc-prize-2025\"\n\n# ---- Config flags (single source of truth) ----\n# Check for custom endpoint first - if set, disable server startup and use provided endpoint\nCUSTOM_ENDPOINT = os.getenv(\"CUSTOM_ENDPOINT\")\nSTART_SERVER = not bool(CUSTOM_ENDPOINT)  # Don't start server if custom endpoint provided\n\nTEST_INFERENCE = True\nSCORE = True                   # default; overridden below, depending on flags\n\n# DEV_RUN can be explicitly set via environment variable, or auto-detected for Kaggle non-rerun\nDEV_RUN = os.getenv(\"DEV_RUN\", \"\").lower() == \"true\"\n\nPRODUCTION_TIMEOUT = 5.5 * 3600\nDEV_TIMEOUT = 90 * R\n\n# Refinement mode?\nENABLE_REFINEMENT=True\n\n# Env-backed flags\nIS_KAGGLE = bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\nIS_RERUN  = IS_KAGGLE and os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\", \"\").lower() == \"true\"\n\n# Auto-detect dev mode for Kaggle non-rerun environments (unless explicitly overridden)\nif IS_KAGGLE and not IS_RERUN and not os.getenv(\"DEV_RUN\"):\n    DEV_RUN = True\n    print(\"üîß Auto-detected Kaggle development environment (non-rerun)\")\n\n# # String env flag for external tools\n# os.environ[\"SUBMIT\"] = \"true\"\n\n# Ensure client key is set\nos.environ.setdefault(\"OPENAI_API_KEY\", \"EMPTY\")\n\n# ---- Paths ----\nif IS_KAGGLE:\n    ARC_DATA_ROOT   = Path(\"/kaggle/input\")\n    MODEL_SAVE_DIR = Path(\"/kaggle/working\")\n    SUBMIT_DIR      = Path(\"/kaggle/working\")\n    ARC_PROGRAMS_PARQUET = SUBMIT_DIR\n\n    print(\"üîç Searching for models in Kaggle environment...\")\n\n    # Auto-find model path in Kaggle's dataset structure\n    model_dataset_path = ARC_DATA_ROOT / MODEL_KAGGLE\n    print(f\"   Looking for model dataset: {model_dataset_path}\")\n\n    if model_dataset_path.exists() and model_dataset_path.is_dir():\n        # Kaggle datasets have version folders, find the first subdirectory\n        subdirs = [d for d in model_dataset_path.iterdir() if d.is_dir()]\n        if subdirs:\n            MODEL_PATH = subdirs[0]  # Use the first (usually only) version folder\n            print(f\"   ‚úÖ Found model at: {MODEL_PATH}\")\n            # List what's inside to confirm it's right\n            model_contents = list(MODEL_PATH.iterdir())[:5]  # Show first 5 items\n            print(f\"      Contents: {[f.name for f in model_contents]}\")\n        else:\n            # Fallback if no subdirectory found\n            MODEL_PATH = model_dataset_path\n            print(f\"   ‚ö†Ô∏è No version folder found for model, using: {MODEL_PATH}\")\n    else:\n        MODEL_PATH = model_dataset_path\n        print(f\"   ‚ùå Model dataset not found at: {MODEL_PATH}\")\n        print(f\"      Available datasets: {[d.name for d in ARC_DATA_ROOT.iterdir() if d.is_dir()][:10]}\")\n\n    print(f\"\\nüì¶ Final model path:\")\n    print(f\"   Model: {MODEL_PATH}\")\n\nelse:\n    ARC_DATA_ROOT   = Path(\"/workspace/arc-agi-2025/data\")\n    MODEL_SAVE_DIR = Path(\"/workspace/arc-agi-2025/llm_python/fine-tuning\")\n    SUBMIT_DIR      = Path(\"/workspace/arc-agi-2025/llm_python/submissions\")\n    ARC_PROGRAMS_PARQUET = Path(\"/workspace/arc-agi-2025/llm_python/datasets/inference\")\n\n    # Use local/RunPod model path\n    MODEL_PATH = MODEL_HF\n\n    print(f\"üì¶ Local/RunPod model path:\")\n    print(f\"   Model: {MODEL_PATH}\")\n\n# Set up paths - parquet files are saved by task runner in different locations\nif IS_KAGGLE:\n    # On Kaggle, parquet files are saved directly in /kaggle/working by task runner\n    inference_dir = \"/kaggle/working\"\nelse:\n    # On RunPod/local, parquet files are saved in llm_python/datasets/inference\n    inference_dir = \"llm_python/datasets/inference\"\n\n# Export envs for downstream processes\nos.environ[\"ARC_DATA_ROOT\"]   = str(ARC_DATA_ROOT)\nos.environ[\"MODEL_SAVE_DIR\"] = str(MODEL_SAVE_DIR)\nos.environ[\"SUBMIT_DIR\"]      = str(SUBMIT_DIR)\nos.environ[\"ARC_PROGRAMS_PARQUET\"] = str(ARC_PROGRAMS_PARQUET)\nos.environ[\"MODEL_PATH\"] = str(MODEL_PATH)\n\n# Export config flags for subprocess use\nos.environ[\"IS_KAGGLE\"] = str(IS_KAGGLE).lower()\nos.environ[\"IS_RERUN\"] = str(IS_RERUN).lower()\nos.environ[\"DATASET\"] = DATASET\n\n# Ensure directories exist\nfor p in (MODEL_SAVE_DIR, SUBMIT_DIR):\n    p.mkdir(parents=True, exist_ok=True)\n\nif IS_RERUN:\n    # Kaggle competition rerun - use full configuration\n    print(f\"üèÜ Competition rerun detected\")\n    SCORE = False\n    os.environ[\"SUBMIT\"] = \"true\" # means we don't have test information available for task-wise scoring\n    # Full timeouts for competition\n    SAMPLING_TIMEOUT = PRODUCTION_TIMEOUT\n    REFINEMENT_TIMEOUT_PER_PHASE = PRODUCTION_TIMEOUT // R  # Each refinement phase gets 1/R of production timeout\nelif DEV_RUN:\n    # Development mode - use reduced timeouts for faster testing\n    print(f\"üîß Development mode enabled - applying shorter timeouts\")\n    SCORE = True  # if we're generating a submission, do scoring\n    os.environ[\"SUBMIT\"] = \"true\" # means we don't have test information available for task-wise scoring. Mimic Kaggle environment.\n    # Short timeouts for dev testing\n    SAMPLING_TIMEOUT = DEV_TIMEOUT\n    REFINEMENT_TIMEOUT_PER_PHASE = DEV_TIMEOUT // R  # Each refinement phase gets 1/R of dev timeout\nelse:\n    # Production mode (RunPod/local long run) - use full configuration\n    print(f\"üñ•Ô∏è Production mode (RunPod/local environment)\")\n    SCORE = True  # if we're generating a submission, do scoring\n    os.environ[\"SUBMIT\"] = \"false\" # means we will use task test outputs to score row by row\n    # Full timeouts for production runs\n    SAMPLING_TIMEOUT = PRODUCTION_TIMEOUT\n    REFINEMENT_TIMEOUT_PER_PHASE = PRODUCTION_TIMEOUT // R  # Each refinement phase gets 1/R of production timeout\n\n# ENABLE_REFINEMENT Mode configuration\nif ENABLE_REFINEMENT:\n    print(\"üß™ ENABLE_REFINEMENT ENABLED\")\n    print(\"   ‚Üí Will run: Sampling ‚Üí Refinement\")\n    print(f\"   ‚Üí Sampling: {SAMPLING_ATTEMPTS} attempts, {MAX_WORKERS} workers, {SAMPLING_TIMEOUT}s timeout\")\n    print(f\"   ‚Üí Refinement: {R} phases √ó {REFINEMENT_ATTEMPTS_PER_PHASE} attempts, {MAX_WORKERS} workers, {REFINEMENT_TIMEOUT_PER_PHASE}s timeout each\")\nelse:\n    print(\"üîÑ Standard mode (ENABLE_REFINEMENT disabled)\")\n    print(f\"   ‚Üí Will run: Sampling only ({SAMPLING_ATTEMPTS} attempts, {MAX_WORKERS} workers, {SAMPLING_TIMEOUT}s timeout)\")\n\n# Custom endpoint configuration\nif CUSTOM_ENDPOINT:\n    model_name = MODEL_HF\n    print(f\"üîó Using custom endpoint: {CUSTOM_ENDPOINT}\")\n    print(\"   ‚Üí Server startup disabled\")\nelse:\n    print(\"üöÄ Will start local server (no custom endpoint specified)\")\n\n# Optional: quick summary (helps avoid accidental submits)\nprint(\n    \"Mode summary ‚Üí \"\n    f\"IS_KAGGLE={IS_KAGGLE} | IS_RERUN={IS_RERUN} | DEV_RUN={DEV_RUN} | ENABLE_REFINEMENT={ENABLE_REFINEMENT} |\\n\"\n    f\"TEST_INFERENCE={TEST_INFERENCE} | SCORE={SCORE} | SUBMIT={os.environ['SUBMIT']} | MODEL={MODEL_PATH}\"\n)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-18T18:30:38.945922Z","iopub.execute_input":"2025-09-18T18:30:38.946380Z","iopub.status.idle":"2025-09-18T18:30:38.961773Z","shell.execute_reply.started":"2025-09-18T18:30:38.946363Z","shell.execute_reply":"2025-09-18T18:30:38.961303Z"},"papermill":{"duration":0.037384,"end_time":"2025-09-14T19:17:30.040804","exception":false,"start_time":"2025-09-14T19:17:30.003420","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"üîß Auto-detected Kaggle development environment (non-rerun)\nüîç Searching for models in Kaggle environment...\n   Looking for model dataset: /kaggle/input/gpt-oss-20b/transformers/default\n   ‚úÖ Found model at: /kaggle/input/gpt-oss-20b/transformers/default/1\n      Contents: ['model.safetensors.index.json', 'model-00000-of-00002.safetensors', 'USAGE_POLICY', 'config.json', 'model-00001-of-00002.safetensors']\n\nüì¶ Final model path:\n   Model: /kaggle/input/gpt-oss-20b/transformers/default/1\nüñ•Ô∏è Production mode (RunPod/local environment)\nüß™ ENABLE_REFINEMENT ENABLED\n   ‚Üí Will run: Sampling ‚Üí Refinement\n   ‚Üí Sampling: 8 attempts, 32 workers, 18000s timeout\n   ‚Üí Refinement: 4 phases √ó 4 attempts, 32 workers, 4500s timeout each\nüöÄ Will start local server (no custom endpoint specified)\nMode summary ‚Üí IS_KAGGLE=True | IS_RERUN=False | DEV_RUN=False | ENABLE_REFINEMENT=True |\nTEST_INFERENCE=True | SCORE=True | SUBMIT=false | MODEL=/kaggle/input/gpt-oss-20b/transformers/default/1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nimport torch\nimport numpy as np\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA version (PyTorch): {torch.version.cuda}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"NumPy version: {np.__version__}\")\nif torch.cuda.is_available():\n   print(f\"GPU count: {torch.cuda.device_count()}\")\n   print(f\"GPU name: {torch.cuda.get_device_name(0)}\")","metadata":{"execution":{"iopub.status.busy":"2025-09-18T18:30:56.528322Z","iopub.execute_input":"2025-09-18T18:30:56.528894Z","iopub.status.idle":"2025-09-18T18:31:08.079908Z","shell.execute_reply.started":"2025-09-18T18:30:56.528865Z","shell.execute_reply":"2025-09-18T18:31:08.079321Z"},"papermill":{"duration":12.898677,"end_time":"2025-09-14T19:17:42.942988","exception":false,"start_time":"2025-09-14T19:17:30.044311","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPyTorch version: 2.8.0+cu128\nCUDA version (PyTorch): 12.8\nCUDA available: True\nNumPy version: 1.26.4\nGPU count: 4\nGPU name: NVIDIA L4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sglang\nprint(\"SGLang version:\", sglang.__version__)\n\ntry:\n    import flashinfer\n    print(\"FlashInfer version:\", flashinfer.__version__)\nexcept ImportError:\n    print(\"FlashInfer not installed\")","metadata":{"execution":{"iopub.status.busy":"2025-09-18T18:31:08.080784Z","iopub.execute_input":"2025-09-18T18:31:08.081130Z","iopub.status.idle":"2025-09-18T18:31:11.324464Z","shell.execute_reply.started":"2025-09-18T18:31:08.081111Z","shell.execute_reply":"2025-09-18T18:31:11.323810Z"},"papermill":{"duration":3.554569,"end_time":"2025-09-14T19:17:46.501106","exception":false,"start_time":"2025-09-14T19:17:42.946537","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"SGLang version: 0.5.1.post3\n","output_type":"stream"},{"name":"stderr","text":"W0918 18:31:10.784000 100 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW0918 18:31:10.784000 100 arc-agi-2025-aux/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n","output_type":"stream"},{"name":"stdout","text":"FlashInfer version: 0.2.14.post1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"if IS_KAGGLE:\n    import os, shutil, subprocess, stat\n    \n    # 1) Where to place binaries + cache\n    WRK_BIN = \"/kaggle/working/bin\"\n    TRITON_CACHE = \"/kaggle/working/.triton\"\n    os.makedirs(WRK_BIN, exist_ok=True)\n    os.makedirs(TRITON_CACHE, exist_ok=True)\n    \n    # 2) Preferred source for ptxas/cuobjdump/nvdisasm\n    SYSTEM_CUDA_BIN = \"/usr/local/cuda/bin\"\n    FALLBACK_VENDORED = \"/kaggle/usr/lib/sglang_utility/triton/backends/nvidia/bin\"  # if you have it\n    \n    def copy_tool(name: str):\n        for src_dir in (SYSTEM_CUDA_BIN, FALLBACK_VENDORED):\n            src = os.path.join(src_dir, name)\n            if os.path.exists(src):\n                dst = os.path.join(WRK_BIN, name)\n                shutil.copy2(src, dst)\n                # ensure executable bit\n                os.chmod(dst, os.stat(dst).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n                return dst\n        raise FileNotFoundError(f\"Could not find {name} in {SYSTEM_CUDA_BIN} or {FALLBACK_VENDORED}\")\n    \n    ptxas_path = copy_tool(\"ptxas\")\n    try:\n        cuobjdump_path = copy_tool(\"cuobjdump\")\n    except FileNotFoundError:\n        cuobjdump_path = None  # optional\n    try:\n        nvdisasm_path = copy_tool(\"nvdisasm\")\n    except FileNotFoundError:\n        nvdisasm_path = None  # optional\n    \n    # 3) Environment for Triton/JIT\n    os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n    os.environ[\"PATH\"] = f\"{WRK_BIN}:{os.environ.get('PATH','')}\"\n    os.environ[\"TRITON_CACHE_DIR\"] = TRITON_CACHE\n    os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n    os.environ[\"CUDA_PATH\"] = \"/usr/local/cuda\"\n    \n    # Helpful fallbacks if you still hit capture issues:\n    # os.environ[\"SGLANG_DISABLE_CUDA_GRAPH\"] = \"1\"      # skip CUDA graphs (degrades perf but avoids capture)\n    # os.environ[\"TRITON_CODEGEN_FATBIN\"] = \"0\"          # can reduce Triton fatbin steps on some setups\n    \n    # 4) Smoke test: ensure ptxas runs from the new location\n    print(\"ptxas ->\", subprocess.check_output([ptxas_path, \"--version\"]).decode().strip())\n    \n    # Now it's safe to import heavy libs that trigger Triton\n    import torch","metadata":{"execution":{"iopub.status.busy":"2025-09-18T18:31:11.325120Z","iopub.execute_input":"2025-09-18T18:31:11.325427Z","iopub.status.idle":"2025-09-18T18:31:11.747917Z","shell.execute_reply.started":"2025-09-18T18:31:11.325411Z","shell.execute_reply":"2025-09-18T18:31:11.747328Z"},"papermill":{"duration":0.723513,"end_time":"2025-09-14T19:17:47.228354","exception":false,"start_time":"2025-09-14T19:17:46.504841","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"ptxas -> ptxas: NVIDIA (R) Ptx optimizing assembler\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Thu_Jun__6_02:14:54_PDT_2024\nCuda compilation tools, release 12.5, V12.5.82\nBuild cuda_12.5.r12.5/compiler.34385749_0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Multi-GPU server launcher - one server per GPU\nimport os, sys, time, subprocess, json, socket, requests\n\nif START_SERVER:\n  # Ensure log/output directory exists\n  os.makedirs(SUBMIT_DIR, exist_ok=True)  # <<<\n\n  # Get GPU count\n  try:\n      import torch\n      if torch.cuda.is_available():\n          torch.cuda.empty_cache()\n          torch.cuda.synchronize()\n          print(\"CUDA memory cleared.\")\n      num_gpus = torch.cuda.device_count()\n  except Exception:\n      num_gpus = 1\n      \n  print(f\"üî• Detected {num_gpus} GPU(s) - starting {num_gpus} servers\")\n\n  # Kill any existing sglang processes\n  subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n  time.sleep(3)\n\n  model_path_to_use = str(MODEL_PATH)\n  print(f\"üîß Using model from {model_path_to_use}\")\n\n  servers = []  # Track all server processes\n  base_port = 8080\n\n  # Start one server per GPU\n  for gpu_id in range(num_gpus):\n      port = base_port + gpu_id\n      log_file_path = f\"{SUBMIT_DIR}/sglang_server_gpu{gpu_id}.log\"\n      print(f\"GPU {gpu_id} ‚Üí port {port} ‚Üí log {log_file_path}\")\n\n      # Environment with specific GPU visibility\n      env = os.environ.copy()\n      env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n      env[\"PYTHONUNBUFFERED\"] = \"1\"  # <<< force unbuffered logs from child too\n\n      # Debug: Print environment\n      print(f\"   CUDA_VISIBLE_DEVICES={env['CUDA_VISIBLE_DEVICES']}\")\n\n      SERVER_CMD = [\n            sys.executable, \"-u\", \"-m\", \"sglang.launch_server\",\n            \"--host\", \"0.0.0.0\",\n            \"--port\", str(port),\n            \"--model-path\", model_path_to_use,\n            \"--dp\", \"1\",                    # <<< one process sees one GPU\n            \"--enable-metrics\",\n            \"--grammar-backend\", \"none\",\n      ]\n        \n      # Add Qwen-specific flag\n      if 'qwen' in model_path_to_use.lower():\n          SERVER_CMD.extend([\"--kv-cache-dtype\", \"fp8_e4m3\"])\n\n      print(f\"   Command: {' '.join(SERVER_CMD)}\")\n\n      # Launch server with explicit error handling\n      try:\n          log_f = open(log_file_path, \"w\")\n          log_f.write(f\"Starting SGLang server for GPU {gpu_id}\\n\")\n          log_f.write(f\"Command: {' '.join(SERVER_CMD)}\\n\")\n          log_f.write(f\"CUDA_VISIBLE_DEVICES: {env['CUDA_VISIBLE_DEVICES']}\\n\")\n          log_f.write(\"=\" * 50 + \"\\n\")\n          log_f.flush()\n\n          proc = subprocess.Popen(\n              SERVER_CMD, \n              stdout=log_f, \n              stderr=subprocess.STDOUT, \n              env=env, \n              cwd=SUBMIT_DIR\n          )\n\n          time.sleep(2)\n          \n          poll_result = proc.poll()\n          if poll_result is not None:\n              print(f\"‚ùå GPU {gpu_id} server process exited immediately with code {poll_result}\")\n              log_f.write(f\"Process exited immediately with code {poll_result}\\n\")\n              log_f.flush()  # <<< ensure you see the exit line\n              log_f.close()\n              continue\n          else:\n              print(f\"‚úÖ GPU {gpu_id} server process started successfully (PID {proc.pid})\")\n      \n      except Exception as e:\n          print(f\"‚ùå Failed to start GPU {gpu_id} server: {e}\")\n          try:\n              log_f.write(f\"Failed to start server: {e}\\n\")\n              log_f.flush()  # <<<\n              log_f.close()\n          except:\n              pass\n          continue\n      \n      servers.append({\n          'gpu_id': gpu_id,\n          'port': port,\n          'process': proc,\n          'log_file': log_f,\n          'log_path': log_file_path,\n          'health_url': f\"http://127.0.0.1:{port}/v1/models\"\n      })\n\n  if not servers:\n      print(\"‚ùå No servers started successfully!\")\n      raise RuntimeError(\"Failed to start any servers\")\n\n  print(f\"üîÑ Started {len(servers)} servers, waiting for readiness...\")\n\n  # Wait for all servers to be ready\n  def wait_ready(url, timeout_s=600):\n      t0 = time.time()\n      while time.time() - t0 < timeout_s:\n          try:\n              r = requests.get(url, timeout=3)\n              if r.status_code == 200:\n                  return True\n          except Exception:\n              pass\n          time.sleep(2)\n      return False\n\n  print(\"üîÑ Waiting for all servers to be ready...\")\n  ready_servers = []\n  \n  for server in servers:\n      print(f\"Checking GPU {server['gpu_id']} on port {server['port']}...\")\n      if wait_ready(server['health_url']):\n          print(f\"‚úÖ GPU {server['gpu_id']} server ready on port {server['port']}\")\n          \n          # Get model name\n          try:\n              response = requests.get(server['health_url'])\n              if response.status_code == 200:\n                  models = response.json()['data']\n                  if models:\n                      model_name = models[0]['id']\n                      server['model_name'] = model_name\n                      print(f\"   Model loaded: {model_name}\")\n                  else:\n                      server['model_name'] = str(MODEL_PATH)\n              else:\n                  server['model_name'] = str(MODEL_PATH)\n          except Exception as e:\n              print(f\"‚ö†Ô∏è Could not get model name for GPU {server['gpu_id']}: {e}\")\n              server['model_name'] = str(MODEL_PATH)\n          \n          ready_servers.append(server)\n      else:\n          print(f\"‚ùå GPU {server['gpu_id']} server failed to start - check {server['log_path']}\")\n          # Check if process is still running\n          if server['process'].poll() is not None:\n              print(f\"   Process exited with code {server['process'].poll()}\")\n          server['log_file'].close()\n\n  # Update servers list to only include ready servers\n  servers = ready_servers\n  \n  if not servers:\n      print(\"‚ùå No servers started successfully!\")\n      raise RuntimeError(\"Failed to start any servers\")\n  \n  print(f\"üöÄ {len(servers)} servers ready! GPU ports: {[s['port'] for s in servers]}\")\n\n  # Use first server's model name for compatibility\n  model_name = servers[0]['model_name']\n\n  # Cleanup functions\n  def stop_all_servers():\n      for server in servers:\n          try:\n              server['process'].terminate()\n              server['process'].wait(timeout=10)\n              server['log_file'].close()\n          except Exception:\n              server['process'].kill()\n      print(\"All servers stopped.\")\n\n  def full_cleanup():\n      # Stop all servers\n      for server in servers:\n          try:\n              server['process'].terminate()\n              server['process'].wait(timeout=10)\n              server['log_file'].close()\n          except Exception:\n              server['process'].kill()\n\n      # Kill any lingering sglang processes\n      subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n\n      # Clear CUDA memory on all GPUs\n      try:\n          import torch\n          if torch.cuda.is_available():\n              for gpu_id in range(torch.cuda.device_count()):\n                  torch.cuda.set_device(gpu_id)\n                  torch.cuda.empty_cache()\n                  torch.cuda.synchronize()\n      except:\n          pass\n\n      print(\"All servers stopped and CUDA memory cleared.\")\n\n  print(\"Call stop_all_servers() or full_cleanup() to shut down all servers.\")","metadata":{"execution":{"iopub.status.busy":"2025-09-18T18:31:11.749184Z","iopub.execute_input":"2025-09-18T18:31:11.749379Z","iopub.status.idle":"2025-09-18T18:35:55.188845Z","shell.execute_reply.started":"2025-09-18T18:31:11.749364Z","shell.execute_reply":"2025-09-18T18:35:55.188258Z"},"papermill":{"duration":180.6259,"end_time":"2025-09-14T19:20:47.865965","exception":false,"start_time":"2025-09-14T19:17:47.240065","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"CUDA memory cleared.\nüî• Detected 4 GPU(s) - starting 4 servers\nüîß Using model from /kaggle/input/gpt-oss-20b/transformers/default/1\nGPU 0 ‚Üí port 8080 ‚Üí log /kaggle/working/sglang_server_gpu0.log\n   CUDA_VISIBLE_DEVICES=0\n   Command: /usr/bin/python3 -u -m sglang.launch_server --host 0.0.0.0 --port 8080 --model-path /kaggle/input/gpt-oss-20b/transformers/default/1 --dp 1 --enable-metrics --grammar-backend none\n‚úÖ GPU 0 server process started successfully (PID 128)\nGPU 1 ‚Üí port 8081 ‚Üí log /kaggle/working/sglang_server_gpu1.log\n   CUDA_VISIBLE_DEVICES=1\n   Command: /usr/bin/python3 -u -m sglang.launch_server --host 0.0.0.0 --port 8081 --model-path /kaggle/input/gpt-oss-20b/transformers/default/1 --dp 1 --enable-metrics --grammar-backend none\n‚úÖ GPU 1 server process started successfully (PID 129)\nGPU 2 ‚Üí port 8082 ‚Üí log /kaggle/working/sglang_server_gpu2.log\n   CUDA_VISIBLE_DEVICES=2\n   Command: /usr/bin/python3 -u -m sglang.launch_server --host 0.0.0.0 --port 8082 --model-path /kaggle/input/gpt-oss-20b/transformers/default/1 --dp 1 --enable-metrics --grammar-backend none\n‚úÖ GPU 2 server process started successfully (PID 130)\nGPU 3 ‚Üí port 8083 ‚Üí log /kaggle/working/sglang_server_gpu3.log\n   CUDA_VISIBLE_DEVICES=3\n   Command: /usr/bin/python3 -u -m sglang.launch_server --host 0.0.0.0 --port 8083 --model-path /kaggle/input/gpt-oss-20b/transformers/default/1 --dp 1 --enable-metrics --grammar-backend none\n‚úÖ GPU 3 server process started successfully (PID 131)\nüîÑ Started 4 servers, waiting for readiness...\nüîÑ Waiting for all servers to be ready...\nChecking GPU 0 on port 8080...\n‚úÖ GPU 0 server ready on port 8080\n   Model loaded: /kaggle/input/gpt-oss-20b/transformers/default/1\nChecking GPU 1 on port 8081...\n‚úÖ GPU 1 server ready on port 8081\n   Model loaded: /kaggle/input/gpt-oss-20b/transformers/default/1\nChecking GPU 2 on port 8082...\n‚úÖ GPU 2 server ready on port 8082\n   Model loaded: /kaggle/input/gpt-oss-20b/transformers/default/1\nChecking GPU 3 on port 8083...\n‚úÖ GPU 3 server ready on port 8083\n   Model loaded: /kaggle/input/gpt-oss-20b/transformers/default/1\nüöÄ 4 servers ready! GPU ports: [8080, 8081, 8082, 8083]\nCall stop_all_servers() or full_cleanup() to shut down all servers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"if TEST_INFERENCE:\n    import time\n    import requests\n    \n    # Use custom endpoint if provided, otherwise use local server\n    base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else \"http://127.0.0.1:8080/v1\"\n    url = f\"{base_url}/chat/completions\"\n    \n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n    }\n    \n    messages = [\n        {\"role\" : \"system\", \"content\" : \"You are an expert at solving abstract reasoning puzzles. Write clean, efficient Python code.\"},\n        {\"role\" : \"user\", \"content\" : \"You are solving an ARC (Abstraction and Reasoning Corpus) task. \\nI will show you training examples with input and output grids, plus a test input grid. Your task is to:\\n\\n1. **Analyze the training examples** to discover patterns that map input grids to output grids\\n2. **Write a Python program** that implements your best understanding of the transformation  \\n3. **DO NOT predict or generate the test output** - your job is only to write the transformation program\\n4. **Attempt a solution** - even if the pattern isn't completely clear, provide your best hypothesis\\n5. **Do not repeat the same transformation** - if you have already tried a transformation, do not repeat it.\\n\\n**IMPORTANT: Your transformation must always produce a 10√ó10 output grid.**\\n\\nThe test input is shown for context so you understand what type of grid your program will eventually process. Focus on learning patterns from training examples and writing code that captures your understanding.\\n\\nTraining Examples:\\n\\nExample 1:\\nInput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n5 0 0 5 0 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n2 0 0 2 0 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 2:\\nInput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 5 0 5 5 0 0 5 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 2 0 2 2 0 0 2 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n\\nExample 3:\\nInput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\nOutput:\\n0 0 5 5 0 5 0 5 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 2 2 0 2 0 2 2 5\\n0 0 0 0 0 0 0 0 0 0\\n\\nTest Input:\\n5 0 5 5 0 0 5 0 5 0\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n0 0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0 5\\n\\nAnalyze the patterns in the training examples and write a Python function that performs this transformation.\\n\\n**Approach Guidelines:**\\n- Look for patterns in shapes, colors, positions, sizes, rotations, reflections, etc.\\n- Even if you can't solve all training examples perfectly, implement what patterns you do observe\\n- A partial solution that captures some aspects is better than returning the input unchanged\\n- If the pattern is unclear, make your best educated guess based on what you can see\\n\\nRequirements:\\n- The function takes a 2D list (grid) where grid[row][col] gives the value at that position\\n- Values are integers from 0-9\\n- Return a new grid (2D list) with the transformation applied\\n- You can use numpy if needed - just add 'import numpy as np' at the start of your function\\n- Aim to handle the training examples as well as possible, even if not perfectly\\n- Your function should attempt some meaningful transformation based on the patterns you observe\\n\\nYou MUST end your response with the following exact format:\\n\\nFinal answer:\\n```python\\ndef transform(grid):\\n    # Your transformation logic here (implement your best understanding)\\n    return transformed_grid\\n```\\n\"}\n    ]\n    \n    payload = {\n        \"model\": model_name,  # from your polling loop\n        \"messages\": messages,\n        # \"max_tokens\": 1000\n        \"max_tokens\": 10\n    }\n    \n    start_time = time.time()\n    response = requests.post(url, headers=headers, json=payload, timeout=600)\n    print(response)\n    end_time = time.time()\n    \n    response.raise_for_status()\n    result = response.json()\n    output_text = result[\"choices\"][0][\"message\"][\"content\"]\n    \n    # Estimate token count (4 chars/token assumption)\n    estimated_tokens = len(output_text) / 4\n    elapsed_time = end_time - start_time\n    tokens_per_second = estimated_tokens / elapsed_time\n    \n    print(\"‚úÖ Response received:\")\n    print(output_text)\n    print(f\"\\n‚è± Elapsed time: {elapsed_time:.2f} seconds\")\n    print(f\"üî¢ Estimated tokens: {estimated_tokens:.1f}\")\n    print(f\"‚ö° Output tokens/sec: {tokens_per_second:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2025-09-18T18:35:55.189497Z","iopub.execute_input":"2025-09-18T18:35:55.189681Z","iopub.status.idle":"2025-09-18T18:36:12.253163Z","shell.execute_reply.started":"2025-09-18T18:35:55.189665Z","shell.execute_reply":"2025-09-18T18:36:12.252540Z"},"papermill":{"duration":10.111292,"end_time":"2025-09-14T19:26:58.025629","exception":false,"start_time":"2025-09-14T19:26:47.914337","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"<Response [200]>\n‚úÖ Response received:\n<|channel|>analysis<|message|>We need to analyze training examples.\n\n‚è± Elapsed time: 17.06 seconds\nüî¢ Estimated tokens: 16.8\n‚ö° Output tokens/sec: 0.98\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"if not IS_KAGGLE:\n      %cd /workspace/arc-agi-2025\n\n# Multi-GPU parallel sampling\nif 'servers' not in globals() or not servers:\n    print(\"‚ùå No servers available - falling back to single GPU mode\")\n    # Fallback to original single server approach\n    base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else \"http://127.0.0.1:8080/v1\"\n    servers_to_use = [{'port': 8080, 'gpu_id': 0}]\n    num_gpus = 1\nelse:\n    servers_to_use = servers\n    num_gpus = len(servers)\n\nprint(f\"üî• Running sampling phase across {num_gpus} GPU(s)\")\n\n# Split attempts across GPUs\nattempts_per_gpu = SAMPLING_ATTEMPTS // num_gpus\nremainder_attempts = SAMPLING_ATTEMPTS % num_gpus\n\nSUBSET = \"test\" if IS_RERUN else \"evaluation\"\n\nprint(f\"Sampling Inference ‚Üí {'competition' if IS_RERUN else 'dev'} | total_attempts={SAMPLING_ATTEMPTS} | {num_gpus} GPUs | subset={SUBSET} | timeout={SAMPLING_TIMEOUT}s\")\n\nsampling_processes = []\n\n# Launch one task runner per GPU\nfor i, server_info in enumerate(servers_to_use):\n    gpu_id = server_info.get('gpu_id', i)\n    port = server_info['port']\n\n    # Add delay between launches to ensure unique parquet filenames\n    if i > 0:\n        print(f\"‚è±Ô∏è Waiting 7 seconds before launching GPU {gpu_id} to ensure unique timestamp...\")\n        time.sleep(7)\n    \n    # Distribute attempts (give remainder to first GPUs)\n    attempts = attempts_per_gpu + (1 if i < remainder_attempts else 0)\n    workers = MAX_WORKERS  # Each runner uses full worker count\n    \n    base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else f\"http://127.0.0.1:{port}/v1\"\n    \n    print(f\"GPU {gpu_id}: {attempts} attempts ‚Üí port {port}\")\n\n    # Build the command for this GPU\n    cmd_args = [\n      \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n      \"--dataset\", DATASET,\n      \"--subset\", SUBSET,\n      \"--max_workers\", str(workers),\n      \"--max_attempts\", str(attempts),\n      \"--model\", model_name,\n      \"--base-url\", base_url,\n      \"--unsafe-executor\",\n      \"--single\"\n    ]\n\n    if 'qwen' in model_name.lower():\n        cmd_args.extend([\"--max-tokens\", \"2000\"])\n        cmd_args.extend([\"--qwen-no-think\"])\n    else:\n        cmd_args.extend([\"--max-tokens\", \"64000\"])\n\n    # Add parquet output directory if set\n    if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n        cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n\n    log_file_path = f\"{SUBMIT_DIR}/sampling_gpu{gpu_id}.log\"\n    print(f\"   Logging to: {log_file_path}\")\n\n    # Launch subprocess\n    log_file = open(log_file_path, \"w\")\n    process = subprocess.Popen(\n        cmd_args,\n        stdout=log_file,\n        stderr=subprocess.STDOUT,\n        text=True,\n        cwd=os.getcwd()\n    )\n    \n    sampling_processes.append({\n        'gpu_id': gpu_id,\n        'process': process,\n        'log_file': log_file,\n        'log_path': log_file_path,\n        'attempts': attempts,\n        'port': port\n    })\n\nprint(f\"üöÄ Launched {len(sampling_processes)} parallel sampling processes\")\nprint(\"Command example:\", \" \".join(cmd_args))\n\n# Wait for all sampling processes to complete concurrently\nimport time\nstart_time = time.time()\ncompleted_processes = []\nfailed_processes = []\nremaining_processes = sampling_processes.copy()\n\nprint(f\"‚è≥ Waiting for all {len(sampling_processes)} sampling processes to complete...\")\n\nwhile remaining_processes and (time.time() - start_time) < SAMPLING_TIMEOUT:\n    for proc_info in remaining_processes.copy():\n        gpu_id = proc_info['gpu_id']\n        poll_result = proc_info['process'].poll()\n\n        if poll_result is not None:  # Process has finished\n            remaining_processes.remove(proc_info)\n            proc_info['log_file'].close()\n\n            if poll_result == 0:\n                print(f\"‚úÖ GPU {gpu_id} sampling completed naturally ({proc_info['attempts']} attempts)\")\n                completed_processes.append(proc_info)\n            else:\n                print(f\"‚ùå GPU {gpu_id} sampling failed with return code {poll_result}\")\n                print(f\"üìù Check {proc_info['log_path']} for details\")\n                failed_processes.append(proc_info)\n\n    if remaining_processes:\n        time.sleep(1)  # Check every second\n\n# Handle any processes that timed out - treat as successful\nfor proc_info in remaining_processes:\n    gpu_id = proc_info['gpu_id']\n    print(f\"‚è∞ GPU {gpu_id} sampling timeout reached ({SAMPLING_TIMEOUT}s) - terminating\")\n    proc_info['process'].terminate()\n    try:\n        proc_info['process'].wait(timeout=30)\n        print(f\"‚úÖ GPU {gpu_id} process terminated gracefully\")\n    except subprocess.TimeoutExpired:\n        print(f\"‚ö†Ô∏è GPU {gpu_id} process didn't terminate gracefully, forcing kill\")\n        proc_info['process'].kill()\n        proc_info['process'].wait()\n\n    proc_info['log_file'].close()\n    # Treat timeout as successful completion\n    print(f\"‚úÖ GPU {gpu_id} sampling completed (timeout) ({proc_info['attempts']} attempts)\")\n    completed_processes.append(proc_info)\n\nprint(f\"üìä Sampling Results: {len(completed_processes)} succeeded, {len(failed_processes)} failed\")\n\nif not completed_processes:\n    print(\"‚ùå All sampling processes failed!\")\n    raise RuntimeError(\"No sampling processes completed successfully\")\nelse:\n    total_completed_attempts = sum(p['attempts'] for p in completed_processes)\n    print(f\"‚úÖ Successfully completed {total_completed_attempts} total attempts across {len(completed_processes)} GPUs\")","metadata":{"papermill":{"duration":525.478862,"end_time":"2025-09-14T19:35:43.513043","exception":false,"start_time":"2025-09-14T19:26:58.034181","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T18:36:12.253807Z","iopub.execute_input":"2025-09-18T18:36:12.254036Z","execution_failed":"2025-09-18T19:07:14.618Z"}},"outputs":[{"name":"stdout","text":"üî• Running sampling phase across 4 GPU(s)\nSampling Inference ‚Üí dev | total_attempts=8 | 4 GPUs | subset=evaluation | timeout=18000s\nGPU 0: 2 attempts ‚Üí port 8080\n   Logging to: /kaggle/working/sampling_gpu0.log\n‚è±Ô∏è Waiting 7 seconds before launching GPU 1 to ensure unique timestamp...\nGPU 1: 2 attempts ‚Üí port 8081\n   Logging to: /kaggle/working/sampling_gpu1.log\n‚è±Ô∏è Waiting 7 seconds before launching GPU 2 to ensure unique timestamp...\nGPU 2: 2 attempts ‚Üí port 8082\n   Logging to: /kaggle/working/sampling_gpu2.log\n‚è±Ô∏è Waiting 7 seconds before launching GPU 3 to ensure unique timestamp...\nGPU 3: 2 attempts ‚Üí port 8083\n   Logging to: /kaggle/working/sampling_gpu3.log\nüöÄ Launched 4 parallel sampling processes\nCommand example: uv run python -u -m llm_python.run_arc_tasks_soar --dataset arc-prize-2025 --subset evaluation --max_workers 32 --max_attempts 2 --model /kaggle/input/gpt-oss-20b/transformers/default/1 --base-url http://127.0.0.1:8083/v1 --unsafe-executor --single --max-tokens 64000 --parquet-output-dir /kaggle/working\n‚è≥ Waiting for all 4 sampling processes to complete...\n‚úÖ GPU 2 sampling completed naturally (2 attempts)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"if ENABLE_REFINEMENT:\n    print(\"üîÑ Checking if server restart is needed...\")\n    \n    # Since we're using the same model for both initial and refinement inference,\n    # we don't need to restart the server\n    print(\"‚úÖ Using same model for refinement - no server restart needed\")\n    print(f\"üéØ Continuing with existing model: {MODEL_PATH}\")\n    \n    # Just verify the server is still running and get the model name\n    if START_SERVER:\n        try:\n            HEALTH_URL = \"http://127.0.0.1:8080/v1/models\"\n            response = requests.get(HEALTH_URL, timeout=5)\n            if response.status_code == 200:\n                models = response.json()['data']\n                if models:\n                    model_name = models[0]['id']\n                    print(f\"‚úÖ Server is running with model: {model_name}\")\n                else:\n                    print(\"‚ùå No models found on server\")\n            else:\n                print(f\"‚ùå Server health check failed: {response.status_code}\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Could not verify server status: {e}\")\n            print(\"‚ùå Server may not be running properly\")\n    else:\n        print(\"‚ÑπÔ∏è START_SERVER is False - assuming server is managed externally\")","metadata":{"papermill":{"duration":190.200335,"end_time":"2025-09-14T19:38:53.770008","exception":false,"start_time":"2025-09-14T19:35:43.569673","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-09-18T19:07:14.619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Multi-GPU Parallel Refinement - Looped Implementation\nimport glob\n\nif ENABLE_REFINEMENT:\n    print(\"üîÑ Running refinement inference (ENABLE_REFINEMENT mode)\")\n    \n    if not IS_KAGGLE:\n        %cd /workspace/arc-agi-2025\n\n    # Use available servers from sampling phase\n    if 'servers' not in globals() or not servers:\n        print(\"‚ùå No servers available - falling back to single GPU mode\")\n        servers_to_use = [{'port': 8080, 'gpu_id': 0}]\n        num_gpus = 1\n    else:\n        servers_to_use = servers\n        num_gpus = len(servers)\n\n    SUBSET = \"test\" if IS_RERUN else \"evaluation\"\n\n    # Run R refinement phases\n    for phase in range(1, R + 1):\n        print(f\"\\nüéØ Phase {phase} of {R} refinement phases\")\n        print(f\"üî• Running refinement phase {phase} across {num_gpus} GPU(s)\")\n\n        # Split attempts across GPUs for this phase\n        attempts_per_gpu = REFINEMENT_ATTEMPTS_PER_PHASE // num_gpus\n        remainder_attempts = REFINEMENT_ATTEMPTS_PER_PHASE % num_gpus\n\n        print(f\"Refinement Phase {phase} ‚Üí {'competition' if IS_RERUN else 'dev'} | total_attempts={REFINEMENT_ATTEMPTS_PER_PHASE} | {num_gpus} GPUs | subset={SUBSET} | timeout={REFINEMENT_TIMEOUT_PER_PHASE}s\")\n\n        # üîë Find ALL parquet files in inference_dir (from sampling + previous refinement phases)\n        parquet_files = glob.glob(os.path.join(inference_dir, \"*.parquet\"))\n        if not parquet_files:\n            raise FileNotFoundError(f\"No parquet files found in {inference_dir}\")\n        \n        # Sort by creation time for consistent ordering\n        parquet_files.sort(key=os.path.getctime)\n        print(f\"üìÇ Using ALL {len(parquet_files)} parquet files for refinement phase {phase}:\")\n        for pf in parquet_files:\n            print(f\"  ‚Ä¢ {os.path.basename(pf)}\")\n\n        refinement_processes = []\n\n        # Launch one refinement task runner per GPU\n        for i, server_info in enumerate(servers_to_use):\n            gpu_id = server_info.get('gpu_id', i)\n            port = server_info['port']\n\n            # Add delay between launches to ensure unique parquet filenames\n            if i > 0:\n                print(f\"‚è±Ô∏è Waiting 7 seconds before launching GPU {gpu_id} refinement to ensure unique timestamp...\")\n                time.sleep(7)\n            \n            # Distribute attempts (give remainder to first GPUs)\n            attempts = attempts_per_gpu + (1 if i < remainder_attempts else 0)\n            workers = MAX_WORKERS  # Each runner uses full worker count\n            \n            base_url = CUSTOM_ENDPOINT if CUSTOM_ENDPOINT else f\"http://127.0.0.1:{port}/v1\"\n            \n            print(f\"GPU {gpu_id}: {attempts} refinement attempts ‚Üí port {port}\")\n\n            # Build the command for this GPU\n            cmd_args = [\n                \"uv\", \"run\", \"python\", \"-u\", \"-m\", \"llm_python.run_arc_tasks_soar\",\n                \"--dataset\", DATASET,\n                \"--subset\", SUBSET,\n                \"--max_workers\", str(workers),\n                \"--max_attempts\", str(attempts),\n                \"--model\", model_name,\n                \"--base-url\", base_url,\n                \"--unsafe-executor\",\n                \"--single\",\n                \"--refinement-ds\"\n            ]\n            \n            # Add ALL parquet files as refinement datasets (each GPU gets same files)\n            cmd_args.extend(parquet_files)\n\n            if 'qwen' in model_name.lower():\n                cmd_args.extend([\"--max-tokens\", \"2000\"])\n                cmd_args.extend([\"--qwen-no-think\"])\n            else:\n                cmd_args.extend([\"--max-tokens\", \"64000\"])\n\n            # Add parquet output directory if set\n            if os.getenv(\"ARC_PROGRAMS_PARQUET\"):\n              cmd_args.extend([\"--parquet-output-dir\", os.getenv(\"ARC_PROGRAMS_PARQUET\")])\n              cmd_args.extend([\"--rex-stats\"])\n\n            log_file_path = f\"{SUBMIT_DIR}/refinement_phase{phase}_gpu{gpu_id}.log\"\n            print(f\"   Logging to: {log_file_path}\")\n\n            # Launch subprocess\n            log_file = open(log_file_path, \"w\")\n            process = subprocess.Popen(\n                cmd_args,\n                stdout=log_file,\n                stderr=subprocess.STDOUT,\n                text=True,\n                cwd=os.getcwd()\n            )\n            \n            refinement_processes.append({\n                'gpu_id': gpu_id,\n                'process': process,\n                'log_file': log_file,\n                'log_path': log_file_path,\n                'attempts': attempts,\n                'port': port\n            })\n\n        print(f\"üöÄ Launched {len(refinement_processes)} parallel refinement processes for phase {phase}\")\n        print(\"Command example:\", \" \".join(cmd_args[:15]) + \" ...\")  # Truncate long command\n\n        # Wait for all refinement processes to complete concurrently\n        start_time = time.time()\n        completed_refinement = []\n        failed_refinement = []\n        remaining_processes = refinement_processes.copy()\n\n        print(f\"‚è≥ Waiting for all {len(refinement_processes)} refinement processes to complete...\")\n\n        while remaining_processes and (time.time() - start_time) < REFINEMENT_TIMEOUT_PER_PHASE:\n            for proc_info in remaining_processes.copy():\n                gpu_id = proc_info['gpu_id']\n                poll_result = proc_info['process'].poll()\n\n                if poll_result is not None:  # Process has finished\n                    remaining_processes.remove(proc_info)\n                    proc_info['log_file'].close()\n\n                    if poll_result == 0:\n                        print(f\"‚úÖ GPU {gpu_id} refinement phase {phase} completed naturally ({proc_info['attempts']} attempts)\")\n                        completed_refinement.append(proc_info)\n                    else:\n                        print(f\"‚ùå GPU {gpu_id} refinement phase {phase} failed with return code {poll_result}\")\n                        print(f\"üìù Check {proc_info['log_path']} for details\")\n                        failed_refinement.append(proc_info)\n\n            if remaining_processes:\n                time.sleep(1)  # Check every second\n\n        # Handle any processes that timed out - treat as successful\n        for proc_info in remaining_processes:\n            gpu_id = proc_info['gpu_id']\n            print(f\"‚è∞ GPU {gpu_id} refinement phase {phase} timeout reached ({REFINEMENT_TIMEOUT_PER_PHASE}s) - terminating\")\n            proc_info['process'].terminate()\n            try:\n                proc_info['process'].wait(timeout=30)\n                print(f\"‚úÖ GPU {gpu_id} process terminated gracefully\")\n            except subprocess.TimeoutExpired:\n                print(f\"‚ö†Ô∏è GPU {gpu_id} process didn't terminate gracefully, forcing kill\")\n                proc_info['process'].kill()\n                proc_info['process'].wait()\n\n            proc_info['log_file'].close()\n            # Treat timeout as successful completion\n            print(f\"‚úÖ GPU {gpu_id} refinement phase {phase} completed (timeout) ({proc_info['attempts']} attempts)\")\n            completed_refinement.append(proc_info)\n\n        print(f\"üìä Refinement Phase {phase} Results: {len(completed_refinement)} succeeded, {len(failed_refinement)} failed\")\n\n        if completed_refinement:\n            total_completed_attempts = sum(p['attempts'] for p in completed_refinement)\n            print(f\"‚úÖ Successfully completed {total_completed_attempts} total refinement attempts for phase {phase} across {len(completed_refinement)} GPUs\")\n        else:\n            print(f\"‚ö†Ô∏è All refinement phase {phase} processes failed - continuing to next phase\" if phase < R else \"‚ö†Ô∏è All refinement phase {phase} processes failed\")\n\n    print(f\"\\nüéâ Completed all {R} refinement phases!\")\n\nelse:\n    print(\"üîÑ Skipping refinement phase (ENABLE_REFINEMENT=false)\")\n    print(\"   ‚Üí Standard mode runs sampling phase only\")","metadata":{"papermill":{"duration":556.082824,"end_time":"2025-09-14T19:48:09.870010","exception":false,"start_time":"2025-09-14T19:38:53.787186","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-09-18T19:07:14.619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate submission using ALL parquet files\n\nprint(\"üéØ Generating submission from ALL parquet files in the directory...\")\n\nimport subprocess\n\noutput_dir = str(SUBMIT_DIR)\n\n# Command to generate submission using ALL parquet files\nsubmission_cmd = [\n    \"uv\", \"run\", \"python\", \"-m\", \"llm_python.generate_submission\",\n    \"--parquet-path\", inference_dir,\n    \"--n-files\", \"-1\",  # Use ALL files instead of just the 2 most recent\n    \"--dataset\", DATASET,\n    \"--subset\", SUBSET,\n    \"--output-dir\", output_dir,\n    \"--debug\"\n]\n\nprint(f\"Running submission generation: {' '.join(submission_cmd)}\")\nprint(f\"üìÇ Looking for parquet files in: {inference_dir}\")\n\ntry:\n    result = subprocess.run(\n        submission_cmd,\n        capture_output=True,\n        text=True,\n        timeout=300,  # 5 minute timeout\n        cwd=os.getcwd()\n    )\n    \n    if result.returncode == 0:\n        print(\"‚úÖ Submission generation completed successfully!\")\n        print(result.stdout)\n        \n        # Update submit_dir to point to the generated file\n        submit_dir = f\"{output_dir}/submission.json\"\n        print(f\"üìÅ Submission file: {submit_dir}\")\n    else:\n        print(f\"‚ùå Submission generation failed with return code {result.returncode}\")\n        print(f\"STDOUT: {result.stdout}\")\n        print(f\"STDERR: {result.stderr}\")\n        # Fallback to default submission path\n        submit_dir = f\"{SUBMIT_DIR}/submission.json\"\n        \nexcept subprocess.TimeoutExpired:\n    print(\"‚è±Ô∏è Submission generation timed out\")\n    submit_dir = f\"{SUBMIT_DIR}/submission.json\"\nexcept Exception as e:\n    print(f\"‚ùå Submission generation error: {e}\")\n    submit_dir = f\"{SUBMIT_DIR}/submission.json\"","metadata":{"papermill":{"duration":10.205901,"end_time":"2025-09-14T19:48:20.105564","exception":false,"start_time":"2025-09-14T19:48:09.899663","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-09-18T19:07:14.619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Only score in dev/commit runs\nif SCORE and not IS_RERUN:\n    !uv run python -m llm_python.score_submission --submission {submit_dir} --dataset {DATASET} --subset {SUBSET}\nelse:\n    print(\"Skipping local scoring (competition rerun or SCORE=False).\")","metadata":{"papermill":{"duration":1.295028,"end_time":"2025-09-14T19:48:21.430229","exception":false,"start_time":"2025-09-14T19:48:20.135201","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:23:22.367802Z","iopub.execute_input":"2025-09-18T17:23:22.367992Z","iopub.status.idle":"2025-09-18T17:23:23.571889Z","shell.execute_reply.started":"2025-09-18T17:23:22.367976Z","shell.execute_reply":"2025-09-18T17:23:23.571191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Multi-GPU cleanup - stop all servers and free resources\nif START_SERVER and 'full_cleanup' in globals():\n    print(\"üßπ Cleaning up all servers and resources...\")\n    full_cleanup()\nelif START_SERVER and 'servers' in globals() and servers:\n    print(\"üßπ Cleaning up all servers and resources...\")\n    \n    # Stop all servers\n    for server in servers:\n        try:\n            print(f\"Stopping GPU {server['gpu_id']} server (PID {server['process'].pid})...\")\n            server['process'].terminate()\n            server['process'].wait(timeout=10)\n            server['log_file'].close()\n        except Exception as e:\n            print(f\"Force killing GPU {server['gpu_id']} server...\")\n            server['process'].kill()\n\n    # Kill any lingering sglang processes\n    subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True)\n\n    # Clear CUDA memory on all GPUs\n    try:\n        import torch\n        if torch.cuda.is_available():\n            for gpu_id in range(torch.cuda.device_count()):\n                torch.cuda.set_device(gpu_id)\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n            print(f\"‚úÖ Cleared CUDA memory on {torch.cuda.device_count()} GPUs\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not clear CUDA memory: {e}\")\n\n    print(\"‚úÖ All servers stopped and CUDA memory cleared.\")\n    \nelse:\n    print(\"üîç No multi-GPU cleanup needed (START_SERVER=False or no servers found)\")","metadata":{"papermill":{"duration":0.089127,"end_time":"2025-09-14T19:48:21.549571","exception":false,"start_time":"2025-09-14T19:48:21.460444","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:23:23.572794Z","iopub.execute_input":"2025-09-18T17:23:23.573012Z","iopub.status.idle":"2025-09-18T17:23:53.725162Z","shell.execute_reply.started":"2025-09-18T17:23:23.572991Z","shell.execute_reply":"2025-09-18T17:23:53.724605Z"}},"outputs":[],"execution_count":null}]}