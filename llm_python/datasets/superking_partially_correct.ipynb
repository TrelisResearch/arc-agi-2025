{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06035807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc2fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=\"trelis-arc\")\n",
    "\n",
    "table_name = \"trelis-arc.arc.superking_partially_correct_clean\"\n",
    "file_name = table_name.split('.')[-1]\n",
    "source_table = \"trelis-arc.arc.superking_ext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b66a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing BigQuery table creation...\n",
      "‚úì Table `trelis-arc.arc.superking_partially_correct_clean` created successfully\n"
     ]
    }
   ],
   "source": [
    "create_final_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{table_name}` AS\n",
    "\n",
    "-- Clean programs by collapsing multiple empty lines into single empty lines\n",
    "WITH programs_cleaned AS (\n",
    "    SELECT \n",
    "        k.task_id,\n",
    "        -- Clean code by collapsing multiple consecutive newlines into at most one empty line\n",
    "        -- Pattern matches multiple consecutive newlines with optional whitespace\n",
    "        REGEXP_REPLACE(k.code, r'\\\\n(\\\\s*\\\\n)+', '\\\\n\\\\n') as code,\n",
    "        k.model,\n",
    "        k.predicted_train_output,\n",
    "        k.predicted_test_output,\n",
    "        k.correct_train_input,\n",
    "        k.correct_test_input\n",
    "    FROM `{source_table}` k\n",
    "    WHERE k.model != 'hodel-translated'\n",
    "),\n",
    "-- Calculate metrics and filter by grid size and correctness\n",
    "programs_with_metrics AS (\n",
    "    SELECT \n",
    "        task_id,\n",
    "        code,\n",
    "        model,\n",
    "        predicted_train_output,\n",
    "        predicted_test_output,\n",
    "        correct_train_input,\n",
    "        correct_test_input,\n",
    "        -- Count correct train examples\n",
    "        (SELECT SUM(IF(element, 1, 0)) \n",
    "         FROM UNNEST(correct_train_input.list)) as correct_train_count,\n",
    "        -- Count correct test examples\n",
    "        (SELECT SUM(IF(element, 1, 0)) \n",
    "         FROM UNNEST(correct_test_input.list)) as correct_test_count,\n",
    "        -- Check grid sizes for train output\n",
    "        (SELECT MAX(ARRAY_LENGTH(grid_2d.element.list)) \n",
    "         FROM UNNEST(predicted_train_output.list) AS grid_2d) as max_train_grid_height,\n",
    "        (SELECT MAX(ARRAY_LENGTH(row_1d.element.list)) \n",
    "         FROM UNNEST(predicted_train_output.list) AS grid_2d,\n",
    "              UNNEST(grid_2d.element.list) AS row_1d) as max_train_grid_width,\n",
    "        -- Check grid sizes for test output\n",
    "        (SELECT MAX(ARRAY_LENGTH(grid_2d.element.list)) \n",
    "         FROM UNNEST(predicted_test_output.list) AS grid_2d) as max_test_grid_height,\n",
    "        (SELECT MAX(ARRAY_LENGTH(row_1d.element.list)) \n",
    "         FROM UNNEST(predicted_test_output.list) AS grid_2d,\n",
    "              UNNEST(grid_2d.element.list) AS row_1d) as max_test_grid_width\n",
    "    FROM programs_cleaned\n",
    "),\n",
    "-- Filter by grid size and require at least one correct in train OR test\n",
    "programs_filtered AS (\n",
    "    SELECT *,\n",
    "        -- Create a hash key from task_id and cleaned code\n",
    "        TO_HEX(SHA256(CONCAT(task_id, '|', code))) as key\n",
    "    FROM programs_with_metrics\n",
    "    WHERE max_train_grid_height <= 40 AND max_train_grid_width <= 40\n",
    "      AND max_test_grid_height <= 40 AND max_test_grid_width <= 40\n",
    "      AND (correct_train_count > 0 OR correct_test_count > 0)\n",
    ")\n",
    "SELECT task_id, code, model, predicted_train_output, predicted_test_output,\n",
    "       correct_train_input, correct_test_input, key\n",
    "FROM programs_filtered\n",
    "ORDER BY task_id, code\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing BigQuery table creation...\")\n",
    "job = client.query(create_final_table_query)\n",
    "result = job.result()\n",
    "print(f\"‚úì Table `{table_name}` created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f284dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total programs in table: 331,430\n"
     ]
    }
   ],
   "source": [
    "# Check table size\n",
    "count_query = f\"SELECT COUNT(*) as total_count FROM `{table_name}`\"\n",
    "count_result = client.query(count_query).result()\n",
    "total_count = next(count_result).total_count\n",
    "print(f\"Total programs in table: {total_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91aa0c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BigQuery table data...\n",
      "Exporting BigQuery table 'trelis-arc.arc.superking_partially_correct_clean' to GCS with sharding...\n",
      "Waiting for BigQuery export to complete...\n",
      "Waiting for BigQuery export to complete...\n",
      "‚úì Export to GCS completed successfully\n",
      "Downloading sharded files from GCS...\n",
      "‚úì Export to GCS completed successfully\n",
      "Downloading sharded files from GCS...\n",
      "Found 10 sharded files\n",
      "Found 10 sharded files\n",
      "‚úì Downloaded shard 1/10\n",
      "‚úì Downloaded shard 1/10\n",
      "‚úì Downloaded shard 2/10\n",
      "‚úì Downloaded shard 2/10\n",
      "‚úì Downloaded shard 3/10\n",
      "‚úì Downloaded shard 3/10\n",
      "‚úì Downloaded shard 4/10\n",
      "‚úì Downloaded shard 4/10\n",
      "‚úì Downloaded shard 5/10\n",
      "‚úì Downloaded shard 5/10\n",
      "‚úì Downloaded shard 6/10\n",
      "‚úì Downloaded shard 6/10\n",
      "‚úì Downloaded shard 7/10\n",
      "‚úì Downloaded shard 7/10\n",
      "‚úì Downloaded shard 8/10\n",
      "‚úì Downloaded shard 8/10\n",
      "‚úì Downloaded shard 9/10\n",
      "‚úì Downloaded shard 9/10\n",
      "‚úì Downloaded shard 10/10\n",
      "Combining sharded files...\n",
      "‚úì Downloaded shard 10/10\n",
      "Combining sharded files...\n",
      "‚úì Combined 331430 rows into /tmp/superking_partially_correct_clean.parquet\n",
      "Reading combined parquet file...\n",
      "‚úì Combined 331430 rows into /tmp/superking_partially_correct_clean.parquet\n",
      "Reading combined parquet file...\n",
      "Loaded 331430 rows from sharded BigQuery table\n",
      "Loaded 331430 programs from BigQuery table\n",
      "Loaded 331430 rows from sharded BigQuery table\n",
      "Loaded 331430 programs from BigQuery table\n"
     ]
    }
   ],
   "source": [
    "from llm_python.datasets.bigquery_export import load_bigquery_table_as_dataframe\n",
    "\n",
    "# Load BigQuery table as DataFrame, automatically handles sharding for large tables\n",
    "print(\"Loading BigQuery table data...\")\n",
    "raw_data = load_bigquery_table_as_dataframe(\n",
    "    client=client,\n",
    "    table_name=table_name,\n",
    "    use_sharding=True,\n",
    ")\n",
    "print(f\"Loaded {len(raw_data)} programs from BigQuery table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cf045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting BigQuery data structure...\n",
      "Sample row columns: ['task_id', 'code', 'model', 'predicted_train_output', 'predicted_test_output', 'correct_train_input', 'correct_test_input', 'key']\n",
      "Train output type: <class 'dict'>\n",
      "Train correct type: <class 'dict'>\n",
      "Key example: d0fd42df8a59468f9e14211fd974ed75823c6cd7773ad12a72d1242b3fd708c8\n",
      "\n",
      "==================================================\n",
      "Converting BigQuery data to SOAR format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting BQ to SOAR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 331430/331430 [00:35<00:00, 9436.10it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 331430 programs from 331430 input rows\n",
      "Saving final dataset to: /tmp/superking_partially_correct_clean.parquet\n",
      "‚úì Saved 331430 programs to /tmp/superking_partially_correct_clean.parquet with proper PyArrow schema\n",
      "‚úì Saved 331430 programs to /tmp/superking_partially_correct_clean.parquet with proper PyArrow schema\n"
     ]
    }
   ],
   "source": [
    "from llm_python.datasets.bigquery_converter import convert_bigquery_to_soar, save_soar_parquet\n",
    "\n",
    "# First, let's inspect the actual data structure\n",
    "print(\"Inspecting BigQuery data structure...\")\n",
    "sample_row = raw_data.iloc[0]\n",
    "print(f\"Sample row columns: {sample_row.index.tolist()}\")\n",
    "print(f\"Train output type: {type(sample_row['predicted_train_output'])}\")\n",
    "print(f\"Train correct type: {type(sample_row['correct_train_input'])}\")\n",
    "print(f\"Key example: {sample_row['key']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Convert BigQuery data to SOAR format using our reusable function\n",
    "print(\"Converting BigQuery data to SOAR format...\")\n",
    "final_dataset = convert_bigquery_to_soar(raw_data, show_progress=True)\n",
    "\n",
    "# Save the final dataset\n",
    "if len(final_dataset) > 0:\n",
    "    output_path = f\"/tmp/{file_name}.parquet\"\n",
    "    print(f\"Saving final dataset to: {output_path}\")\n",
    "    \n",
    "    save_soar_parquet(final_dataset, output_path)\n",
    "else:\n",
    "    print(\"No valid data to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719e91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET VALIDATION\n",
      "================================================================================\n",
      "‚úì Starting dataset validation...\n",
      "‚úì Dataset shape: (331430, 8)\n",
      "‚úì Unique tasks: 976\n",
      "‚úÖ Dataset validation passed!\n",
      "üìä 331,430 programs across 976 tasks\n",
      "ü§ñ Models: 62\n",
      "üìè Programs per task: 1-6530 (avg: 339.6)\n",
      "‚úì Starting dataset validation...\n",
      "‚úì Dataset shape: (331430, 8)\n",
      "‚úì Unique tasks: 976\n",
      "‚úÖ Dataset validation passed!\n",
      "üìä 331,430 programs across 976 tasks\n",
      "ü§ñ Models: 62\n",
      "üìè Programs per task: 1-6530 (avg: 339.6)\n"
     ]
    }
   ],
   "source": [
    "# Validate the final dataset using our reusable validation function\n",
    "from llm_python.datasets.schema import validate_soar_dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "file_path = f\"/tmp/{file_name}.parquet\"\n",
    "\n",
    "results = validate_soar_dataset(pd.read_parquet(file_path), max_grid_size=40, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a171c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    task_id reasoning                                               code  \\\n",
      "0  00576224            def transform(grid):\\n\\n    A = grid[0][0]\\n  ...   \n",
      "1  00576224            def transform(grid):\\n\\n    A = grid[0][0]\\n  ...   \n",
      "2  00576224            def transform(grid):\\n\\n    A = grid[0][0]\\n  ...   \n",
      "3  00576224            def transform(grid):\\n\\n    B1 = grid\\n\\n    B...   \n",
      "4  00576224            def transform(grid):\\n\\n    B1 = grid\\n\\n    B...   \n",
      "5  00576224            def transform(grid):\\n\\n    a = grid[0][0]\\n  ...   \n",
      "6  00576224            def transform(grid):\\n\\n    a = grid[0][0]\\n  ...   \n",
      "7  00576224            def transform(grid):\\n\\n    a = grid[0][0]\\n  ...   \n",
      "8  00576224            def transform(grid):\\n\\n    a = grid[0][0]\\n  ...   \n",
      "9  00576224            def transform(grid):\\n\\n    a, b = grid[0]\\n\\n...   \n",
      "\n",
      "  correct_train_input correct_test_input  \\\n",
      "0        [True, True]             [True]   \n",
      "1        [True, True]             [True]   \n",
      "2        [True, True]             [True]   \n",
      "3        [True, True]             [True]   \n",
      "4        [True, True]             [True]   \n",
      "5        [True, True]             [True]   \n",
      "6        [True, True]             [True]   \n",
      "7        [True, True]             [True]   \n",
      "8        [True, True]             [True]   \n",
      "9        [True, True]             [True]   \n",
      "\n",
      "                              predicted_train_output  \\\n",
      "0  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "1  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "2  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "3  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "4  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "5  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "6  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "7  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "8  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "9  [[[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, ...   \n",
      "\n",
      "                               predicted_test_output  \\\n",
      "0  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "1  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "2  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "3  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "4  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "5  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "6  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "7  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "8  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "9  [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, ...   \n",
      "\n",
      "                                               model  \n",
      "0  Trelis/Qwen3-4B_dsarc-agi-1-train-programs-bes...  \n",
      "1  Trelis/Qwen3-4B_dsarc-programs-50-full-200-par...  \n",
      "2  Trelis/Qwen3-4B_dsarc-agi-1-train-programs-bes...  \n",
      "3                                            o4-mini  \n",
      "4                                            o4-mini  \n",
      "5                                 openai/gpt-oss-20b  \n",
      "6                                Trelis/lorge-16-jul  \n",
      "7                                Trelis/lorge-16-jul  \n",
      "8  Trelis/Qwen3-4B_dsarc-agi-1-train-programs-bes...  \n",
      "9  Trelis/Qwen3-4B_dsarc-programs-50-full-200-par...  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "sample_df = con.execute(f\"SELECT * FROM '{file_path}' LIMIT 10\").fetchdf()\n",
    "con.close()\n",
    "print(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
