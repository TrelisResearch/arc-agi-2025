# Fine-tuning configuration for ARC-AGI model
# This file can be modified for different experiment runs

# Test mode - set to true for quick test runs with only 128 rows of data.
test_run: false

# Data source configuration
data:
  # Dataset source: "huggingface" or "parquet"  
  # If source is "parquet", will load from parquet files
  source: "huggingface"  # can be overridden by DATA_SOURCE env variable
  # source: "parquet"  # can be overridden by DATA_SOURCE env variable
  
  # Parquet configuration (used when source is "parquet")
  parquet:
    path: "../datasets/inference/"  # overridden by ARC_PROGRAMS_PARQUET env variable if set
    # Filter for non-transductive programs (when loading from parquet)
    filters:
      is_transductive: false  # Only load non-transductive programs for fine-tuning
    # Fallback incorrect programs per task (when no correct programs exist)
    max_incorrect_per_task: 0  # Default to including zero incorrect programs per task
      
  # HuggingFace dataset (used when source is "huggingface" and dataset_slug is provided)
  dataset_slug: "Trelis/arc-agi-1-refinement-finetuning"

# Execution mode - controls checkpoint behavior
# "full": save all intermediate checkpoints and optionally push to hub (controlled by PUSH_TO_HUB)
# "final_only": only save and merge final checkpoint (TTT mode) and optionally push (controlled by PUSH_TO_HUB)
# execution_mode: "final_only"  # overridden by FINE_TUNING_MODE env variable
execution_mode: "full"  # overridden by FINE_TUNING_MODE env variable

# Hub pushing configuration
# Controls whether models are pushed to HuggingFace Hub after saving locally
# push_to_hub: true  # overridden by PUSH_TO_HUB env variable
push_to_hub: true  # overridden by PUSH_TO_HUB env variable (false on Kaggle, true elsewhere)

# Model configuration
model:
  slug: "Qwen/Qwen3-4B"  # overridden by MODEL_SLUG env variable if set
  # Alternative models:
  # slug: "julien31/Soar-qwen-7b"
  # slug: "Qwen/Qwen2.5-Coder-7B-Instruct"
  # slug: "Qwen/Qwen3-30B-A3B"
  # slug: "Trelis/Qwen3-4B_ds-arc-agi-1-perfect-50-c642"  # pre-trained model example
  
  max_length: 32768  # Max eval set ~19,400 tokens + headroom
  lora_rank: 128

# Training configuration
training:
  multi_gpu: false  # Set to true to use multi-gpu training, NOT YET SUPPORTED!
  batch_size_global: 4 # Per-device batch size (effective batch size will be 32, grad accum will be set based on number of GPUs, if using multi-gpu training)
  max_rows: null  # Set to integer to limit training data, null for all
  enable_thinking: false  # Required for training qwen base model

# Override max_rows when in test mode
overrides:
  test_run_max_rows: 128  # 32*4 rows for test runs