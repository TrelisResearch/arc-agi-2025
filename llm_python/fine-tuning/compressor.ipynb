{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4199f5-54de-44df-a267-312b5f292db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install unsloth llmcompressor -qU --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b083c2f-e741-4309-b457-860977c65301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\n",
      "4.55.2\n"
     ]
    }
   ],
   "source": [
    "# Then run:\n",
    "import transformers\n",
    "print(transformers.__file__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3383f0c0-0187-45d9-9f78-9bbc86cdf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_to_compress = \"Trelis/arc-1-fake-ttt-blended-c802\"\n",
    "# output_dir = f\"{model_to_compress}-fp8\"\n",
    "\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# print(transformers.__version__)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_to_compress, torch_dtype=\"auto\", trust_remote_code=True, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_to_compress, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816f56ee-a308-4cc6-8f3c-34accf562d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothBCOTrainer: No module named 'UnslothBCOTrainer'. Using tempfile instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.719 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e81d3e3621946b4b882022f3f2e1828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8488803d090445c6809637ce0434df1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1a17aa4aa84be9a8a62e1bde118507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833e2e58fee44369a91b34530bf481f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975cd024089d4e9f9ba48756647f930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cdfbb1f5764d31bf086b881e74c608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cde57f3b71f405cbcbb7d9b559bd63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30cca8a3a3d40c4b29c3f3fe48da3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a3c3500d0544e8a2e65e6f7fbec585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c03d1be762f4905a122fc9fa8b7ca5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499deef5aed449d4b8976b1ef271434a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04841f2c4ea4e4fa601da63ee0cd425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/workspace/hub\" # (recommended) override just the repo cache\n",
    "print(os.environ[\"HF_HOME\"])\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_to_compress = \"Trelis/arc-1-fake-ttt-blended-c802\"\n",
    "output_dir = f\"{model_to_compress}-fp8\"\n",
    "\n",
    "model_max_length = 32768 # max eval set length is around 19,400 tokens. Add some headroom for responses and arc-agi-2 test lengths. This doesn't matter a whole lot because the rows of data are padded to max length.\n",
    "lora_rank = 128\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_to_compress,\n",
    "    max_seq_length = model_max_length,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = False,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # cache_dir = '/workspace',\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    "    \n",
    "    # for using fast_inference\n",
    "    # fast_inference = False, # removing this as it takes up VRAM\n",
    "    # gpu_memory_utilization=0.3, # not needed if not using vLLM for inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a235fb6-d98b-48f3-914b-3e15e1409ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15T08:25:36.122118+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-08-15T08:25:36.125013+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-08-15T08:25:36.196211+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-08-15T08:25:36.196751+0000 | IndependentPipeline | INFO - Inferred `DataFreePipeline` for `QuantizationModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 799/799 [00:00<00:00, 633027.75it/s]\n",
      "Calibrating weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 799/799 [00:00<00:00, 7622.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15T08:25:36.315506+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
      "2025-08-15T08:25:36.317466+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "2025-08-15T08:25:36.335636+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compressing model: 547it [00:09, 56.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('arc-1-fake-ttt-blended-c802-FP8-Dynamic/tokenizer_config.json',\n",
       " 'arc-1-fake-ttt-blended-c802-FP8-Dynamic/special_tokens_map.json',\n",
       " 'arc-1-fake-ttt-blended-c802-FP8-Dynamic/chat_template.jinja',\n",
       " 'arc-1-fake-ttt-blended-c802-FP8-Dynamic/vocab.json',\n",
       " 'arc-1-fake-ttt-blended-c802-FP8-Dynamic/merges.txt',\n",
       " 'arc-1-fake-ttt-blended-c802-FP8-Dynamic/added_tokens.json',\n",
       " 'arc-1-fake-ttt-blended-c802-FP8-Dynamic/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "\n",
    "# Configure the simple PTQ quantization\n",
    "recipe = QuantizationModifier(\n",
    "  targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n",
    "\n",
    "# Apply the quantization algorithm.\n",
    "oneshot(model=model, recipe=recipe)\n",
    "\n",
    "# Save the model.\n",
    "new_name = model_to_compress.rstrip(\"/\").split(\"/\")[-1] + \"-FP8-Dynamic\"\n",
    "SAVE_DIR = 'compressed/' + new_name\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b92734-ab7f-462b-9b93-0540d32ca35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c246a03f5348bcb241b75d6c623dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba20f1eda3a6462689fce846ff28d416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48493dcaee2c43a8ab16acab8772b844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed to https://huggingface.co/Trelis/arc-1-fake-ttt-blended-c802-FP8-Dynamic\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "owner = \"Trelis\"  # use your exact org handle/case\n",
    "repo_id = f\"{owner}/{new_name}\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)  # add private=True if desired\n",
    "\n",
    "# # Ensure large weights go to LFS\n",
    "# from pathlib import Path\n",
    "# (Path(output_dir)/\".gitattributes\").write_text(\n",
    "#     \"*.safetensors filter=lfs diff=lfs merge=lfs -text\\n*.bin filter=lfs diff=lfs merge=lfs -text\\n\"\n",
    "# )\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=SAVE_DIR,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Add FP8 (W8A8 dynamic activations) compressed checkpoint\",\n",
    ")\n",
    "print(f\"Pushed to https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc70974-01ea-4ed8-9726-064fbfe10b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
