{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166172b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = next((parent for parent in [Path.cwd()] + list(Path.cwd().parents) if (parent / \"pyproject.toml\").exists()), Path.cwd())\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17caddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc390986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 ARC-AGI-1 training tasks\n",
      "First 10 task IDs: ['49d1d64f', '890034e9', '1f0c79e5', '1e32b0e9', '22168020', '0b148d64', '3ac3eb23', '54d82841', '2dc579da', 'a9f96cdd']\n"
     ]
    }
   ],
   "source": [
    "from llm_python.utils.task_loader import TaskLoader\n",
    "\n",
    "# Initialize task loader\n",
    "task_loader = TaskLoader()\n",
    "\n",
    "arc_agi_1_training_dir = task_loader.data_root / \"arc-agi-1\" / \"training\"\n",
    "training_task_ids = []\n",
    "\n",
    "if arc_agi_1_training_dir.exists():\n",
    "    for task_file in arc_agi_1_training_dir.glob(\"*.json\"):\n",
    "        training_task_ids.append(task_file.stem)\n",
    "\n",
    "print(f\"Found {len(training_task_ids)} ARC-AGI-1 training tasks\")\n",
    "print(f\"First 10 task IDs: {training_task_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9eea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table with 400 training task IDs\n",
      "\n",
      "First few rows:\n",
      "    task_id\n",
      "0  49d1d64f\n",
      "1  890034e9\n",
      "2  1f0c79e5\n",
      "3  1e32b0e9\n",
      "4  22168020\n"
     ]
    }
   ],
   "source": [
    "# Create a DuckDB table with the training task IDs\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame\n",
    "training_task_df = pd.DataFrame({'task_id': training_task_ids})\n",
    "\n",
    "# Create table in DuckDB\n",
    "con.execute(\"DROP TABLE IF EXISTS arc_agi_1_training_tasks\")\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE arc_agi_1_training_tasks AS \n",
    "    SELECT * FROM training_task_df\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Created table with {len(training_task_ids)} training task IDs\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(con.execute(\"SELECT * FROM arc_agi_1_training_tasks LIMIT 5\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c110fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "              column_name   column_type null   key default extra\n",
      "0                 task_id       VARCHAR  YES  None    None  None\n",
      "1                    code       VARCHAR  YES  None    None  None\n",
      "2  predicted_train_output  BIGINT[][][]  YES  None    None  None\n",
      "3   predicted_test_output  BIGINT[][][]  YES  None    None  None\n",
      "4     correct_train_input     BOOLEAN[]  YES  None    None  None\n",
      "5      correct_test_input     BOOLEAN[]  YES  None    None  None\n",
      "6                   model       VARCHAR  YES  None    None  None\n"
     ]
    }
   ],
   "source": [
    "# Describe the schema and some basic stats of the parquet files\n",
    "schema = con.execute(\"\"\"\n",
    "DESCRIBE SELECT * FROM '/tmp/king_programs_partition_*.parquet'\n",
    "\"\"\").df()\n",
    "print(\"\\nSchema:\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded05651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fully correct programs with deduplication (this may take a moment)...\n",
      "Loaded 29,832 deduplicated fully correct programs\n",
      "Programs available for 370 unique tasks\n",
      "\n",
      "Programs per task distribution:\n",
      "   programs_per_task  num_tasks\n",
      "0                  1         10\n",
      "1                  2          5\n",
      "2                  3          4\n",
      "3                  4          4\n",
      "4                  5          3\n",
      "5                  6          4\n",
      "6                  7          4\n",
      "7                  8         13\n",
      "8                  9         13\n",
      "9                 10         21\n",
      "Loaded 29,832 deduplicated fully correct programs\n",
      "Programs available for 370 unique tasks\n",
      "\n",
      "Programs per task distribution:\n",
      "   programs_per_task  num_tasks\n",
      "0                  1         10\n",
      "1                  2          5\n",
      "2                  3          4\n",
      "3                  4          4\n",
      "4                  5          3\n",
      "5                  6          4\n",
      "6                  7          4\n",
      "7                  8         13\n",
      "8                  9         13\n",
      "9                 10         21\n"
     ]
    }
   ],
   "source": [
    "# Load only fully correct programs with deduplication\n",
    "print(\"Loading fully correct programs with deduplication (this may take a moment)...\")\n",
    "con.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS fully_correct_programs;\n",
    "CREATE TABLE fully_correct_programs AS\n",
    "WITH deduplicated_programs AS (\n",
    "    SELECT \n",
    "        p.task_id,\n",
    "        p.code,\n",
    "        p.predicted_train_output,\n",
    "        p.predicted_test_output,\n",
    "        p.correct_train_input,\n",
    "        p.correct_test_input,\n",
    "        p.model,\n",
    "        LENGTH(p.code) as program_length,\n",
    "        -- Create normalized code for deduplication\n",
    "        LOWER(REGEXP_REPLACE(p.code, '\\s+', '', 'g')) as normalized_code,\n",
    "        -- Keep the shortest program among duplicates, with model as tiebreaker\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY p.task_id, LOWER(REGEXP_REPLACE(p.code, '\\s+', '', 'g'))\n",
    "            ORDER BY LENGTH(p.code) ASC, p.model ASC, p.code ASC\n",
    "        ) as dedup_rank\n",
    "    FROM '/tmp/king_programs_partition_*.parquet' p\n",
    "    INNER JOIN arc_agi_1_training_tasks t ON p.task_id = t.task_id\n",
    "    WHERE p.model != 'hodel-translated'\n",
    "    AND (SELECT SUM(s) FROM UNNEST(p.correct_train_input) AS t(s)) = ARRAY_LENGTH(p.correct_train_input)\n",
    "    AND (SELECT SUM(s) FROM UNNEST(p.correct_test_input) AS t(s)) = ARRAY_LENGTH(p.correct_test_input)\n",
    ")\n",
    "SELECT \n",
    "    task_id,\n",
    "    code,\n",
    "    predicted_train_output,\n",
    "    predicted_test_output,\n",
    "    correct_train_input,\n",
    "    correct_test_input,\n",
    "    model,\n",
    "    program_length,\n",
    "    ROW_NUMBER() OVER (\n",
    "        PARTITION BY task_id \n",
    "        ORDER BY program_length ASC, code ASC\n",
    "    ) as rank_in_task\n",
    "FROM deduplicated_programs\n",
    "WHERE dedup_rank = 1\n",
    "\"\"\")\n",
    "\n",
    "row_count = con.execute(\"SELECT COUNT(*) as count FROM fully_correct_programs\").fetchone()[0]\n",
    "print(f\"Loaded {row_count:,} deduplicated fully correct programs\")\n",
    "\n",
    "# Show some basic stats\n",
    "task_count = con.execute(\"SELECT COUNT(DISTINCT task_id) FROM fully_correct_programs\").fetchone()[0]\n",
    "print(f\"Programs available for {task_count} unique tasks\")\n",
    "\n",
    "# Show distribution of programs per task\n",
    "print(\"\\nPrograms per task distribution:\")\n",
    "task_dist = con.execute(\"\"\"\n",
    "SELECT \n",
    "    programs_per_task,\n",
    "    COUNT(*) as num_tasks\n",
    "FROM (\n",
    "    SELECT task_id, COUNT(*) as programs_per_task\n",
    "    FROM fully_correct_programs \n",
    "    GROUP BY task_id\n",
    ") \n",
    "GROUP BY programs_per_task\n",
    "ORDER BY programs_per_task\n",
    "LIMIT 10\n",
    "\"\"\").df()\n",
    "print(task_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36941cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final training dataset with top N programs per task...\n",
      "Final training dataset contains 12,008 programs\n",
      "\n",
      "Tasks by number of programs in final dataset:\n",
      "program_count\n",
      "1      10\n",
      "2       5\n",
      "3       4\n",
      "4       4\n",
      "5       3\n",
      "6       4\n",
      "7       4\n",
      "8      13\n",
      "9      13\n",
      "10     21\n",
      "11      5\n",
      "12      8\n",
      "13      4\n",
      "14      5\n",
      "15      6\n",
      "16      9\n",
      "17      2\n",
      "18      4\n",
      "19      3\n",
      "20      2\n",
      "21      3\n",
      "22      3\n",
      "23      6\n",
      "24      1\n",
      "25      1\n",
      "26      5\n",
      "27      2\n",
      "28      4\n",
      "29      2\n",
      "30      6\n",
      "32      4\n",
      "33      4\n",
      "34      5\n",
      "35      1\n",
      "36      3\n",
      "37      5\n",
      "38      1\n",
      "39      2\n",
      "40      2\n",
      "41      1\n",
      "42      4\n",
      "43      3\n",
      "44      2\n",
      "45      2\n",
      "47      4\n",
      "48      2\n",
      "49      2\n",
      "50    161\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create final training dataset with only top N fully correct programs per task\n",
    "print(f\"Creating final training dataset with top N programs per task...\")\n",
    "con.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS training_dataset;\n",
    "CREATE TABLE training_dataset AS\n",
    "SELECT \n",
    "    task_id,\n",
    "    code,\n",
    "    predicted_train_output,\n",
    "    predicted_test_output,\n",
    "    correct_train_input,\n",
    "    correct_test_input,\n",
    "    model,\n",
    "    program_length,\n",
    "    rank_in_task\n",
    "FROM fully_correct_programs \n",
    "WHERE rank_in_task <= 50\n",
    "\"\"\")\n",
    "\n",
    "final_count = con.execute(\"SELECT COUNT(*) FROM training_dataset\").fetchone()[0]\n",
    "print(f\"Final training dataset contains {final_count:,} programs\")\n",
    "\n",
    "# Show how many tasks have different numbers of programs\n",
    "print(\"\\nTasks by number of programs in final dataset:\")\n",
    "task_program_counts = con.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as program_count,\n",
    "    COUNT(DISTINCT task_id) as num_tasks\n",
    "FROM training_dataset\n",
    "GROUP BY task_id\n",
    "ORDER BY program_count\n",
    "\"\"\").df()\n",
    "print(task_program_counts.groupby('program_count').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eef2cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Summary Statistics:\n",
      "   total_programs  unique_tasks  min_length  max_length  avg_length  \\\n",
      "0           12008           370          42        6477  826.533145   \n",
      "\n",
      "   median_length  \n",
      "0          680.0  \n",
      "\n",
      "Tasks with most programs (top 10):\n",
      "    task_id  program_count  min_length  max_length  avg_length\n",
      "0  007bbfb7             50         304         854      553.42\n",
      "1  08ed6ac7             50         297         753      601.02\n",
      "2  0b148d64             50         547        1036      862.90\n",
      "3  0ca9ddb6             50         505        1031      865.86\n",
      "4  0d3d703e             50         154         352      277.86\n",
      "5  1190e5a7             50         455         820      697.32\n",
      "6  178fcbfb             50         398         911      735.66\n",
      "7  1cf80156             50         277         575      463.02\n",
      "8  1e0a9b12             50         245         595      466.80\n",
      "9  1f642eb9             50         544        1985     1188.90\n"
     ]
    }
   ],
   "source": [
    "# Compute summary statistics for fully correct programs\n",
    "print(\"Training Dataset Summary Statistics:\")\n",
    "summary_stats = con.execute(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_programs,\n",
    "    COUNT(DISTINCT task_id) as unique_tasks,\n",
    "    MIN(program_length) as min_length,\n",
    "    MAX(program_length) as max_length,\n",
    "    AVG(program_length) as avg_length,\n",
    "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY program_length) as median_length\n",
    "FROM training_dataset\n",
    "\"\"\").df()\n",
    "print(summary_stats)\n",
    "\n",
    "# Show tasks with the most programs (up to 8 each)\n",
    "print(f\"\\nTasks with most programs (top 10):\")\n",
    "top_tasks = con.execute(\"\"\"\n",
    "SELECT \n",
    "    task_id,\n",
    "    COUNT(*) as program_count,\n",
    "    MIN(program_length) as min_length,\n",
    "    MAX(program_length) as max_length,\n",
    "    AVG(program_length) as avg_length\n",
    "FROM training_dataset\n",
    "GROUP BY task_id\n",
    "ORDER BY program_count DESC, task_id\n",
    "LIMIT 10\n",
    "\"\"\").df()\n",
    "print(top_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147b0357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting final dataset...\n",
      "Final training dataset shape: (12008, 7)\n",
      "Number of unique tasks: 370\n",
      "\n",
      "Dataset saved to: /tmp/arc_training_dataset_fully_correct_50.parquet\n",
      "\n",
      "Programs per task distribution:\n",
      "Mean: 32.45\n",
      "Median: 38.50\n",
      "Min: 1, Max: 50\n",
      "\n",
      "Tasks with exactly 8 programs: 13\n",
      "Tasks with fewer than 8 programs: 34\n",
      "\n",
      "Sample task program counts:\n",
      "task_id\n",
      "007bbfb7    50\n",
      "00d62c1b    40\n",
      "017c7c7b    29\n",
      "025d127b     9\n",
      "0520fde7    37\n",
      "05269061    15\n",
      "05f2a901    23\n",
      "06df4c85     9\n",
      "08ed6ac7    50\n",
      "0962bcdd    16\n",
      "dtype: int64\n",
      "Final training dataset shape: (12008, 7)\n",
      "Number of unique tasks: 370\n",
      "\n",
      "Dataset saved to: /tmp/arc_training_dataset_fully_correct_50.parquet\n",
      "\n",
      "Programs per task distribution:\n",
      "Mean: 32.45\n",
      "Median: 38.50\n",
      "Min: 1, Max: 50\n",
      "\n",
      "Tasks with exactly 8 programs: 13\n",
      "Tasks with fewer than 8 programs: 34\n",
      "\n",
      "Sample task program counts:\n",
      "task_id\n",
      "007bbfb7    50\n",
      "00d62c1b    40\n",
      "017c7c7b    29\n",
      "025d127b     9\n",
      "0520fde7    37\n",
      "05269061    15\n",
      "05f2a901    23\n",
      "06df4c85     9\n",
      "08ed6ac7    50\n",
      "0962bcdd    16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Export final training dataset with only original columns\n",
    "print(\"Exporting final dataset...\")\n",
    "training_dataset = con.execute(\"\"\"\n",
    "SELECT \n",
    "    task_id,\n",
    "    code,\n",
    "    predicted_train_output,\n",
    "    predicted_test_output,\n",
    "    correct_train_input,\n",
    "    correct_test_input,\n",
    "    model\n",
    "FROM training_dataset\n",
    "ORDER BY task_id, rank_in_task\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Final training dataset shape: {training_dataset.shape}\")\n",
    "print(f\"Number of unique tasks: {training_dataset['task_id'].nunique()}\")\n",
    "\n",
    "# Save to parquet\n",
    "output_path = \"/tmp/arc_training_dataset_fully_correct_50.parquet\"\n",
    "training_dataset.to_parquet(output_path)\n",
    "print(f\"\\nDataset saved to: {output_path}\")\n",
    "\n",
    "# Show distribution of programs per task\n",
    "print(f\"\\nPrograms per task distribution:\")\n",
    "task_counts = training_dataset.groupby('task_id').size()\n",
    "print(f\"Mean: {task_counts.mean():.2f}\")\n",
    "print(f\"Median: {task_counts.median():.2f}\")\n",
    "print(f\"Min: {task_counts.min()}, Max: {task_counts.max()}\")\n",
    "print(f\"\\nTasks with exactly 8 programs: {(task_counts == 8).sum()}\")\n",
    "print(f\"Tasks with fewer than 8 programs: {(task_counts < 8).sum()}\")\n",
    "\n",
    "# Show a few sample task IDs and their program counts\n",
    "print(f\"\\nSample task program counts:\")\n",
    "print(task_counts.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
