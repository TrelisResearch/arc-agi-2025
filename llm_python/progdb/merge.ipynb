{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817391b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB connection established\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Connect to DuckDB\n",
    "conn = duckdb.connect()\n",
    "\n",
    "print(\"DuckDB connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de614a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parquet files from glob patterns...\n",
      "Pattern 1: 11 files found\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/*.parquet\n",
      "Pattern 2: 5 files found\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--julien31--soar_arc_train_5M/snapshots/cca6061b73c3e80bde9ad3278ad3cfc959bb59de/*.parquet\n",
      "Pattern 3: 1 files found\n",
      "  /home/lewis/code/trelis-arc/hodel_programs.parquet\n",
      "Pattern 4: 1 files found\n",
      "  /home/lewis/code/trelis-arc/soar_log_programs_v2.parquet\n",
      "\n",
      "Total parquet files found: 18\n",
      "\n",
      "Sample files:\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00010-of-00011.parquet\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00001-of-00011.parquet\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00009-of-00011.parquet\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00002-of-00011.parquet\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00007-of-00011.parquet\n",
      "  ... and 13 more\n"
     ]
    }
   ],
   "source": [
    "# Define parquet file glob patterns to merge\n",
    "parquet_globs = [\n",
    "    \"/home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/*.parquet\",\n",
    "    \"/home/lewis/.cache/huggingface/hub/datasets--julien31--soar_arc_train_5M/snapshots/cca6061b73c3e80bde9ad3278ad3cfc959bb59de/*.parquet\",\n",
    "    \"/home/lewis/code/trelis-arc/hodel_programs.parquet\",\n",
    "    \"/home/lewis/code/trelis-arc/soar_log_programs_v2.parquet\"\n",
    "]\n",
    "\n",
    "# Collect all parquet files from the glob patterns\n",
    "print(\"Collecting parquet files from glob patterns...\")\n",
    "all_parquet_files = []\n",
    "\n",
    "for i, pattern in enumerate(parquet_globs):\n",
    "    files = glob.glob(pattern)\n",
    "    if files:\n",
    "        all_parquet_files.extend(files)\n",
    "        print(f\"Pattern {i+1}: {len(files)} files found\")\n",
    "        print(f\"  {pattern}\")\n",
    "    else:\n",
    "        print(f\"Pattern {i+1}: No files found\")\n",
    "        print(f\"  {pattern}\")\n",
    "\n",
    "print(f\"\\nTotal parquet files found: {len(all_parquet_files)}\")\n",
    "\n",
    "# Show some sample files\n",
    "if all_parquet_files:\n",
    "    print(\"\\nSample files:\")\n",
    "    for f in all_parquet_files[:5]:\n",
    "        print(f\"  {f}\")\n",
    "    if len(all_parquet_files) > 5:\n",
    "        print(f\"  ... and {len(all_parquet_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340560d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process 18 parquet files\n",
      "File-by-file processing approach will handle memory efficiently!\n"
     ]
    }
   ],
   "source": [
    "# Ready for file-by-file processing\n",
    "print(f\"Ready to process {len(all_parquet_files)} parquet files\")\n",
    "print(\"File-by-file processing approach will handle memory efficiently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620ea4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking schemas by sampling files...\n",
      "\n",
      "Checking schema for file 1:\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00010-of-00011.parquet\n",
      "  Columns: ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "  Row count: 30,165\n",
      "\n",
      "Checking schema for file 2:\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00001-of-00011.parquet\n",
      "  Columns: ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "  Row count: 30,166\n",
      "\n",
      "Checking schema for file 3:\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00009-of-00011.parquet\n",
      "  Columns: ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "  Row count: 30,165\n",
      "\n",
      "Checking schema for file 4:\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00002-of-00011.parquet\n",
      "  Columns: ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "  Row count: 30,166\n",
      "\n",
      "Checking schema for file 5:\n",
      "  /home/lewis/.cache/huggingface/hub/datasets--Trelis--log-programs-20250805/snapshots/d34fb29527e27a4ab818366d0617f9c545abd643/data/train-00007-of-00011.parquet\n",
      "  Columns: ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "  Row count: 30,166\n",
      "\n",
      "Schemas collected from 5 sample files\n",
      "Schema consistency across samples: ✅ Consistent\n"
     ]
    }
   ],
   "source": [
    "# Check schema by sampling a few files\n",
    "print(\"Checking schemas by sampling files...\")\n",
    "\n",
    "# Sample up to 5 files to check schema consistency\n",
    "sample_size = min(5, len(all_parquet_files))\n",
    "sample_files = all_parquet_files[:sample_size]\n",
    "\n",
    "schemas = []\n",
    "for i, sample_file in enumerate(sample_files):\n",
    "    try:\n",
    "        print(f\"\\nChecking schema for file {i+1}:\")\n",
    "        print(f\"  {sample_file}\")\n",
    "        \n",
    "        # Create a temporary view for this single file\n",
    "        conn.execute(f\"DROP VIEW IF EXISTS temp_sample\")\n",
    "        conn.execute(f\"CREATE VIEW temp_sample AS SELECT * FROM read_parquet('{sample_file}')\")\n",
    "        \n",
    "        # Get column info\n",
    "        schema_info = conn.execute(\"DESCRIBE temp_sample\").fetchall()\n",
    "        column_names = [col[0] for col in schema_info]\n",
    "        schemas.append((sample_file, column_names))\n",
    "        \n",
    "        print(f\"  Columns: {column_names}\")\n",
    "            \n",
    "        # Get row count for this sample file\n",
    "        count = conn.execute(\"SELECT COUNT(*) FROM temp_sample\").fetchone()[0]\n",
    "        print(f\"  Row count: {count:,}\")\n",
    "        \n",
    "        # Clean up\n",
    "        conn.execute(\"DROP VIEW temp_sample\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with file {i+1}: {e}\")\n",
    "\n",
    "print(f\"\\nSchemas collected from {len(schemas)} sample files\")\n",
    "\n",
    "# Check if all sampled files have the same schema\n",
    "if len(schemas) > 1:\n",
    "    first_schema = schemas[0][1]\n",
    "    schema_consistent = all(schema[1] == first_schema for schema in schemas)\n",
    "    print(f\"Schema consistency across samples: {'✅ Consistent' if schema_consistent else '❌ Inconsistent'}\")\n",
    "    \n",
    "    if not schema_consistent:\n",
    "        print(\"Schema differences found:\")\n",
    "        for i, (file_path, columns) in enumerate(schemas):\n",
    "            print(f\"  File {i+1}: {columns}\")\n",
    "else:\n",
    "    print(\"Only one sample file checked - cannot verify consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce426bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for minimum required columns...\n",
      "Available columns: ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "Has basic required columns ['code', 'task_id']: ✅ Yes\n",
      "✅ Files are compatible for merging\n",
      "Files to process: 18\n",
      "\n",
      "Sample data from first file:\n",
      "  code: def transform(grid):\n",
      "    transformed = []\n",
      "    for ..., task_id: 6f473927\n",
      "  code: def transform(input_grid):\n",
      "    \n",
      "    \n",
      "    \n",
      "    tran..., task_id: 6f473927\n",
      "  code: def transform(grid):\n",
      "    transformed = []\n",
      "    for ..., task_id: 6f473927\n"
     ]
    }
   ],
   "source": [
    "# Check if files have the minimum required columns for merging\n",
    "print(\"\\nChecking for minimum required columns...\")\n",
    "\n",
    "# Basic required columns for any merge (can be customized)\n",
    "basic_required = ['code', 'task_id']\n",
    "\n",
    "if schemas:\n",
    "    # Use the first schema as reference\n",
    "    available_columns = [col.lower() for col in schemas[0][1]]\n",
    "    has_basic_required = all(req_col in available_columns for req_col in basic_required)\n",
    "    \n",
    "    print(f\"Available columns: {schemas[0][1]}\")\n",
    "    print(f\"Has basic required columns {basic_required}: {'✅ Yes' if has_basic_required else '❌ No'}\")\n",
    "    \n",
    "    if has_basic_required:\n",
    "        print(f\"✅ Files are compatible for merging\")\n",
    "        compatible_files = all_parquet_files\n",
    "        print(f\"Files to process: {len(compatible_files)}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample data from first file:\")\n",
    "        try:\n",
    "            conn.execute(f\"DROP VIEW IF EXISTS temp_sample\")\n",
    "            conn.execute(f\"CREATE VIEW temp_sample AS SELECT * FROM read_parquet('{all_parquet_files[0]}')\")\n",
    "            sample = conn.execute(\"SELECT code, task_id FROM temp_sample LIMIT 3\").fetchall()\n",
    "            for row in sample:\n",
    "                print(f\"  code: {row[0][:50]}{'...' if len(str(row[0])) > 50 else ''}, task_id: {row[1]}\")\n",
    "            conn.execute(\"DROP VIEW temp_sample\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error sampling: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ Files missing required columns. Available: {available_columns}\")\n",
    "        compatible_files = []\n",
    "else:\n",
    "    print(\"❌ No schema information available\")\n",
    "    compatible_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d81df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra memory-efficient batch merge of 18 parquet files...\n",
      "Batch size: 100,000 rows\n",
      "Required columns: ['task_id', 'code', 'predicted_train_output', 'predicted_test_output', 'correct_train_input', 'correct_test_input', 'model']\n",
      "Note: No deduplication will be performed - raw merge only\n",
      "\n",
      "Available columns in files:\n",
      "  ['task_id', 'reasoning', 'code', 'correct_train_input', 'correct_test_input', 'predicted_train_output', 'predicted_test_output', 'train_input', 'test_input', 'model', 'generation']\n",
      "  ✅ All required columns present\n",
      "\n",
      "Proceeding with ultra memory-efficient batch processing...\n",
      "\n",
      "Processing file 1/18\n",
      "  File: train-00010-of-00011.parquet\n",
      "  Total records in file: 30,165\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,165\n",
      "    Creating initial parquet file...\n",
      "    Batch processed: 30,165 rows\n",
      "  File completed: 30,165 rows processed\n",
      "  Cumulative records: 30,165\n",
      "\n",
      "Processing file 2/18\n",
      "  File: train-00001-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,165 rows\n",
      "  File completed: 30,165 rows processed\n",
      "  Cumulative records: 30,165\n",
      "\n",
      "Processing file 2/18\n",
      "  File: train-00001-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 60,331\n",
      "\n",
      "Processing file 3/18\n",
      "  File: train-00009-of-00011.parquet\n",
      "  Total records in file: 30,165\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,165\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 60,331\n",
      "\n",
      "Processing file 3/18\n",
      "  File: train-00009-of-00011.parquet\n",
      "  Total records in file: 30,165\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,165\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,165 rows\n",
      "  File completed: 30,165 rows processed\n",
      "  Cumulative records: 90,496\n",
      "\n",
      "Processing file 4/18\n",
      "  File: train-00002-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,165 rows\n",
      "  File completed: 30,165 rows processed\n",
      "  Cumulative records: 90,496\n",
      "\n",
      "Processing file 4/18\n",
      "  File: train-00002-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 120,662\n",
      "\n",
      "Processing file 5/18\n",
      "  File: train-00007-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 120,662\n",
      "\n",
      "Processing file 5/18\n",
      "  File: train-00007-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 150,828\n",
      "  >>> Progress: 5/18 files, 5 batches processed\n",
      "\n",
      "Processing file 6/18\n",
      "  File: train-00004-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 150,828\n",
      "  >>> Progress: 5/18 files, 5 batches processed\n",
      "\n",
      "Processing file 6/18\n",
      "  File: train-00004-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 180,994\n",
      "\n",
      "Processing file 7/18\n",
      "  File: train-00006-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 180,994\n",
      "\n",
      "Processing file 7/18\n",
      "  File: train-00006-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 211,160\n",
      "\n",
      "Processing file 8/18\n",
      "  File: train-00005-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 211,160\n",
      "\n",
      "Processing file 8/18\n",
      "  File: train-00005-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 241,326\n",
      "\n",
      "Processing file 9/18\n",
      "  File: train-00003-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 241,326\n",
      "\n",
      "Processing file 9/18\n",
      "  File: train-00003-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 271,492\n",
      "\n",
      "Processing file 10/18\n",
      "  File: train-00000-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 271,492\n",
      "\n",
      "Processing file 10/18\n",
      "  File: train-00000-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 301,658\n",
      "  >>> Progress: 10/18 files, 10 batches processed\n",
      "\n",
      "Processing file 11/18\n",
      "  File: train-00008-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 301,658\n",
      "  >>> Progress: 10/18 files, 10 batches processed\n",
      "\n",
      "Processing file 11/18\n",
      "  File: train-00008-of-00011.parquet\n",
      "  Total records in file: 30,166\n",
      "  Processing in 1 batches of 100,000 rows\n",
      "    Batch 1/1: rows 0 to 30,166\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 331,824\n",
      "\n",
      "Processing file 12/18\n",
      "  File: train_part_1.parquet\n",
      "  Total records in file: 814,790\n",
      "  Processing in 9 batches of 100,000 rows\n",
      "    Batch 1/9: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 30,166 rows\n",
      "  File completed: 30,166 rows processed\n",
      "  Cumulative records: 331,824\n",
      "\n",
      "Processing file 12/18\n",
      "  File: train_part_1.parquet\n",
      "  Total records in file: 814,790\n",
      "  Processing in 9 batches of 100,000 rows\n",
      "    Batch 1/9: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/9: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/9: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/9: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/9: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/9: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/9: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/9: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/9: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/9: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/9: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/9: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/9: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/9: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/9: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/9: rows 800,000 to 814,790\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/9: rows 800,000 to 814,790\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 14,790 rows\n",
      "  File completed: 814,790 rows processed\n",
      "  Cumulative records: 1,146,614\n",
      "\n",
      "Processing file 13/18\n",
      "  File: train_part_0.parquet\n",
      "  Total records in file: 910,780\n",
      "  Processing in 10 batches of 100,000 rows\n",
      "    Batch 1/10: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 14,790 rows\n",
      "  File completed: 814,790 rows processed\n",
      "  Cumulative records: 1,146,614\n",
      "\n",
      "Processing file 13/18\n",
      "  File: train_part_0.parquet\n",
      "  Total records in file: 910,780\n",
      "  Processing in 10 batches of 100,000 rows\n",
      "    Batch 1/10: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/10: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/10: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/10: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/10: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/10: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/10: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/10: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/10: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/10: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/10: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/10: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/10: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/10: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/10: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/10: rows 800,000 to 900,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/10: rows 800,000 to 900,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 10/10: rows 900,000 to 910,780\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 10/10: rows 900,000 to 910,780\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 10,780 rows\n",
      "  File completed: 910,780 rows processed\n",
      "  Cumulative records: 2,057,394\n",
      "\n",
      "Processing file 14/18\n",
      "  File: train_part_2.parquet\n",
      "  Total records in file: 1,167,319\n",
      "  Processing in 12 batches of 100,000 rows\n",
      "    Batch 1/12: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 10,780 rows\n",
      "  File completed: 910,780 rows processed\n",
      "  Cumulative records: 2,057,394\n",
      "\n",
      "Processing file 14/18\n",
      "  File: train_part_2.parquet\n",
      "  Total records in file: 1,167,319\n",
      "  Processing in 12 batches of 100,000 rows\n",
      "    Batch 1/12: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/12: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/12: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/12: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/12: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/12: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/12: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/12: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/12: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/12: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/12: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/12: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/12: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/12: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/12: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/12: rows 800,000 to 900,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/12: rows 800,000 to 900,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 10/12: rows 900,000 to 1,000,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 10/12: rows 900,000 to 1,000,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 11/12: rows 1,000,000 to 1,100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 11/12: rows 1,000,000 to 1,100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 12/12: rows 1,100,000 to 1,167,319\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 12/12: rows 1,100,000 to 1,167,319\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 67,319 rows\n",
      "  File completed: 1,167,319 rows processed\n",
      "  Cumulative records: 3,224,713\n",
      "\n",
      "Processing file 15/18\n",
      "  File: train_part_3.parquet\n",
      "  Total records in file: 1,068,073\n",
      "  Processing in 11 batches of 100,000 rows\n",
      "    Batch 1/11: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 67,319 rows\n",
      "  File completed: 1,167,319 rows processed\n",
      "  Cumulative records: 3,224,713\n",
      "\n",
      "Processing file 15/18\n",
      "  File: train_part_3.parquet\n",
      "  Total records in file: 1,068,073\n",
      "  Processing in 11 batches of 100,000 rows\n",
      "    Batch 1/11: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/11: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 2/11: rows 100,000 to 200,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/11: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 3/11: rows 200,000 to 300,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/11: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 4/11: rows 300,000 to 400,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/11: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 5/11: rows 400,000 to 500,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/11: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 6/11: rows 500,000 to 600,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/11: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 7/11: rows 600,000 to 700,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/11: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 8/11: rows 700,000 to 800,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/11: rows 800,000 to 900,000\n",
      "    Appending batch to existing parquet file...\n",
      "    Batch processed: 100,000 rows\n",
      "    Batch 9/11: rows 800,000 to 900,000\n",
      "    Appending batch to existing parquet file...\n",
      "  ❌ Error processing file: Query interrupted\n",
      "  ❌ Error processing file: Query interrupted\n",
      "\n",
      "Processing file 16/18\n",
      "  File: train_part_4.parquet\n",
      "  Total records in file: 965,525\n",
      "  Processing in 10 batches of 100,000 rows\n",
      "    Batch 1/10: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n",
      "\n",
      "Processing file 16/18\n",
      "  File: train_part_4.parquet\n",
      "  Total records in file: 965,525\n",
      "  Processing in 10 batches of 100,000 rows\n",
      "    Batch 1/10: rows 0 to 100,000\n",
      "    Appending batch to existing parquet file...\n"
     ]
    }
   ],
   "source": [
    "# Partitioned merge: process 1/10th of each file at a time\n",
    "output_path = \"/home/lewis/code/trelis-arc/king_programs.parquet\"\n",
    "NUM_PARTITIONS = 10  # Process files in 10 partitions\n",
    "\n",
    "# Define the columns we want to select from each file - ALL MUST BE PRESENT\n",
    "required_columns = ['task_id', 'code', 'predicted_train_output', 'predicted_test_output', \n",
    "                   'correct_train_input', 'correct_test_input', 'model']\n",
    "\n",
    "if compatible_files:\n",
    "    print(f\"Partitioned merge of {len(compatible_files)} parquet files...\")\n",
    "    print(f\"Number of partitions: {NUM_PARTITIONS}\")\n",
    "    print(f\"Required columns: {required_columns}\")\n",
    "    print(\"Note: No deduplication will be performed - raw merge only\")\n",
    "    \n",
    "    # Check that files have ALL required columns using the first file as reference\n",
    "    if schemas:\n",
    "        available_columns = [col for col in schemas[0][1]]\n",
    "        \n",
    "        print(f\"\\nAvailable columns in files:\")\n",
    "        print(f\"  {available_columns}\")\n",
    "        \n",
    "        # Check which required columns are missing\n",
    "        missing = [col for col in required_columns if col not in available_columns]\n",
    "        if missing:\n",
    "            print(f\"  ❌ MISSING REQUIRED COLUMNS: {missing}\")\n",
    "            print(f\"All files must have ALL required columns: {required_columns}\")\n",
    "            print(\"Please fix the source data or adjust the required columns list.\")\n",
    "        else:\n",
    "            print(f\"  ✅ All required columns present\")\n",
    "            \n",
    "            print(f\"\\nProceeding with partitioned processing...\")\n",
    "            \n",
    "            select_clause = \", \".join(required_columns)\n",
    "            \n",
    "            # First, get row counts for all files to calculate partition sizes\n",
    "            print(\"\\nAnalyzing file sizes for partitioning...\")\n",
    "            file_info = []\n",
    "            total_files_with_data = 0\n",
    "            \n",
    "            for i, file_path in enumerate(compatible_files):\n",
    "                try:\n",
    "                    conn.execute(f\"DROP VIEW IF EXISTS temp_count\")\n",
    "                    conn.execute(f\"CREATE VIEW temp_count AS SELECT {select_clause} FROM read_parquet('{file_path}')\")\n",
    "                    \n",
    "                    file_count = conn.execute(\"SELECT COUNT(*) FROM temp_count\").fetchone()[0]\n",
    "                    conn.execute(\"DROP VIEW temp_count\")\n",
    "                    \n",
    "                    if file_count > 0:\n",
    "                        file_info.append((file_path, file_count))\n",
    "                        total_files_with_data += 1\n",
    "                        print(f\"  File {i+1}: {os.path.basename(file_path)} - {file_count:,} rows\")\n",
    "                    else:\n",
    "                        print(f\"  File {i+1}: {os.path.basename(file_path)} - EMPTY (skipping)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  File {i+1}: {os.path.basename(file_path)} - ERROR: {e}\")\n",
    "            \n",
    "            if not file_info:\n",
    "                print(\"❌ No files with data found!\")\n",
    "            else:\n",
    "                total_records = sum(count for _, count in file_info)\n",
    "                print(f\"\\nFiles with data: {len(file_info)}\")\n",
    "                print(f\"Total records to process: {total_records:,}\")\n",
    "                \n",
    "                # Process each partition\n",
    "                partition_outputs = []\n",
    "                \n",
    "                for partition_num in range(NUM_PARTITIONS):\n",
    "                    print(f\"\\n=== Processing Partition {partition_num + 1}/{NUM_PARTITIONS} ===\")\n",
    "                    \n",
    "                    partition_file = f\"/tmp/king_programs_partition_{partition_num}.parquet\"\n",
    "                    partition_outputs.append(partition_file)\n",
    "                    \n",
    "                    # Create views for this partition from each file\n",
    "                    partition_views = []\n",
    "                    partition_records = 0\n",
    "                    \n",
    "                    for file_idx, (file_path, file_count) in enumerate(file_info):\n",
    "                        # Calculate partition boundaries for this file\n",
    "                        partition_size = file_count // NUM_PARTITIONS\n",
    "                        start_offset = partition_num * partition_size\n",
    "                        \n",
    "                        # Handle the last partition (include remainder)\n",
    "                        if partition_num == NUM_PARTITIONS - 1:\n",
    "                            limit = file_count - start_offset  # Take everything remaining\n",
    "                        else:\n",
    "                            limit = partition_size\n",
    "                        \n",
    "                        if limit <= 0:\n",
    "                            continue\n",
    "                            \n",
    "                        view_name = f\"partition_{partition_num}_file_{file_idx}\"\n",
    "                        partition_views.append(view_name)\n",
    "                        \n",
    "                        print(f\"  File {file_idx + 1}: rows {start_offset:,} to {start_offset + limit:,}\")\n",
    "                        \n",
    "                        try:\n",
    "                            conn.execute(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "                            conn.execute(f\"\"\"\n",
    "                                CREATE VIEW {view_name} AS \n",
    "                                SELECT {select_clause} \n",
    "                                FROM read_parquet('{file_path}')\n",
    "                                LIMIT {limit} OFFSET {start_offset}\n",
    "                            \"\"\")\n",
    "                            \n",
    "                            # Count actual records in this partition\n",
    "                            actual_count = conn.execute(f\"SELECT COUNT(*) FROM {view_name}\").fetchone()[0]\n",
    "                            partition_records += actual_count\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error creating view for {os.path.basename(file_path)}: {e}\")\n",
    "                            partition_views.remove(view_name)\n",
    "                    \n",
    "                    if partition_views:\n",
    "                        print(f\"  Partition {partition_num + 1} total records: {partition_records:,}\")\n",
    "                        \n",
    "                        # Create UNION of all partition views\n",
    "                        union_query = \" UNION ALL \".join([f\"SELECT * FROM {view}\" for view in partition_views])\n",
    "                        \n",
    "                        print(f\"  Writing partition {partition_num + 1} to disk...\")\n",
    "                        conn.execute(f\"COPY ({union_query}) TO '{partition_file}' (FORMAT PARQUET)\")\n",
    "                        \n",
    "                        # Clean up partition views\n",
    "                        for view in partition_views:\n",
    "                            conn.execute(f\"DROP VIEW IF EXISTS {view}\")\n",
    "                        \n",
    "                        print(f\"  ✅ Partition {partition_num + 1} completed\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ Partition {partition_num + 1} has no data\")\n",
    "                \n",
    "                # Now combine all partitions into final output\n",
    "                print(f\"\\n=== Combining {NUM_PARTITIONS} partitions into final output ===\")\n",
    "                \n",
    "                existing_partitions = [p for p in partition_outputs if os.path.exists(p)]\n",
    "                \n",
    "                if existing_partitions:\n",
    "                    print(f\"Found {len(existing_partitions)} partitions to combine\")\n",
    "                    \n",
    "                    # Create final union query\n",
    "                    partition_reads = [f\"SELECT * FROM read_parquet('{p}')\" for p in existing_partitions]\n",
    "                    final_union = \" UNION ALL \".join(partition_reads)\n",
    "                    \n",
    "                    print(f\"Writing final merged file to {output_path}...\")\n",
    "                    conn.execute(f\"COPY ({final_union}) TO '{output_path}' (FORMAT PARQUET)\")\n",
    "                    \n",
    "                    # Clean up partition files\n",
    "                    print(\"Cleaning up partition files...\")\n",
    "                    for partition_file in existing_partitions:\n",
    "                        try:\n",
    "                            os.remove(partition_file)\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Warning: Could not remove {partition_file}: {e}\")\n",
    "                    \n",
    "                    print(\"✅ Partitioned merge completed successfully!\")\n",
    "                    print(f\"Output written to: {output_path}\")\n",
    "                    \n",
    "                    # Verify the output file\n",
    "                    if os.path.exists(output_path):\n",
    "                        file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
    "                        print(f\"Output file size: {file_size:.1f} MB\")\n",
    "                        \n",
    "                        # Quick verification - check final count\n",
    "                        final_count = conn.execute(f\"SELECT COUNT(*) FROM read_parquet('{output_path}')\").fetchone()[0]\n",
    "                        print(f\"Final record count: {final_count:,}\")\n",
    "                        print(f\"Expected record count: {total_records:,}\")\n",
    "                        \n",
    "                        if final_count == total_records:\n",
    "                            print(\"✅ Record count matches - merge successful!\")\n",
    "                        else:\n",
    "                            print(f\"⚠️ Record count mismatch - some data may be missing\")\n",
    "                    \n",
    "                    # Show final schema\n",
    "                    print(\"\\nFinal merged dataset schema:\")\n",
    "                    final_schema = conn.execute(f\"DESCRIBE (SELECT * FROM read_parquet('{output_path}') LIMIT 1)\").fetchall()\n",
    "                    for col_name, col_type, null, key, default, extra in final_schema:\n",
    "                        print(f\"  {col_name}: {col_type}\")\n",
    "                else:\n",
    "                    print(\"❌ No partitions were created successfully!\")\n",
    "    else:\n",
    "        print(\"❌ No schema information available to validate columns\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No compatible files found for merging!\")\n",
    "    print(\"Please check the schema compatibility issues above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-agi-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
